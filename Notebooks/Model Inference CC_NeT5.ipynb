{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5KjkXSi6aqJ7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_Ml5PJgaAVX"},"outputs":[],"source":["! pip install datasets transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N26-n0hvaFcO"},"outputs":[],"source":["!pip install flask-ngrok\n","!pip install flask-bootstrap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRBsKi1yaG1p"},"outputs":[],"source":["!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n","!tar -xvzf  /content/ngrok-v3-stable-linux-amd64.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1687184732925,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"neT28RP3aRMh","outputId":"30505c69-5311-4718-a510-830ab64acaf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}],"source":["! /content/ngrok config add-authtoken 2OdU9FiLDxahD6uzIKXWPgWOAL2_6gXeqwNHHe5vHcJTGrxer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14869,"status":"ok","timestamp":1687184747789,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"T3xPRVzXaJuj","outputId":"faf36193-6153-4896-f881-9abda4d1a578"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyngrok\n","  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=d9a73ae0a74fff83054c2e8aa929dcb3f6276bad9bb20ad593969a49280eebd9\n","  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-6.0.0\n"]}],"source":["!pip install pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VWrk2GfhuZe"},"outputs":[],"source":["batch_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rZgQ4qsagUl"},"outputs":[],"source":["# Some model classes\n","\n","'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3,3),\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=14):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 3\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=2)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        #self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        #out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(x)\n","        dl1 = out.shape\n","        out = self.layer2(out)\n","        dl2 = out.shape\n","        out = self.layer3(out)\n","        dl3 = out.shape\n","        out = self.layer4(out)\n","        dl4 = out.shape\n","        #out = F.avg_pool2d(out, 4)\n","        #out = out.view(out.size(0), -1)\n","\n","        # Flatten (batch size, channels, features)\n","        out = out.view(batch_size, 512, 140)\n","        out_1_s = out.shape\n","        # Might have to change the last layer to obtain our 14x11 feature vector?\n","        #out = self.linear(out)\n","        #print(f'out_1_s: {out_1_s}, out: {out.shape}')\n","        #print(f'dl1: {dl1}, dl2: {dl2}, dl3: {dl3}, dl4: {dl4}, out: {out_1_s}')\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","# ----------------------------------\n","# Cross Attention Model for Language\n","class CrossAttentionModelLanguage(nn.Module):\n","    # Changed hidden_dim from 140 to 1\n","    def __init__(self, input_dim=64, hidden_dim=1, num_heads=4):\n","        super(CrossAttentionModelLanguage, self).__init__()\n","\n","        self.attention = nn.MultiheadAttention(input_dim, num_heads)\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","\n","    def forward(self, input):\n","        # Reshape the input to (sequence_length, batch_size, input_dim)\n","        input = input.permute(1, 0, 2)\n","\n","        # Apply cross-attention\n","        output, _ = self.attention(input, input, input)\n","\n","        # Reshape the output to (batch_size, sequence_length, input_dim)\n","        output = output.permute(1, 0, 2)\n","\n","        # Apply linear transformation\n","        output = self.linear(output)\n","\n","        return output\n","\n","\n","#--------------------------------\n","# Multimodal Transformer Network\n","from torch.nn import Transformer\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self):\n","        super(TransformerNetwork, self).__init__()\n","\n","        self.embedding_dim = 512\n","        self.num_layers = 8\n","        self.num_heads = 8\n","\n","        self.embedding = nn.Linear(142, self.embedding_dim)\n","        self.transformer = Transformer(\n","            d_model=self.embedding_dim,\n","            nhead=self.num_heads,\n","            num_encoder_layers=self.num_layers,\n","            num_decoder_layers=self.num_layers\n","        )\n","        #self.output_layer = nn.Linear(self.embedding_dim, 1)\n","\n","    def forward(self, input_tensor):\n","        batch_size = input_tensor.size(0)\n","\n","        embedded = self.embedding(input_tensor)\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        output = self.transformer(embedded, embedded)\n","        output = output.permute(1, 0, 2)\n","        #output = self.output_layer(output)\n","\n","        return output.squeeze(2)\n","\n","#-------------------\n","# LSTM Netowkr Part\n","class TwoLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(TwoLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        # Note: we use a Linear layer for dimension adjustment.\n","        # Indeed, the previous multi-modal transformer outputs a 1024 + prev_action length tensor,\n","        # and here the LSTM layers have a 512 dim where we also need to bypass residual connections.\n","        # We use the linear layer below to have a matching size with the input/output of these LSTMs\n","        # in order to concatenate them.\n","        self.linear = nn.Linear(input_size, hidden_size)\n","\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2, batch_first=True)\n","\n","        # This linear layer is for the final mapping\n","        #self.fc = nn.Linear(hidden_size, output_size) #fully connected last layer\n","\n","    def forward(self, input):\n","        # Perform dimension adjustment using the linear layer\n","        adjusted_input = self.linear(input)\n","        #print(f'adjusted_input: {adjusted_input.shape}')\n","\n","        # Perform the forward pass through each LSTM layer\n","        output, _hidden_state = self.lstm(adjusted_input)\n","\n","        # Perform residual connections between LSTM layers\n","        residual_output = adjusted_input + output\n","\n","        # Extract the hidden state of the last LSTM layer after residual connections\n","        last_hidden_state = residual_output[:, -1, :]\n","\n","        #out = self.fc(last_hidden_state)\n","        return last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4We_fogaktm"},"outputs":[],"source":["\n","import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","class CCNeT5(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        #self.rgb_model = nn.Sequential(*resnet_blocks)\n","        self.rgb_model = ResNet18()\n","        self.language_model = CrossAttentionModelLanguage()\n","\n","        # Multi Modal Part Combining RGB + Language\n","        #self.multimodal_transformer = MultiModalTransformer()\n","        self.multimodal_transformer = TransformerNetwork()\n","\n","        # Here we should concatenate with the previous Action\n","        # when doing the forward pass\n","\n","        # Two-Layer LSTM\n","        self.lstm = TwoLSTM(1089, 513)\n","\n","        # Last output Layer to create a proper action space from the original language embeddings\n","        self.fc = nn.Linear(513, 577) #fully connected last layer\n","\n","        # T5 Decreased\n","        # Linear layer to reduce the size of the T5 output frm 577 to 512\n","        self.fc_t5 = nn.Linear(577, 512)\n","\n","\n","    # todo: pass previous action\n","    def forward(self, rgb_input, language_input, previous_action, t5_output):\n","        # Process RGB input\n","        rgb_output = self.rgb_model(rgb_input)\n","\n","        # Process language input\n","        language_output = self.language_model(language_input)\n","\n","        #print(f'shape rbg_output: {rgb_output.shape}, shape language_output: {language_output.shape}')\n","        flattened_tensor1 = rgb_output.view(batch_size, 512*140)\n","        flattened_tensor2 = language_output.view(batch_size, -1)\n","\n","        # Concatenate tensor1 and flattened_tensor2 along the last dimension\n","        fused_output = torch.cat((flattened_tensor1, flattened_tensor2), dim=1).view(batch_size, 512, 141)\n","\n","        # Add T5 output to image and language tensor\n","        t5_output = self.fc_t5(t5_output).unsqueeze(dim=2) # Change shape from (batch, 577) to (batch, 512, 1)\n","        # Concatenate with fused output\n","        fused_output = torch.cat((fused_output, t5_output), dim=2)\n","\n","        # Further processing or fusion of modalities\n","        #print(f'fused_output: {fused_output.shape} - {fused_output.dtype}')\n","        # Additional processing steps...\n","\n","        # Permute\n","        #fused_output = fused_output.permute(0, 2, 1)\n","\n","        # Feed into Multimodal Transformer\n","        multimodal_output = self.multimodal_transformer(fused_output)\n","        #print(f'multimodal_output: {multimodal_output.shape} - type: {multimodal_output.dtype}')\n","\n","        # TODO: concatenate with previous action\n","        #print(f'previous_action: {previous_action.shape}')\n","        #tensor2_reshaped = F.pad(previous_action, (0, 447), mode='constant')\n","        #tensor2_expanded = torch.unsqueeze(tensor2_reshaped, dim=2)  # Shape: [8, 1024, 577]\n","        #combined_tensor = torch.cat((multimodal_output, tensor2_expanded), dim=2).type(torch.float32) # Shape: [8, 1024, 1089]\n","\n","        tensor2 = previous_action.unsqueeze(1)\n","        # Expand tensor2 dimensions to match tensor1\n","        tensor2 = tensor2.expand(-1, 512, -1)\n","        # Combine the tensors\n","        combined_tensor = torch.cat((multimodal_output, tensor2), dim=2).type(torch.float32)\n","        #print(f'concatenation previous action: {combined_tensor.shape}')\n","\n","        # Feed into LSTM\n","        # + Permute to fit the right format\n","        #lstm_output = self.lstm(combined_tensor.permute(0, 2, 1))\n","        lstm_output = self.lstm(combined_tensor)\n","        #print(f'lstm_output: {lstm_output.shape}')\n","\n","        # Last layer to target action space\n","        action_output = self.fc(lstm_output)\n","\n","        return action_output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1086,"status":"ok","timestamp":1687185115212,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"xmLWNQu9D0Bn","outputId":"2aea4504-388e-4201-bf0f-75c66237e024"},"outputs":[{"name":"stdout","output_type":"stream","text":["pad: 1590\n","Loaded CC_NeT5 Tokenizer with vocabulary size being 1592.\n"]}],"source":["# 1 - Get out Tokenizer Class\n","css_fields = ['top', 'left', 'width', 'height']\n","special_characters = ['.', ',', '#', ':', '-', '/', '(', ')', 'https://', '@', '\u0026', '\"', \"'\", '!', '?', ';', '+', '=',\n","                      '*', '$', '€', '*', '`']\n","\n","\n","def round_to_nearest_ten(number):\n","    return round(number / 10) * 10\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","# Turn it into a class\n","class CCNeT5Tokenizer:\n","    def __init__(self, vocab_path):\n","        stoi = {}\n","        itos = {}\n","        self.padding_char = '\u003cPAD\u003e'\n","        self.special_characters = ['.', ',', '#', ':', '-', '/', '(', ')', 'https://', '@', '\u0026', '\"', \"'\", '!', '?',\n","                                   ';', '+', '=', '*', '$', '€', '*', '`']\n","\n","        with open(vocab_path, 'r') as file:\n","            for index, line in enumerate(file):\n","                line = line.strip()\n","                stoi[line] = index\n","                itos[index] = line\n","\n","        stoi[' '] = stoi['']\n","        itos[stoi[' ']] = ' '\n","        self.stoi = stoi\n","        self.stoi[self.padding_char] = len(stoi.keys())-1  # Add PADDING character\n","        # We do not need itos as we don't implement a de-tokinezing function.\n","        self.itos = itos\n","        self.itos[len(stoi.keys()) - 2] = self.padding_char  # Add PADDING character\n","        print(f'pad: {len(stoi.keys()) - 2}')\n","        print(f'Loaded CC_NeT5 Tokenizer with vocabulary size being {len(self.stoi)}.')\n","\n","        # Instantiate the embedding function\n","        vocab_size = len(stoi)\n","\n","    # Provide a string and tokenize it: utterance or task name.\n","    def tokenize_string(self, string):\n","        tokenized_string = []\n","        for w in str(string).lower().split(' '):\n","            for sc in self.special_characters:\n","                if sc in w:\n","                    w = w.replace(sc, 'Ø' + sc + 'Ø')\n","            p1_words = [s for s in w.split('Ø') if s != '']\n","            # Check if the found words exist in our vocab, else subdivide\n","            for fw in p1_words:\n","                if fw not in self.stoi.keys():\n","                    for c in fw:\n","                        tokenized_string.append(self.stoi[c])\n","                else:\n","                    tokenized_string.append(self.stoi[fw])\n","        return tokenized_string\n","\n","    # Turns an array into a string\n","    def detokenize_array(self, array):\n","        reconstruted_string = []\n","        #pad_index = self.itos[self.stoi[self.padding_char]]\n","        for v in array:\n","            v = int(v)\n","            s = self.itos[v]\n","            if s != self.padding_char:\n","                reconstruted_string.append(s)\n","\n","        #reconstruted_string = ' '.join(reconstruted_string)\n","        cleaned_string = ''\n","        last_single = False\n","        for s in reconstruted_string:\n","            if len(s) \u003e 1:\n","                if last_single:\n","                    cleaned_string += ' '\n","                    last_single = False\n","                cleaned_string += s + ' '\n","            else:\n","                # We do this to treat single length characters\n","                cleaned_string += s\n","                last_single = True\n","        return cleaned_string\n","\n","    # Truncate and pad the incoming sentences based on a max_size argument\n","    def truncate_pad_entry(self, tokenized_array, max_size):\n","        if len(tokenized_array) \u003c max_size:\n","            while len(tokenized_array) \u003c max_size:\n","                tokenized_array.append(self.stoi[self.padding_char])\n","            return tokenized_array\n","        elif len(tokenized_array) \u003e max_size:\n","            # Too long, so we automatically truncate\n","            return tokenized_array[:max_size]\n","        else:\n","            # Just avoid iterating under the hood\n","            return tokenized_array\n","\n","    # Tokenize a DOM dictionary\n","    def tokenize_dom(self, dom):\n","        tokenized_dom = []\n","\n","        # Add opening tag\n","        element_tag = dom['tag'].lower()\n","        if 'input' in dom.keys():\n","            # if tag is input, reformat it\n","            element_tag = dom['input'].lower() + '_' + dom['type'].lower()\n","            tokenized_dom.append(self.stoi['\u003c' + element_tag])\n","        else:\n","            # add normal tag\n","            tokenized_dom.append(self.stoi['\u003c' + dom['tag'].lower()])\n","\n","        for field in dom:\n","            if field != 'tag' and field != 'children' and field != 'type' and field != 'text' and field not in css_fields:  # and field != 'value':\n","\n","                tokenized_dom.append(self.stoi[field.lower()])\n","\n","                # Ensure we don't have float values, we'll stick with integers\n","                if isinstance(dom[field], float):\n","                    tokenized_dom.append(self.stoi[str(int(round_to_nearest_ten(dom[field])))])\n","                else:\n","                    words = str(dom[field]).lower().split(' ')\n","                    for word in words:\n","                        for sc in special_characters:\n","                            if sc in word:\n","                                word = word.replace(sc, 'Ø' + sc + 'Ø')\n","                        p1_words = [s for s in word.split('Ø') if s != '']\n","\n","                        # decides whether we keep the full word, or just the letters:\n","                        if field == 'value' or (False and (field == 'label' or field == 'button')):\n","                            for w in p1_words:\n","                                # Take individual characters of the value string\n","                                processed_words = [self.stoi[w[i:i + 1]] for i in range(0, len(w), 1)]\n","                                tokenized_dom += processed_words\n","\n","                        elif field == 'ref':\n","                            for w in p1_words:\n","                                # Take individual characters of the value string\n","                                processed_words = [self.stoi[w[i:i + 3]] for i in range(0, len(w), 3)]\n","                                tokenized_dom += processed_words\n","                        else:\n","                            # Use the full word:\n","                            for w in p1_words:\n","                                tokenized_dom.append(self.stoi[str(w)])\n","\n","            elif field == 'text':\n","                tokenized_dom.append(self.stoi['text'])\n","                for c in dom[field]:\n","                    tokenized_dom.append(self.stoi[c])\n","            elif field in css_fields:\n","                # Cast field\n","                tokenized_dom.append(self.stoi[field])\n","                css_value = int(round_to_nearest_ten(float(dom[field])))\n","                tokenized_dom.append(self.stoi[str(css_value)])\n","\n","        if 'children' in dom.keys():\n","            tokenized_dom.append(self.stoi['children'])\n","            for child in dom['children']:\n","                tokenized_dom += self.tokenize_dom(child)\n","                # for v in found_vocab:\n","                #    tokenized_dom.append(self.stoi[v])\n","\n","        # add closing tag\n","        tokenized_dom.append(self.stoi['\u003c/' + element_tag + '\u003e'])\n","\n","        return tokenized_dom\n","\n","vocab_path=\"/content/drive/MyDrive/WebAI/Notebooks/CC_NeT5/vocab.txt\"\n","embedding_fn_path = '/content/drive/MyDrive/WebAI/Configs/embedding_weights.pth'\n","tokenizer = CCNeT5Tokenizer(vocab_path=vocab_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0Kj4LY3EjI3"},"outputs":[],"source":["\n","\n","# Load some of the images we have here and try to save them to the dataset\n","import os\n","from typing import List, Tuple\n","import numpy as np\n","from PIL import Image\n","import torch\n","import pandas as pd\n","import re\n","import json\n","from torch.utils.data import DataLoader\n","import random\n","import math\n","\n","class DatasetLoader:\n","    #def __init__(self, screenshots_path, dataset, start_index, end_index, dom_tokenizer, batch_size):\n","    def __init__(self, screenshots_path, dataset, dom_tokenizer, batch_size):\n","\n","        # Get the Subset of the dataset\n","        #self.start_index = start_index\n","        #self.end_index = end_index\n","        self.df = dataset['train'].to_pandas()##[self.start_index:self.end_index]\n","        # When creating the class, parse JSON files into a state-based list structure\n","        self.df['processed_states'] = self.df['processed_states'].apply(lambda x: json.loads(re.sub(r'\\b(True|False)\\b', lambda m: m.group(0).lower(), x.replace(\"'\", '\"'))))\n","        print(f'Dataset subset has {len(self.df)} rows')\n","\n","        self.screenshots_path = screenshots_path\n","        # If want to load them from the directory\n","        self.indices, found_screenshots = self.get_file_indices()\n","        # Or load the previously saved indices\n","        #self.indices = self.read_indices_from_file('/content/drive/MyDrive/WebAI/file_indices.txt')\n","        found_screenshots = len(self.indices)\n","        print(f'Found a total of {found_screenshots} state screenshots for {len(self.indices)} episodes.')\n","\n","        # Load all images from the drive\n","        #self.add_images_to_episodes()\n","        # Load N samples\n","\n","        self.dom_tokenizer = dom_tokenizer\n","        self.dom_seq_length = 492\n","        self.utterance_seq_length = 16\n","        self.task_name_seq_length = 4\n","\n","        self.target_action_type_seq_length = 1\n","        self.target_ref_seq_length = 1\n","        self.target_keydown_seq_length = 8 #32\n","\n","        # Size of the final layer output being composed of\n","        # 1 (action),\n","        # + 64 (ref) (\n","        # + 64x8 (keydown text times target_keydown_seq_length)\n","        self.model_output_size = 577\n","\n","        self.batch_size = batch_size\n","\n","        self.max_ref_nb = 500\n","\n","    # Add the images to the different episodes of the dataset\n","    def add_images_to_episodes_old(self):\n","        found_images = []\n","        i = self.start_index\n","        image_counter = 0\n","        while i \u003c self.end_index:\n","            if i % 100 == 0:\n","              print(f'Done: {i}')\n","            # Get K indices\n","            images = []\n","            try:\n","              state_indexes = self.indices[i]\n","              for state_index in state_indexes:\n","                  images.append(self.read_image(i, state_index))\n","                  image_counter += 1\n","              found_images.append(images)\n","            except:\n","              print(f'error {i}')\n","            i += 1\n","        # Add new column\n","        self.df['episode_images'] = found_images\n","        print(f'Found total of {image_counter} images in the loaded episodes.')\n","\n","    # Load the images in batches\n","    def add_images_to_episodes(self):\n","      found_images = []\n","      image_counter = 0\n","\n","      for i in range(self.start_index, self.end_index):\n","        if i%100 == 0:\n","          print(f'Done :{i}')\n","        try:\n","            state_indexes = self.indices[i]\n","            images = [self.read_image(i, state_index) for state_index in state_indexes]\n","            found_images.append(images)\n","            image_counter += len(images)\n","        except Exception as e:\n","            print(f'Error at index {i}: {str(e)}')\n","            # FFUUUCK NEED TO DO SOMETHING HERE IF ERROR...\n","            state_indexes = self.indices[i]\n","            for state_index in state_indexes:\n","              try:\n","                images.append(self.read_image(i, state_index))\n","                image_counter += 1\n","              except:\n","                'ehhh'\n","            found_images.append(images)\n","\n","\n","      self.df['episode_images'] = found_images\n","      print(f'Found a total of {image_counter} images in the loaded episodes.')\n","\n","\n","    def get_file_indices(self) -\u003e List[Tuple[int, int]]:\n","        indices = []\n","        for filename in os.listdir(self.screenshots_path):\n","            if filename.endswith('.png'):\n","                # Extract the N and K values from the filename\n","                N, K = filename.replace('sc_', '').replace('st_', '').replace('.png', '').split('_')\n","                N = int(N)\n","                K = int(K)\n","                indices.append((N, K))\n","\n","        # Turn the indices into a dictionary\n","        indices_dict = {}\n","        for (N, K) in indices:\n","            if N not in indices_dict:\n","                indices_dict[N] = [K]\n","            else:\n","                indices_dict[N].append(K)\n","                indices_dict[N] = sorted(indices_dict[N])\n","\n","        return indices_dict, len(indices)\n","\n","    def save_file_indices_to_file(self, dictionary, filename):\n","      with open(filename, 'w') as file:\n","          for key, value in dictionary.items():\n","              line = f\"{key}: {value}\\n\"\n","              file.write(line)\n","\n","    def read_indices_from_file(self, filename):\n","        dictionary = {}\n","        with open(filename, 'r') as file:\n","            for line in file:\n","                key, value = line.strip().split(': ')\n","                key = int(key)\n","                value = [int(num) for num in value.strip('[]').split(', ')]\n","                dictionary[key] = value\n","        return dictionary\n","\n","    import torch\n","\n","    # Reads images into tensor\n","    def read_image_old(self, N, K) -\u003e torch.Tensor:\n","        filename = f'sc_{N}_st_{K}.png'\n","        filepath = os.path.join(self.screenshots_path, filename)\n","        image = Image.open(filepath).convert('RGB')\n","        tensor = torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) / 255.0\n","        return tensor\n","\n","    # Pure NP instead of tensor\n","    def read_image(self, N, K) -\u003e np.array:\n","        filename = f'sc_{N}_st_{K}.png'\n","        filepath = os.path.join(self.screenshots_path, filename)\n","        image = Image.open(filepath).convert('RGB')\n","        array = np.transpose(np.array(image) / 255.0, (2, 0, 1))\n","        return array\n","\n","\n","    def save_image_png(self, N, K):\n","        loaded_image = self.read_image(N, K).numpy()\n","        print(f'Image has shape {loaded_image.shape}')\n","\n","        # scale the pixel values from [0,1] to [0, 255]\n","        array = np.clip(loaded_image * 255, 0, 255).astype('uint8')\n","        img = Image.fromarray(array.transpose(1, 2, 0), mode='RGB')\n","        img.save('screenshots/test_img.png')\n","\n","    # Turn our dataset into a series of state tensors ready for training.\n","    # We though ensure to clearly separate our different instances:\n","    # - todo: T5-Data (for later, depends between SL or RL current approach)\n","    # - RBG\n","    # - todo: Tokenized DOM\n","    # - todo: Task Instruction\n","    # They are separated because they are not going to be fed in the same manner into the model.\n","    # Basically into one single row.\n","    # todo: skip rows that have negative rewards\n","    def process_dataset(self):\n","        rbg_data = []\n","        dom_data = []\n","        utterance_data = []\n","        task_name_data = []\n","\n","        target_action = []\n","        target_refs = []\n","        target_keydown = []\n","\n","        episode_previous_actions = []\n","\n","        # Some metrics\n","        duplicated_images = 0\n","        cut_images = 0\n","\n","        for (index, row) in self.df.iterrows():\n","            if float(row['reward']) \u003c= 0: # Skips failed episodes\n","                continue\n","            if len(row['episode_images']) != len(row['processed_states']): # Skips episodes with no matching pictures amount and states\n","                len_processed_states = len(row['processed_states'])\n","                len_episode_images = len(row['episode_images'])\n","                print(f'Error: episode_images and processed_states don\\' have the same length for index {index}: len_episode_images={len_episode_images}, len_processed_states={len_processed_states}')\n","                # We just discard it, fuck it if it's just an individual row and only one image missing out of n states.\n","                # Data processing showed that only few states were missing from the zip and got fixed except a couple of them\n","\n","                # Ok actually fix it by cloning the last rgb\n","                if len_episode_images \u003c len_processed_states:\n","                  last_img = row['episode_images'][len(row['episode_images'])-1]\n","                  while len(row['episode_images']) \u003c len_processed_states:\n","                    row['episode_images'].append(last_img)\n","                    duplicated_images += 1\n","                else:\n","                  # Too few processed states for pictures, cut last pic.\n","                  # We keep the processed states over the generated screenshots\n","                  row['episode_images'] = row['episode_images'][:len_processed_states]\n","                  cut_images += len_episode_images - len_processed_states\n","                #continue\n","            for rbg in row['episode_images']:\n","                rbg = np.array(rbg, dtype='float64')\n","                #print(f'rbg shape: {rbg.shape}')\n","                rbg_data.append(rbg)\n","\n","\n","            # Process DOM Data\n","            for (index_state, state) in enumerate(row['processed_states']):\n","\n","                # Output Tokens\n","                if state['action_type'] == 'click': # Append boolean for action click or keydown\n","                    t_action = np.array([0])\n","                    target_action.append(t_action)\n","                else:\n","                    t_action = np.array([1])\n","                    target_action.append(t_action)\n","\n","                # Tokenize ref and appends to target\n","                #sr = state['refs']\n","                #print(f'state ref: {sr})\n","                #t_ref = np.array(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(str(state['refs'])), self.target_ref_seq_length))\n","                # NEW: don't tokenize with ITOS, pass ref value directly to reduce the vocab size\n","                if abs(int(state['refs'])) \u003e= self.max_ref_nb:\n","                  state_ref = self.max_ref_nb-1\n","                else:\n","                  state_ref = abs(int(state['refs']))\n","                if state_ref \u003c 0:\n","                  print(f'ERRROR state_ref: {state_ref}')\n","                  return\n","                t_ref = np.array(self.dom_tokenizer.truncate_pad_entry([state_ref], self.target_ref_seq_length))\n","                target_refs.append(t_ref)\n","\n","                # Tokenize target text and appends to target\n","                t_keydown = np.array(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(state['keydown_text']), self.target_keydown_seq_length))\n","                target_keydown.append(t_keydown)\n","\n","                # Input Tokens\n","                # Tokenize dom\n","                tokenized_dom = self.dom_tokenizer.tokenize_dom(state['dom'])\n","                tokenized_dom = self.dom_tokenizer.truncate_pad_entry(tokenized_dom, self.dom_seq_length)\n","                dom_data.append(tokenized_dom)\n","\n","                # Tokenize utterance\n","                tokenized_utterance = self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(row['utterance']), self.utterance_seq_length)\n","                utterance_data.append(tokenized_utterance)\n","\n","                # Tokenize task name\n","                tokenized_task_name = self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(row['task_name']), self.task_name_seq_length)\n","                task_name_data.append(tokenized_task_name)\n","\n","                # Deal with the previous action\n","                if index_state \u003c len(row['processed_states'])-1:\n","                    # Default one so empty for the first action\n","                    if index_state == 0:\n","                        episode_previous_actions.append([])\n","                        #episode_previous_actions.append(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(''), self.model_output_size))\n","\n","                    # Add previous actions one by one\n","                    episode_previous_actions.append([t_action, t_ref, t_keydown])\n","                elif index_state \u003e= len(row['processed_states'])-1 and index_state == 0:\n","                    episode_previous_actions.append([])\n","                # Else no action to add\n","\n","\n","\n","\n","        rbg_data = np.stack(rbg_data)\n","        dom_data = np.array(dom_data)\n","        utterance_data = np.array(utterance_data)\n","        task_name_data = np.array(task_name_data)\n","        target_action = np.array(target_action)\n","        target_refs = np.array(target_refs)\n","        target_keydown = np.array(target_keydown)\n","\n","        print(f'duplicated_images: {duplicated_images}, cut_images: {cut_images}')\n","\n","        return rbg_data, dom_data, utterance_data, task_name_data, target_action, target_refs, target_keydown, episode_previous_actions\n","\n","\n","    # Batchify the dataset\n","    def get_dataloder(self, dataset):\n","        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n","        return train_loader\n","\n","    # Creates random indexes to split and shuffle the test and training datasets\n","    def create_train_test_dataset(self, length):\n","        # Calculate the number of random indexes (10% of length)\n","        num_random_indexes = int(length * 0.1)\n","\n","        # Generate random indexes without replacement\n","        random_indexes = random.sample(range(length), num_random_indexes)\n","\n","        # Create train and test datasets\n","        train_dataset = []\n","        test_dataset = []\n","\n","        for i in range(length):\n","            if i in random_indexes:\n","                test_dataset.append(i)\n","            else:\n","                train_dataset.append(i)\n","\n","        # Shuffle the train and test datasets\n","        random.shuffle(train_dataset)\n","        random.shuffle(test_dataset)\n","\n","        return train_dataset, test_dataset\n","\n","    # Devise datasets\n","    def split_datasets(self, rbg_data, dom_data, utterance_data, task_name_data, target_action, target_refs, target_keydown, episode_previous_actions):\n","        train_indexes, test_indexes = self.create_train_test_dataset(length=len(rbg_data))\n","\n","        train_rgb = rbg_data[train_indexes]\n","        train_dom = dom_data[train_indexes]\n","        train_utterance = utterance_data[train_indexes]\n","        train_task_name = task_name_data[train_indexes]\n","        train_target_action = target_action[train_indexes]\n","        train_target_ref = target_refs[train_indexes]\n","        train_target_keydown = target_keydown[train_indexes]\n","        train_episode_previous_action = []\n","        for index in train_indexes:\n","            train_episode_previous_action.append(episode_previous_actions[index])\n","\n","        # Create a boolean mask of the indexes\n","        test_rgb = rbg_data[test_indexes]\n","        test_dom = dom_data[test_indexes]\n","        test_utterance = utterance_data[test_indexes]\n","        test_task_name = task_name_data[test_indexes]\n","        test_target_action = target_action[test_indexes]\n","        test_target_ref = target_refs[test_indexes]\n","        test_target_keydown = target_keydown[test_indexes]\n","        test_episode_previous_action = []\n","        for index in test_indexes:\n","            test_episode_previous_action.append(episode_previous_actions[index])\n","\n","        return train_rgb, train_dom, train_utterance, train_task_name, train_target_action, train_target_ref, \\\n","               train_target_keydown, train_episode_previous_action, test_rgb, test_dom, test_utterance, \\\n","               test_task_name, test_target_action, test_target_ref, test_target_keydown, test_episode_previous_action\n","\n","    # Get a dictionary with each task name, load N samples per task\n","    def get_predefined_indices(self, path, N, dataset):\n","      def load_dictionary_from_file(file_path):\n","        with open(file_path, 'r') as file:\n","            dictionary = json.load(file)\n","            return dictionary\n","      dict_tasks = load_dictionary_from_file(path)\n","      print(dict_tasks)\n","\n","      import random\n","\n","      def sample_N(x, N):\n","        if len(x) \u003c N:\n","          return x\n","        return random.sample(x, N)\n","\n","      downsampled_tasks = {}\n","      downsampled_tasks_counts = {}\n","      total_counts = 0\n","      for key, value in dict_tasks.items():\n","        samples = sample_N(value, N)\n","        total_counts += len(samples)\n","        downsampled_tasks[key] = sorted(samples)\n","        downsampled_tasks_counts[key] = len(samples)\n","\n","      print(f'Got total downsampled_tasks_counts: {total_counts}')\n","      print(f'downsampled_tasks: {downsampled_tasks}')\n","\n","      # Put them into a single list\n","      indices = []\n","      for key, samples in downsampled_tasks.items():\n","        #print(f'samples: {samples}')\n","        indices += samples\n","      indices = sorted(indices)\n","      print(f'found indices: {indices}')\n","      print(f'self indices: {self.indices}')\n","\n","      # Now load corresponding K values for RGB data from the original loaded indices\n","      indices_to_load = {}\n","      failed_sample_indexes = []\n","      for index in indices:\n","        try:\n","          samples = self.indices[index]\n","          #if index not in indices_to_load.keys():\n","            #indices_to_load[index] = []\n","          indices_to_load[index] = samples\n","        except Exception as e:\n","          print(f'Could not load samples at index {index}: {e}')\n","          failed_sample_indexes.append(index)\n","      print(f'failed_sample_indexes: {failed_sample_indexes}')\n","\n","      found_images = []\n","      image_counter = 0\n","      # Now load RGB pictures\n","      i = 0\n","\n","      # List our indices in order:\n","      ordered_indices = sorted(indices_to_load.keys())\n","      print(f'indices_to_load: {indices_to_load}')\n","      print(f'ordered_indices: {ordered_indices}')\n","\n","      for row_index in ordered_indices:\n","        state_indexes = indices_to_load[row_index]\n","        #print(f'state_indexes: {row_index}-{state_indexes}')\n","        if i % 50 == 0:\n","          print(f'Done: {i}')\n","        loaded_images_episode = 0\n","        episode_images = []\n","\n","        try:\n","          images = [self.read_image(row_index, state_index) for state_index in state_indexes]\n","          image_counter += len(images)\n","          loaded_images_episode += len(images)\n","          episode_images = images\n","        except Exception as e:\n","          episode_images = []\n","          print(f'Error at index {i}: {str(e)}')\n","          # FFUUUCK NEED TO DO SOMETHING HERE IF ERROR...\n","          for state_index in state_indexes:\n","            try:\n","              episode_images.append(self.read_image(row_index, state_index))\n","              image_counter += 1\n","              loaded_images_episode += 1\n","            except:\n","              'ehhh'\n","        found_images.append(episode_images)\n","        if loaded_images_episode != len(state_indexes):\n","          print(f'Not same length loaded_images_episode: {loaded_images_episode} - {len(state_indexes)} for index {row_index}')\n","        if len(episode_images) != len(state_indexes):\n","          print(f'Not same length episode_images: {len(episode_images)} - {len(state_indexes)} for index {row_index}')\n","\n","        i += 1\n","\n","\n","      print(f'Found a total of {image_counter} images in the loaded episodes.')\n","\n","\n","      # Reformat our self.df with the selected rows\n","      self.df = dataset.iloc[ordered_indices]\n","      # When creating the class, parse JSON files into a state-based list structure\n","      self.df['processed_states'] = self.df['processed_states'].apply(lambda x: json.loads(re.sub(r'\\b(True|False)\\b', lambda m: m.group(0).lower(), x.replace(\"'\", '\"'))))\n","      print(f'Dataset subset sampled has {len(self.df)} rows')\n","      print(f'found_images for total episodes: {len(found_images)}')\n","      self.df.loc[:, 'episode_images'] = found_images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23808,"status":"ok","timestamp":1687185219595,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"axjtmq5LEvKZ","outputId":"9963a246-8c43-47ca-b418-202a636b0d81"},"outputs":[{"data":{"text/plain":["[\"Cloning into 'miniwob_zip'...\",\n"," 'remote: Enumerating objects: 24148, done.\\x1b[K',\n"," 'remote: Counting objects:   0% (1/603)\\x1b[K',\n"," 'remote: Counting objects:   1% (7/603)\\x1b[K',\n"," 'remote: Counting objects:   2% (13/603)\\x1b[K',\n"," 'remote: Counting objects:   3% (19/603)\\x1b[K',\n"," 'remote: Counting objects:   4% (25/603)\\x1b[K',\n"," 'remote: Counting objects:   5% (31/603)\\x1b[K',\n"," 'remote: Counting objects:   6% (37/603)\\x1b[K',\n"," 'remote: Counting objects:   7% (43/603)\\x1b[K',\n"," 'remote: Counting objects:   8% (49/603)\\x1b[K',\n"," 'remote: Counting objects:   9% (55/603)\\x1b[K',\n"," 'remote: Counting objects:  10% (61/603)\\x1b[K',\n"," 'remote: Counting objects:  11% (67/603)\\x1b[K',\n"," 'remote: Counting objects:  12% (73/603)\\x1b[K',\n"," 'remote: Counting objects:  13% (79/603)\\x1b[K',\n"," 'remote: Counting objects:  14% (85/603)\\x1b[K',\n"," 'remote: Counting objects:  15% (91/603)\\x1b[K',\n"," 'remote: Counting objects:  16% (97/603)\\x1b[K',\n"," 'remote: Counting objects:  17% (103/603)\\x1b[K',\n"," 'remote: Counting objects:  18% (109/603)\\x1b[K',\n"," 'remote: Counting objects:  19% (115/603)\\x1b[K',\n"," 'remote: Counting objects:  20% (121/603)\\x1b[K',\n"," 'remote: Counting objects:  21% (127/603)\\x1b[K',\n"," 'remote: Counting objects:  22% (133/603)\\x1b[K',\n"," 'remote: Counting objects:  23% (139/603)\\x1b[K',\n"," 'remote: Counting objects:  24% (145/603)\\x1b[K',\n"," 'remote: Counting objects:  25% (151/603)\\x1b[K',\n"," 'remote: Counting objects:  26% (157/603)\\x1b[K',\n"," 'remote: Counting objects:  27% (163/603)\\x1b[K',\n"," 'remote: Counting objects:  28% (169/603)\\x1b[K',\n"," 'remote: Counting objects:  29% (175/603)\\x1b[K',\n"," 'remote: Counting objects:  30% (181/603)\\x1b[K',\n"," 'remote: Counting objects:  31% (187/603)\\x1b[K',\n"," 'remote: Counting objects:  32% (193/603)\\x1b[K',\n"," 'remote: Counting objects:  33% (199/603)\\x1b[K',\n"," 'remote: Counting objects:  34% (206/603)\\x1b[K',\n"," 'remote: Counting objects:  35% (212/603)\\x1b[K',\n"," 'remote: Counting objects:  36% (218/603)\\x1b[K',\n"," 'remote: Counting objects:  37% (224/603)\\x1b[K',\n"," 'remote: Counting objects:  38% (230/603)\\x1b[K',\n"," 'remote: Counting objects:  39% (236/603)\\x1b[K',\n"," 'remote: Counting objects:  40% (242/603)\\x1b[K',\n"," 'remote: Counting objects:  41% (248/603)\\x1b[K',\n"," 'remote: Counting objects:  42% (254/603)\\x1b[K',\n"," 'remote: Counting objects:  43% (260/603)\\x1b[K',\n"," 'remote: Counting objects:  44% (266/603)\\x1b[K',\n"," 'remote: Counting objects:  45% (272/603)\\x1b[K',\n"," 'remote: Counting objects:  46% (278/603)\\x1b[K',\n"," 'remote: Counting objects:  47% (284/603)\\x1b[K',\n"," 'remote: Counting objects:  48% (290/603)\\x1b[K',\n"," 'remote: Counting objects:  49% (296/603)\\x1b[K',\n"," 'remote: Counting objects:  50% (302/603)\\x1b[K',\n"," 'remote: Counting objects:  51% (308/603)\\x1b[K',\n"," 'remote: Counting objects:  52% (314/603)\\x1b[K',\n"," 'remote: Counting objects:  53% (320/603)\\x1b[K',\n"," 'remote: Counting objects:  54% (326/603)\\x1b[K',\n"," 'remote: Counting objects:  55% (332/603)\\x1b[K',\n"," 'remote: Counting objects:  56% (338/603)\\x1b[K',\n"," 'remote: Counting objects:  57% (344/603)\\x1b[K',\n"," 'remote: Counting objects:  58% (350/603)\\x1b[K',\n"," 'remote: Counting objects:  59% (356/603)\\x1b[K',\n"," 'remote: Counting objects:  60% (362/603)\\x1b[K',\n"," 'remote: Counting objects:  61% (368/603)\\x1b[K',\n"," 'remote: Counting objects:  62% (374/603)\\x1b[K',\n"," 'remote: Counting objects:  63% (380/603)\\x1b[K',\n"," 'remote: Counting objects:  64% (386/603)\\x1b[K',\n"," 'remote: Counting objects:  65% (392/603)\\x1b[K',\n"," 'remote: Counting objects:  66% (398/603)\\x1b[K',\n"," 'remote: Counting objects:  67% (405/603)\\x1b[K',\n"," 'remote: Counting objects:  68% (411/603)\\x1b[K',\n"," 'remote: Counting objects:  69% (417/603)\\x1b[K',\n"," 'remote: Counting objects:  70% (423/603)\\x1b[K',\n"," 'remote: Counting objects:  71% (429/603)\\x1b[K',\n"," 'remote: Counting objects:  72% (435/603)\\x1b[K',\n"," 'remote: Counting objects:  73% (441/603)\\x1b[K',\n"," 'remote: Counting objects:  74% (447/603)\\x1b[K',\n"," 'remote: Counting objects:  75% (453/603)\\x1b[K',\n"," 'remote: Counting objects:  76% (459/603)\\x1b[K',\n"," 'remote: Counting objects:  77% (465/603)\\x1b[K',\n"," 'remote: Counting objects:  78% (471/603)\\x1b[K',\n"," 'remote: Counting objects:  79% (477/603)\\x1b[K',\n"," 'remote: Counting objects:  80% (483/603)\\x1b[K',\n"," 'remote: Counting objects:  81% (489/603)\\x1b[K',\n"," 'remote: Counting objects:  82% (495/603)\\x1b[K',\n"," 'remote: Counting objects:  83% (501/603)\\x1b[K',\n"," 'remote: Counting objects:  84% (507/603)\\x1b[K',\n"," 'remote: Counting objects:  85% (513/603)\\x1b[K',\n"," 'remote: Counting objects:  86% (519/603)\\x1b[K',\n"," 'remote: Counting objects:  87% (525/603)\\x1b[K',\n"," 'remote: Counting objects:  88% (531/603)\\x1b[K',\n"," 'remote: Counting objects:  89% (537/603)\\x1b[K',\n"," 'remote: Counting objects:  90% (543/603)\\x1b[K',\n"," 'remote: Counting objects:  91% (549/603)\\x1b[K',\n"," 'remote: Counting objects:  92% (555/603)\\x1b[K',\n"," 'remote: Counting objects:  93% (561/603)\\x1b[K',\n"," 'remote: Counting objects:  94% (567/603)\\x1b[K',\n"," 'remote: Counting objects:  95% (573/603)\\x1b[K',\n"," 'remote: Counting objects:  96% (579/603)\\x1b[K',\n"," 'remote: Counting objects:  97% (585/603)\\x1b[K',\n"," 'remote: Counting objects:  98% (591/603)\\x1b[K',\n"," 'remote: Counting objects:  99% (597/603)\\x1b[K',\n"," 'remote: Counting objects: 100% (603/603)\\x1b[K',\n"," 'remote: Counting objects: 100% (603/603), done.\\x1b[K',\n"," 'remote: Compressing objects:   0% (1/601)\\x1b[K',\n"," 'remote: Compressing objects:   1% (7/601)\\x1b[K',\n"," 'remote: Compressing objects:   2% (13/601)\\x1b[K',\n"," 'remote: Compressing objects:   3% (19/601)\\x1b[K',\n"," 'remote: Compressing objects:   4% (25/601)\\x1b[K',\n"," 'remote: Compressing objects:   5% (31/601)\\x1b[K',\n"," 'remote: Compressing objects:   6% (37/601)\\x1b[K',\n"," 'remote: Compressing objects:   7% (43/601)\\x1b[K',\n"," 'remote: Compressing objects:   8% (49/601)\\x1b[K',\n"," 'remote: Compressing objects:   9% (55/601)\\x1b[K',\n"," 'remote: Compressing objects:  10% (61/601)\\x1b[K',\n"," 'remote: Compressing objects:  11% (67/601)\\x1b[K',\n"," 'remote: Compressing objects:  12% (73/601)\\x1b[K',\n"," 'remote: Compressing objects:  13% (79/601)\\x1b[K',\n"," 'remote: Compressing objects:  14% (85/601)\\x1b[K',\n"," 'remote: Compressing objects:  15% (91/601)\\x1b[K',\n"," 'remote: Compressing objects:  16% (97/601)\\x1b[K',\n"," 'remote: Compressing objects:  17% (103/601)\\x1b[K',\n"," 'remote: Compressing objects:  18% (109/601)\\x1b[K',\n"," 'remote: Compressing objects:  19% (115/601)\\x1b[K',\n"," 'remote: Compressing objects:  20% (121/601)\\x1b[K',\n"," 'remote: Compressing objects:  21% (127/601)\\x1b[K',\n"," 'remote: Compressing objects:  22% (133/601)\\x1b[K',\n"," 'remote: Compressing objects:  23% (139/601)\\x1b[K',\n"," 'remote: Compressing objects:  24% (145/601)\\x1b[K',\n"," 'remote: Compressing objects:  25% (151/601)\\x1b[K',\n"," 'remote: Compressing objects:  26% (157/601)\\x1b[K',\n"," 'remote: Compressing objects:  27% (163/601)\\x1b[K',\n"," 'remote: Compressing objects:  28% (169/601)\\x1b[K',\n"," 'remote: Compressing objects:  29% (175/601)\\x1b[K',\n"," 'remote: Compressing objects:  30% (181/601)\\x1b[K',\n"," 'remote: Compressing objects:  31% (187/601)\\x1b[K',\n"," 'remote: Compressing objects:  32% (193/601)\\x1b[K',\n"," 'remote: Compressing objects:  33% (199/601)\\x1b[K',\n"," 'remote: Compressing objects:  34% (205/601)\\x1b[K',\n"," 'remote: Compressing objects:  35% (211/601)\\x1b[K',\n"," 'remote: Compressing objects:  36% (217/601)\\x1b[K',\n"," 'remote: Compressing objects:  37% (223/601)\\x1b[K',\n"," 'remote: Compressing objects:  38% (229/601)\\x1b[K',\n"," 'remote: Compressing objects:  39% (235/601)\\x1b[K',\n"," 'remote: Compressing objects:  40% (241/601)\\x1b[K',\n"," 'remote: Compressing objects:  41% (247/601)\\x1b[K',\n"," 'remote: Compressing objects:  42% (253/601)\\x1b[K',\n"," 'remote: Compressing objects:  43% (259/601)\\x1b[K',\n"," 'remote: Compressing objects:  44% (265/601)\\x1b[K',\n"," 'remote: Compressing objects:  45% (271/601)\\x1b[K',\n"," 'remote: Compressing objects:  46% (277/601)\\x1b[K',\n"," 'remote: Compressing objects:  47% (283/601)\\x1b[K',\n"," 'remote: Compressing objects:  48% (289/601)\\x1b[K',\n"," 'remote: Compressing objects:  49% (295/601)\\x1b[K',\n"," 'remote: Compressing objects:  50% (301/601)\\x1b[K',\n"," 'remote: Compressing objects:  51% (307/601)\\x1b[K',\n"," 'remote: Compressing objects:  52% (313/601)\\x1b[K',\n"," 'remote: Compressing objects:  53% (319/601)\\x1b[K',\n"," 'remote: Compressing objects:  54% (325/601)\\x1b[K',\n"," 'remote: Compressing objects:  55% (331/601)\\x1b[K',\n"," 'remote: Compressing objects:  56% (337/601)\\x1b[K',\n"," 'remote: Compressing objects:  57% (343/601)\\x1b[K',\n"," 'remote: Compressing objects:  58% (349/601)\\x1b[K',\n"," 'remote: Compressing objects:  59% (355/601)\\x1b[K',\n"," 'remote: Compressing objects:  60% (361/601)\\x1b[K',\n"," 'remote: Compressing objects:  61% (367/601)\\x1b[K',\n"," 'remote: Compressing objects:  62% (373/601)\\x1b[K',\n"," 'remote: Compressing objects:  63% (379/601)\\x1b[K',\n"," 'remote: Compressing objects:  64% (385/601)\\x1b[K',\n"," 'remote: Compressing objects:  65% (391/601)\\x1b[K',\n"," 'remote: Compressing objects:  66% (397/601)\\x1b[K',\n"," 'remote: Compressing objects:  67% (403/601)\\x1b[K',\n"," 'remote: Compressing objects:  68% (409/601)\\x1b[K',\n"," 'remote: Compressing objects:  69% (415/601)\\x1b[K',\n"," 'remote: Compressing objects:  70% (421/601)\\x1b[K',\n"," 'remote: Compressing objects:  71% (427/601)\\x1b[K',\n"," 'remote: Compressing objects:  72% (433/601)\\x1b[K',\n"," 'remote: Compressing objects:  73% (439/601)\\x1b[K',\n"," 'remote: Compressing objects:  74% (445/601)\\x1b[K',\n"," 'remote: Compressing objects:  75% (451/601)\\x1b[K',\n"," 'remote: Compressing objects:  76% (457/601)\\x1b[K',\n"," 'remote: Compressing objects:  77% (463/601)\\x1b[K',\n"," 'remote: Compressing objects:  78% (469/601)\\x1b[K',\n"," 'remote: Compressing objects:  79% (475/601)\\x1b[K',\n"," 'remote: Compressing objects:  80% (481/601)\\x1b[K',\n"," 'remote: Compressing objects:  81% (487/601)\\x1b[K',\n"," 'remote: Compressing objects:  82% (493/601)\\x1b[K',\n"," 'remote: Compressing objects:  83% (499/601)\\x1b[K',\n"," 'remote: Compressing objects:  84% (505/601)\\x1b[K',\n"," 'remote: Compressing objects:  85% (511/601)\\x1b[K',\n"," 'remote: Compressing objects:  86% (517/601)\\x1b[K',\n"," 'remote: Compressing objects:  87% (523/601)\\x1b[K',\n"," 'remote: Compressing objects:  88% (529/601)\\x1b[K',\n"," 'remote: Compressing objects:  89% (535/601)\\x1b[K',\n"," 'remote: Compressing objects:  90% (541/601)\\x1b[K',\n"," 'remote: Compressing objects:  91% (547/601)\\x1b[K',\n"," 'remote: Compressing objects:  92% (553/601)\\x1b[K',\n"," 'remote: Compressing objects:  93% (559/601)\\x1b[K',\n"," 'remote: Compressing objects:  94% (565/601)\\x1b[K',\n"," 'remote: Compressing objects:  95% (571/601)\\x1b[K',\n"," 'remote: Compressing objects:  96% (577/601)\\x1b[K',\n"," 'remote: Compressing objects:  97% (583/601)\\x1b[K',\n"," 'remote: Compressing objects:  98% (589/601)\\x1b[K',\n"," 'remote: Compressing objects:  99% (595/601)\\x1b[K',\n"," 'remote: Compressing objects: 100% (601/601)\\x1b[K',\n"," 'remote: Compressing objects: 100% (601/601), done.\\x1b[K',\n"," 'Receiving objects:   0% (1/24148)',\n"," 'Receiving objects:   0% (9/24148), 29.13 MiB | 29.15 MiB/s',\n"," 'Receiving objects:   0% (9/24148), 62.98 MiB | 31.50 MiB/s',\n"," 'Receiving objects:   1% (242/24148), 79.52 MiB | 31.82 MiB/s',\n"," 'Receiving objects:   2% (483/24148), 79.52 MiB | 31.82 MiB/s',\n"," 'Receiving objects:   3% (725/24148), 79.52 MiB | 31.82 MiB/s',\n"," 'Receiving objects:   4% (966/24148), 79.52 MiB | 31.82 MiB/s',\n"," 'Receiving objects:   5% (1208/24148), 79.52 MiB | 31.82 MiB/s',\n"," 'Receiving objects:   5% (1321/24148), 79.52 MiB | 31.82 MiB/s',\n"," 'Receiving objects:   6% (1449/24148), 95.77 MiB | 31.93 MiB/s',\n"," 'Receiving objects:   7% (1691/24148), 95.77 MiB | 31.93 MiB/s',\n"," 'Receiving objects:   8% (1932/24148), 95.77 MiB | 31.93 MiB/s',\n"," 'Receiving objects:   9% (2174/24148), 95.77 MiB | 31.93 MiB/s',\n"," 'Receiving objects:  10% (2415/24148), 95.77 MiB | 31.93 MiB/s',\n"," 'Receiving objects:  11% (2657/24148), 95.77 MiB | 31.93 MiB/s',\n"," 'Receiving objects:  12% (2898/24148), 95.77 MiB | 31.93 MiB/s',\n"," 'Receiving objects:  13% (3140/24148), 112.51 MiB | 32.15 MiB/s',\n"," 'Receiving objects:  14% (3381/24148), 112.51 MiB | 32.15 MiB/s',\n"," 'Receiving objects:  15% (3623/24148), 112.51 MiB | 32.15 MiB/s',\n"," 'Receiving objects:  16% (3864/24148), 112.51 MiB | 32.15 MiB/s',\n"," 'Receiving objects:  17% (4106/24148), 112.51 MiB | 32.15 MiB/s',\n"," 'Receiving objects:  18% (4347/24148), 112.51 MiB | 32.15 MiB/s',\n"," 'Receiving objects:  18% (4497/24148), 112.51 MiB | 32.15 MiB/s',\n"," 'Receiving objects:  19% (4589/24148), 129.55 MiB | 32.39 MiB/s',\n"," 'Receiving objects:  20% (4830/24148), 129.55 MiB | 32.39 MiB/s',\n"," 'Receiving objects:  21% (5072/24148), 129.55 MiB | 32.39 MiB/s',\n"," 'Receiving objects:  22% (5313/24148), 129.55 MiB | 32.39 MiB/s',\n"," 'Receiving objects:  23% (5555/24148), 129.55 MiB | 32.39 MiB/s',\n"," 'Receiving objects:  24% (5796/24148), 129.55 MiB | 32.39 MiB/s',\n"," 'Receiving objects:  25% (6037/24148), 144.85 MiB | 32.20 MiB/s',\n"," 'Receiving objects:  26% (6279/24148), 144.85 MiB | 32.20 MiB/s',\n"," 'Receiving objects:  27% (6520/24148), 144.85 MiB | 32.20 MiB/s',\n"," 'Receiving objects:  28% (6762/24148), 144.85 MiB | 32.20 MiB/s',\n"," 'Receiving objects:  28% (6937/24148), 160.61 MiB | 33.01 MiB/s',\n"," 'Receiving objects:  29% (7003/24148), 160.61 MiB | 33.01 MiB/s',\n"," 'Receiving objects:  30% (7245/24148), 160.61 MiB | 33.01 MiB/s',\n"," 'Receiving objects:  31% (7486/24148), 160.61 MiB | 33.01 MiB/s',\n"," 'Receiving objects:  32% (7728/24148), 160.61 MiB | 33.01 MiB/s',\n"," 'Receiving objects:  33% (7969/24148), 160.61 MiB | 33.01 MiB/s',\n"," 'Receiving objects:  34% (8211/24148), 160.61 MiB | 33.01 MiB/s',\n"," 'Receiving objects:  35% (8452/24148), 176.27 MiB | 32.70 MiB/s',\n"," 'Receiving objects:  36% (8694/24148), 176.27 MiB | 32.70 MiB/s',\n"," 'Receiving objects:  37% (8935/24148), 176.27 MiB | 32.70 MiB/s',\n"," 'Receiving objects:  38% (9177/24148), 176.27 MiB | 32.70 MiB/s',\n"," 'Receiving objects:  39% (9418/24148), 176.27 MiB | 32.70 MiB/s',\n"," 'Receiving objects:  40% (9660/24148), 176.27 MiB | 32.70 MiB/s',\n"," 'Receiving objects:  40% (9725/24148), 176.27 MiB | 32.70 MiB/s',\n"," 'Receiving objects:  41% (9901/24148), 191.88 MiB | 32.44 MiB/s',\n"," 'Receiving objects:  42% (10143/24148), 191.88 MiB | 32.44 MiB/s',\n"," 'Receiving objects:  43% (10384/24148), 191.88 MiB | 32.44 MiB/s',\n"," 'Receiving objects:  44% (10626/24148), 191.88 MiB | 32.44 MiB/s',\n"," 'Receiving objects:  45% (10867/24148), 208.22 MiB | 32.28 MiB/s',\n"," 'Receiving objects:  46% (11109/24148), 208.22 MiB | 32.28 MiB/s',\n"," 'Receiving objects:  47% (11350/24148), 208.22 MiB | 32.28 MiB/s',\n"," 'Receiving objects:  48% (11592/24148), 208.22 MiB | 32.28 MiB/s',\n"," 'Receiving objects:  48% (11716/24148), 208.22 MiB | 32.28 MiB/s',\n"," 'Receiving objects:  49% (11833/24148), 224.34 MiB | 32.18 MiB/s',\n"," 'Receiving objects:  50% (12074/24148), 224.34 MiB | 32.18 MiB/s',\n"," 'Receiving objects:  51% (12316/24148), 224.34 MiB | 32.18 MiB/s',\n"," 'Receiving objects:  52% (12557/24148), 224.34 MiB | 32.18 MiB/s',\n"," 'Receiving objects:  53% (12799/24148), 238.58 MiB | 31.74 MiB/s',\n"," 'Receiving objects:  54% (13040/24148), 238.58 MiB | 31.74 MiB/s',\n"," 'Receiving objects:  55% (13282/24148), 238.58 MiB | 31.74 MiB/s',\n"," 'Receiving objects:  55% (13361/24148), 238.58 MiB | 31.74 MiB/s',\n"," 'Receiving objects:  56% (13523/24148), 250.79 MiB | 30.73 MiB/s',\n"," 'Receiving objects:  57% (13765/24148), 250.79 MiB | 30.73 MiB/s',\n"," 'Receiving objects:  58% (14006/24148), 250.79 MiB | 30.73 MiB/s',\n"," 'Receiving objects:  59% (14248/24148), 262.91 MiB | 29.64 MiB/s',\n"," 'Receiving objects:  60% (14489/24148), 262.91 MiB | 29.64 MiB/s',\n"," 'Receiving objects:  61% (14731/24148), 262.91 MiB | 29.64 MiB/s',\n"," 'Receiving objects:  61% (14905/24148), 262.91 MiB | 29.64 MiB/s',\n"," 'Receiving objects:  62% (14972/24148), 275.25 MiB | 28.95 MiB/s',\n"," 'Receiving objects:  63% (15214/24148), 275.25 MiB | 28.95 MiB/s',\n"," 'Receiving objects:  64% (15455/24148), 275.25 MiB | 28.95 MiB/s',\n"," 'Receiving objects:  65% (15697/24148), 275.25 MiB | 28.95 MiB/s',\n"," 'Receiving objects:  66% (15938/24148), 288.12 MiB | 28.31 MiB/s',\n"," 'Receiving objects:  67% (16180/24148), 288.12 MiB | 28.31 MiB/s',\n"," 'Receiving objects:  68% (16421/24148), 288.12 MiB | 28.31 MiB/s',\n"," 'Receiving objects:  68% (16461/24148), 288.12 MiB | 28.31 MiB/s',\n"," 'Receiving objects:  69% (16663/24148), 300.23 MiB | 27.52 MiB/s',\n"," 'Receiving objects:  70% (16904/24148), 300.23 MiB | 27.52 MiB/s',\n"," 'Receiving objects:  71% (17146/24148), 300.23 MiB | 27.52 MiB/s',\n"," 'Receiving objects:  72% (17387/24148), 313.05 MiB | 26.91 MiB/s',\n"," 'Receiving objects:  73% (17629/24148), 313.05 MiB | 26.91 MiB/s',\n"," 'Receiving objects:  74% (17870/24148), 313.05 MiB | 26.91 MiB/s',\n"," 'Receiving objects:  75% (18111/24148), 313.05 MiB | 26.91 MiB/s',\n"," 'Receiving objects:  75% (18138/24148), 313.05 MiB | 26.91 MiB/s',\n"," 'Receiving objects:  76% (18353/24148), 327.03 MiB | 26.38 MiB/s',\n"," 'Receiving objects:  77% (18594/24148), 327.03 MiB | 26.38 MiB/s',\n"," 'Receiving objects:  78% (18836/24148), 327.03 MiB | 26.38 MiB/s',\n"," 'Receiving objects:  79% (19077/24148), 339.62 MiB | 25.59 MiB/s',\n"," 'Receiving objects:  80% (19319/24148), 339.62 MiB | 25.59 MiB/s',\n"," 'Receiving objects:  81% (19560/24148), 339.62 MiB | 25.59 MiB/s',\n"," 'Receiving objects:  81% (19711/24148), 339.62 MiB | 25.59 MiB/s',\n"," 'Receiving objects:  82% (19802/24148), 351.97 MiB | 25.18 MiB/s',\n"," 'Receiving objects:  83% (20043/24148), 351.97 MiB | 25.18 MiB/s',\n"," 'Receiving objects:  84% (20285/24148), 351.97 MiB | 25.18 MiB/s',\n"," 'Receiving objects:  85% (20526/24148), 364.76 MiB | 25.30 MiB/s',\n"," 'Receiving objects:  86% (20768/24148), 364.76 MiB | 25.30 MiB/s',\n"," 'Receiving objects:  87% (21009/24148), 364.76 MiB | 25.30 MiB/s',\n"," 'Receiving objects:  88% (21251/24148), 364.76 MiB | 25.30 MiB/s',\n"," 'Receiving objects:  88% (21340/24148), 364.76 MiB | 25.30 MiB/s',\n"," 'Receiving objects:  89% (21492/24148), 376.61 MiB | 25.24 MiB/s',\n"," 'Receiving objects:  90% (21734/24148), 376.61 MiB | 25.24 MiB/s',\n"," 'Receiving objects:  91% (21975/24148), 376.61 MiB | 25.24 MiB/s',\n"," 'Receiving objects:  92% (22217/24148), 376.61 MiB | 25.24 MiB/s',\n"," 'Receiving objects:  93% (22458/24148), 386.58 MiB | 24.73 MiB/s',\n"," 'Receiving objects:  94% (22700/24148), 386.58 MiB | 24.73 MiB/s',\n"," 'Receiving objects:  95% (22941/24148), 386.58 MiB | 24.73 MiB/s',\n"," 'Receiving objects:  95% (23166/24148), 386.58 MiB | 24.73 MiB/s',\n"," 'Receiving objects:  96% (23183/24148), 398.98 MiB | 24.63 MiB/s',\n"," 'Receiving objects:  97% (23424/24148), 398.98 MiB | 24.63 MiB/s',\n"," 'Receiving objects:  98% (23666/24148), 398.98 MiB | 24.63 MiB/s',\n"," 'Receiving objects:  99% (23907/24148), 398.98 MiB | 24.63 MiB/s',\n"," 'remote: Total 24148 (delta 3), reused 599 (delta 2), pack-reused 23545\\x1b[K',\n"," 'Receiving objects: 100% (24148/24148), 398.98 MiB | 24.63 MiB/s',\n"," 'Receiving objects: 100% (24148/24148), 411.46 MiB | 28.47 MiB/s, done.',\n"," 'Resolving deltas:   0% (0/10)',\n"," 'Resolving deltas:  90% (9/10)',\n"," 'Resolving deltas: 100% (10/10)',\n"," 'Resolving deltas: 100% (10/10), done.',\n"," 'Updating files:  12% (4860/38067)',\n"," 'Updating files:  13% (4949/38067)',\n"," 'Updating files:  14% (5330/38067)',\n"," 'Updating files:  15% (5711/38067)',\n"," 'Updating files:  16% (6091/38067)',\n"," 'Updating files:  17% (6472/38067)',\n"," 'Updating files:  18% (6853/38067)',\n"," 'Updating files:  19% (7233/38067)',\n"," 'Updating files:  20% (7614/38067)',\n"," 'Updating files:  21% (7995/38067)',\n"," 'Updating files:  22% (8375/38067)',\n"," 'Updating files:  23% (8756/38067)',\n"," 'Updating files:  24% (9137/38067)',\n"," 'Updating files:  25% (9517/38067)',\n"," 'Updating files:  26% (9898/38067)',\n"," 'Updating files:  27% (10279/38067)',\n"," 'Updating files:  28% (10659/38067)',\n"," 'Updating files:  29% (11040/38067)',\n"," 'Updating files:  30% (11421/38067)',\n"," 'Updating files:  31% (11801/38067)',\n"," 'Updating files:  32% (12182/38067)',\n"," 'Updating files:  32% (12184/38067)',\n"," 'Updating files:  33% (12563/38067)',\n"," 'Updating files:  34% (12943/38067)',\n"," 'Updating files:  35% (13324/38067)',\n"," 'Updating files:  36% (13705/38067)',\n"," 'Updating files:  37% (14085/38067)',\n"," 'Updating files:  38% (14466/38067)',\n"," 'Updating files:  39% (14847/38067)',\n"," 'Updating files:  40% (15227/38067)',\n"," 'Updating files:  41% (15608/38067)',\n"," 'Updating files:  42% (15989/38067)',\n"," 'Updating files:  43% (16369/38067)',\n"," 'Updating files:  44% (16750/38067)',\n"," 'Updating files:  45% (17131/38067)',\n"," 'Updating files:  46% (17511/38067)',\n"," 'Updating files:  47% (17892/38067)',\n"," 'Updating files:  48% (18273/38067)',\n"," 'Updating files:  49% (18653/38067)',\n"," 'Updating files:  50% (19034/38067)',\n"," 'Updating files:  50% (19401/38067)',\n"," 'Updating files:  51% (19415/38067)',\n"," 'Updating files:  52% (19795/38067)',\n"," 'Updating files:  53% (20176/38067)',\n"," 'Updating files:  54% (20557/38067)',\n"," 'Updating files:  55% (20937/38067)',\n"," 'Updating files:  56% (21318/38067)',\n"," 'Updating files:  57% (21699/38067)',\n"," 'Updating files:  58% (22079/38067)',\n"," 'Updating files:  59% (22460/38067)',\n"," 'Updating files:  60% (22841/38067)',\n"," 'Updating files:  61% (23221/38067)',\n"," 'Updating files:  62% (23602/38067)',\n"," 'Updating files:  63% (23983/38067)',\n"," 'Updating files:  63% (24310/38067)',\n"," 'Updating files:  64% (24363/38067)',\n"," 'Updating files:  64% (24420/38067)',\n"," 'Updating files:  65% (24744/38067)',\n"," 'Updating files:  66% (25125/38067)',\n"," 'Updating files:  67% (25505/38067)',\n"," 'Updating files:  68% (25886/38067)',\n"," 'Updating files:  68% (25940/38067)',\n"," 'Updating files:  69% (26267/38067)',\n"," 'Updating files:  70% (26647/38067)',\n"," 'Updating files:  71% (27028/38067)',\n"," 'Updating files:  72% (27409/38067)',\n"," 'Updating files:  73% (27789/38067)',\n"," 'Updating files:  74% (28170/38067)',\n"," 'Updating files:  75% (28551/38067)',\n"," 'Updating files:  76% (28931/38067)',\n"," 'Updating files:  77% (29312/38067)',\n"," 'Updating files:  78% (29693/38067)',\n"," 'Updating files:  79% (30073/38067)',\n"," 'Updating files:  80% (30454/38067)',\n"," 'Updating files:  81% (30835/38067)',\n"," 'Updating files:  82% (31215/38067)',\n"," 'Updating files:  83% (31596/38067)',\n"," 'Updating files:  84% (31977/38067)',\n"," 'Updating files:  84% (32356/38067)',\n"," 'Updating files:  85% (32357/38067)',\n"," 'Updating files:  86% (32738/38067)',\n"," 'Updating files:  87% (33119/38067)',\n"," 'Updating files:  88% (33499/38067)',\n"," 'Updating files:  89% (33880/38067)',\n"," 'Updating files:  90% (34261/38067)',\n"," 'Updating files:  91% (34641/38067)',\n"," 'Updating files:  92% (35022/38067)',\n"," 'Updating files:  93% (35403/38067)',\n"," 'Updating files:  94% (35783/38067)',\n"," 'Updating files:  95% (36164/38067)',\n"," 'Updating files:  96% (36545/38067)',\n"," 'Updating files:  97% (36925/38067)',\n"," 'Updating files:  98% (37306/38067)',\n"," 'Updating files:  99% (37687/38067)',\n"," 'Updating files: 100% (38067/38067)',\n"," 'Updating files: 100% (38067/38067), done.']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["!git clone https://github.com/LucasStill/miniwob_zip.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"elapsed":10817,"status":"ok","timestamp":1687185253118,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"yPW0_OvKE7Lb","outputId":"8a56936f-a966-4e6f-ac6e-d2ab9c9b2e5c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0ef7fd5c0d14ab8b8a960b4ebdc0f5b","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/548 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/LucasThil___parquet/LucasThil--randomized_clean_miniwob_episodes_v2-49501296fc0102ba/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"807da2eac6d84e2a8b76250d818b7cbd","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"420ced3fc1a444d09de92da24e25bac2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/55.1M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be0fdbdd9c3d4d1c8a154174c5515cb1","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85ad0f1cf7e44870939edfbbd85ff4e8","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/13412 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/LucasThil___parquet/LucasThil--randomized_clean_miniwob_episodes_v2-49501296fc0102ba/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22109a456e604971ac7b3ebab1630e47","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","from datasets import load_dataset\n","dataset = load_dataset(\"LucasThil/randomized_clean_miniwob_episodes_v2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46207,"status":"ok","timestamp":1687185299323,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"LGIrwFNGEnIm","outputId":"780782a5-6ba3-410a-874c-028ab377ecbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset subset has 13412 rows\n","Found a total of 9184 state screenshots for 9184 episodes.\n"]}],"source":["#screenshots_path = '/content/drive/MyDrive/WebAI/screenshot_indexes'\n","#screenshots_path = '/content/sample_data_screenshots'\n","screenshots_path = '/content/miniwob_zip'\n","batch_size = 1\n","# NOTE!!!: Check if need to rerun the indices on all pictures instead of range\n","#dataset_loader = DatasetLoader(screenshots_path=screenshots_path, dataset=dataset, start_index=0, end_index=20, dom_tokenizer=tokenizer, batch_size=batch_size)\n","dataset_loader = DatasetLoader(screenshots_path=screenshots_path, dataset=dataset, dom_tokenizer=tokenizer, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWkIW8YBC_Nr"},"outputs":[],"source":["\n","# Some model classes\n","\n","'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3,3),\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=14):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 3\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=2)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        #self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        #out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(x)\n","        dl1 = out.shape\n","        out = self.layer2(out)\n","        dl2 = out.shape\n","        out = self.layer3(out)\n","        dl3 = out.shape\n","        out = self.layer4(out)\n","        dl4 = out.shape\n","        #out = F.avg_pool2d(out, 4)\n","        #out = out.view(out.size(0), -1)\n","\n","        # Flatten (batch size, channels, features)\n","        out = out.view(batch_size, 512, 140)\n","        out_1_s = out.shape\n","        # Might have to change the last layer to obtain our 14x11 feature vector?\n","        #out = self.linear(out)\n","        #print(f'out_1_s: {out_1_s}, out: {out.shape}')\n","        #print(f'dl1: {dl1}, dl2: {dl2}, dl3: {dl3}, dl4: {dl4}, out: {out_1_s}')\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","# ----------------------------------\n","# Cross Attention Model for Language\n","class CrossAttentionModelLanguage(nn.Module):\n","    # Bigger2: Changed hidden_dim from 140 to 1\n","    def __init__(self, input_dim=64, hidden_dim=140, num_heads=4):\n","        super(CrossAttentionModelLanguage, self).__init__()\n","\n","        self.attention = nn.MultiheadAttention(input_dim, num_heads)\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","\n","    def forward(self, input):\n","        # Reshape the input to (sequence_length, batch_size, input_dim)\n","        input = input.permute(1, 0, 2)\n","\n","        # Apply cross-attention\n","        output, _ = self.attention(input, input, input)\n","\n","        # Reshape the output to (batch_size, sequence_length, input_dim)\n","        output = output.permute(1, 0, 2)\n","\n","        # Apply linear transformation\n","        output = self.linear(output)\n","\n","        return output\n","\n","\n","#--------------------------------\n","# Multimodal Transformer Network\n","from torch.nn import Transformer\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self):\n","        super(TransformerNetwork, self).__init__()\n","\n","        self.embedding_dim = 512\n","        self.num_layers = 1\n","        self.num_heads = 8\n","\n","        # Bigger2: 512, self.embedding_dim)# version1: 141\n","        self.embedding = nn.Linear(512, self.embedding_dim)\n","        self.transformer = Transformer(\n","            d_model=self.embedding_dim,\n","            nhead=self.num_heads,\n","            num_encoder_layers=self.num_layers,\n","            num_decoder_layers=self.num_layers\n","        )\n","        #self.output_layer = nn.Linear(self.embedding_dim, 1)\n","\n","    def forward(self, input_tensor):\n","        batch_size = input_tensor.size(0)\n","\n","        embedded = self.embedding(input_tensor)\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        output = self.transformer(embedded, embedded)\n","        output = output.permute(1, 0, 2)\n","        #output = self.output_layer(output)\n","\n","        return output.squeeze(2)\n","\n","#-------------------\n","# LSTM Netowkr Part\n","class TwoLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(TwoLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        # Note: we use a Linear layer for dimension adjustment.\n","        # Indeed, the previous multi-modal transformer outputs a 1024 + prev_action length tensor,\n","        # and here the LSTM layers have a 512 dim where we also need to bypass residual connections.\n","        # We use the linear layer below to have a matching size with the input/output of these LSTMs\n","        # in order to concatenate them.\n","        self.linear = nn.Linear(input_size, hidden_size)\n","\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2, batch_first=True)\n","\n","        # This linear layer is for the final mapping\n","        #self.fc = nn.Linear(hidden_size, output_size) #fully connected last layer\n","\n","    def forward(self, input):\n","        # Perform dimension adjustment using the linear layer\n","        adjusted_input = self.linear(input)\n","        #print(f'adjusted_input: {adjusted_input.shape}')\n","\n","        # Perform the forward pass through each LSTM layer\n","        output, _hidden_state = self.lstm(adjusted_input)\n","\n","        # Perform residual connections between LSTM layers\n","        residual_output = adjusted_input + output\n","\n","        # Extract the hidden state of the last LSTM layer after residual connections\n","        last_hidden_state = residual_output[:, -1, :]\n","\n","        #out = self.fc(last_hidden_state)\n","        return last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1556,"status":"ok","timestamp":1687185370014,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"Spzt3giMamOe","outputId":"cebecbb1-d55d-485f-bd94-0399a3f560bf"},"outputs":[{"data":{"text/plain":["CCNeT5(\n","  (embedding): Embedding(1591, 64)\n","  (rgb_model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","  )\n","  (language_model): CrossAttentionModelLanguage(\n","    (attention): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","    )\n","    (linear): Linear(in_features=64, out_features=140, bias=True)\n","  )\n","  (multimodal_transformer): TransformerNetwork(\n","    (embedding): Linear(in_features=512, out_features=512, bias=True)\n","    (transformer): Transformer(\n","      (encoder): TransformerEncoder(\n","        (layers): ModuleList(\n","          (0): TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (decoder): TransformerDecoder(\n","        (layers): ModuleList(\n","          (0): TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (expand_previous): Linear(in_features=10, out_features=1024, bias=True)\n","  (lstm): TwoLSTM(\n","    (linear): Linear(in_features=283, out_features=513, bias=True)\n","    (lstm): LSTM(513, 513, num_layers=2, batch_first=True)\n","  )\n","  (fc): Linear(in_features=513, out_features=13229, bias=True)\n","  (fc_t5): Linear(in_features=10, out_features=512, bias=True)\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize default model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","multimodal = CCNeT5(vocab_size=len(tokenizer.itos))\n","# Or Load modal manually\n","path = f'/content/drive/MyDrive/WebAI/Model Weights/CCNeT5_OffLine1/CCNeT5_1_smaller_train_best.pth'\n","checkpoint = torch.load(path, map_location=torch.device(device))\n","multimodal.load_state_dict(checkpoint['model_state_dict'])\n","multimodal.to(device)\n","\n","multimodal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vrcIYqeax_a"},"outputs":[],"source":["def make_inference_model(model, rgb_input, language_input, previous_action_input, t5_output):\n","\n","    rgb_rgb_input_data = rgb_input.to(device)\n","    language_input = language_input.to(device)\n","    print(rgb_rgb_input_data.dtype, language_input.dtype, previous_action_input.dtype, t5_output.dtype)\n","\n","    action_output = model.forward(rgb_rgb_input_data, language_input, previous_action_input, t5_output)\n","\n","    print(f'action_output: {action_output.shape} - {action_output.dtype}')\n","\n","    return action_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qr6FW1PHT_-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":943},"executionInfo":{"elapsed":677,"status":"error","timestamp":1687185954144,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"TJ7IvXH-bKLM","outputId":"5e7fdda8-12cc-439e-d04b-6b89e72638e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 160, 210]) torch.Size([1, 512, 64]) torch.Size([1, 10]) torch.Size([1, 10])\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-26-3acd7dd5d795\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 8\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_data_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage_data_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_action_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_inference_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultimodal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb_data_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage_data_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_action_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'prediction: {prediction.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-23-59faf1dda6cb\u003e\u001b[0m in \u001b[0;36mmake_inference_model\u001b[0;34m(model, rgb_input, language_input, previous_action_input, t5_output)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_rgb_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_action_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 7\u001b[0;31m     \u001b[0maction_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_rgb_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_action_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'action_output: {action_output.shape} - {action_output.dtype}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-9-e9d53c38acce\u003e\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, rgb_input, language_input, previous_action, t5_output)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# First turn language inputs into embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 53\u001b[0;31m         \u001b[0membedded_language\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Process language input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"]}],"source":["#batch_size = 1\n","rgb_data_test = torch.randn((batch_size, 3, 160, 210)).to(device)\n","language_data_test = torch.rand((batch_size, 512, 64)).to(device)\n","t5_output = torch.randn([batch_size, 10]).to(device)\n","previous_action_input = torch.randn([batch_size, 10]).to(device)\n","\n","print(rgb_data_test.shape, language_data_test.shape, t5_output.shape, previous_action_input.shape)\n","prediction = make_inference_model(multimodal, rgb_data_test, language_data_test, previous_action_input, t5_output)\n","print(f'prediction: {prediction.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vo42lhjlbC6F"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def find_closest_embedding(input_embedding, embedding_matrix):\n","    \"\"\"\n","    Finds the index of the closest match to the input embedding based on cosine similarity.\n","\n","    Args:\n","        input_embedding (np.ndarray): Array of shape (8,) representing the input embedding.\n","        embedding_matrix (np.ndarray): Array of shape (10, 8) representing the embedding matrix.\n","\n","    Returns:\n","        int: Index of the closest match to the input embedding.\n","    \"\"\"\n","    # Normalize input embedding for cosine similarity\n","    input_embedding_norm = input_embedding / np.linalg.norm(input_embedding)\n","\n","    # Normalize embedding matrix for cosine similarity\n","    embedding_matrix_norm = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n","\n","    # Calculate cosine similarity between input embedding and embedding matrix\n","    similarity_scores = cosine_similarity(input_embedding_norm.reshape(1, -1), embedding_matrix_norm)\n","\n","    # Find the index of the closest match\n","    closest_index = np.argmax(similarity_scores)\n","\n","    return closest_index, similarity_scores\n","\n","# Decode action, ref, keydown from output tensor (1, 577)\n","# Only decode by batch_size = 1\n","def decode_output(tokenizer, tensor):\n","  # Decode action_type into a string\n","  action_type = tensor[0, 0:1]\n","  if int(action_type.item()) == 0:\n","    action_type = 'click'\n","  else:\n","    action_type = 'keydown'\n","\n","  embedded_ref = tensor[:, 1:65]\n","  decoded_ref_raw, _ = find_closest_embedding(embedded_ref, tokenizer.embedding_fn.embedding.weight.detach().numpy())\n","  print(f'decoded_ref_raw: {decoded_ref_raw}')\n","  decoded_ref = tokenizer.itos[decoded_ref_raw]\n","\n","  keydown = tensor[0, 65:577].view(8, 64)\n","  keydowns = []\n","  for key in keydown:\n","      decoded_keydown, _ = find_closest_embedding(key, tokenizer.embedding_fn.embedding.weight.detach().numpy())\n","      keydowns.append(tokenizer.itos[decoded_keydown])\n","  keydowns = ''.join(keydowns).replace(tokenizer.padding_char, '')\n","\n","  output = {'action_type': action_type, 'ref': decoded_ref, 'keydown': keydowns}\n","  return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686569091545,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"Y3cfR3rdWY-8","outputId":"533a7310-2621-4c5f-b18f-821eea233746"},"outputs":[{"name":"stdout","output_type":"stream","text":["decoded_ref_raw: 83\n"]},{"data":{"text/plain":["{'action_type': 'click', 'ref': '1', 'keydown': ''}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Decode output\n","decode_output(tokenizer, prediction.to('cpu').detach())"]},{"cell_type":"markdown","metadata":{"id":"ugMIDaZA1PFS"},"source":["# Actor Critic Model\n","If using V-MPO, you should load the Actor Critic weight of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5151,"status":"ok","timestamp":1686588854628,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"q_WnaGi71UUs","outputId":"855f712f-15f1-41c1-f925-44d8696c401d"},"outputs":[{"data":{"text/plain":["CCNeT5(\n","  (rgb_model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","  )\n","  (language_model): CrossAttentionModelLanguage(\n","    (attention): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","    )\n","    (linear): Linear(in_features=64, out_features=1, bias=True)\n","  )\n","  (multimodal_transformer): TransformerNetwork(\n","    (embedding): Linear(in_features=142, out_features=512, bias=True)\n","    (transformer): Transformer(\n","      (encoder): TransformerEncoder(\n","        (layers): ModuleList(\n","          (0-7): 8 x TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (decoder): TransformerDecoder(\n","        (layers): ModuleList(\n","          (0-7): 8 x TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (lstm): TwoLSTM(\n","    (linear): Linear(in_features=1089, out_features=513, bias=True)\n","    (lstm): LSTM(513, 513, num_layers=2, batch_first=True)\n","  )\n","  (fc): Linear(in_features=513, out_features=577, bias=True)\n","  (fc_t5): Linear(in_features=577, out_features=512, bias=True)\n",")"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize default model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","actor_critic = CCNeT5()\n","# Or Load modal manually\n","path = f'/content/drive/MyDrive/WebAI/Model Weights/CCNeT5_OffLine1/CCNeT5_{0}_sampled_combined_loss.pth'\n","checkpoint = torch.load(path, map_location=torch.device(device))\n","actor_critic.load_state_dict(checkpoint['model_state_dict'])\n","actor_critic.to(device)\n","\n","actor_critic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOf_U6GVBKUh"},"outputs":[],"source":["import pickle\n","mse_loss_fn = nn.MSELoss()\n","import torch.optim as optim\n","optimizer = optim.AdamW(actor_critic.parameters(), lr=1e-4, weight_decay=1e-1, betas=(0.9, 0.99))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"executionInfo":{"elapsed":5,"status":"error","timestamp":1686581695089,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"qtQgWxbwGMfn","outputId":"8f5c3dfc-345e-4c7f-ed18-f63167555771"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-68-869502c88825\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mprevious_loss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'item'"]}],"source":["previous_loss_value.item()"]},{"cell_type":"markdown","metadata":{"id":"frLTzOgl1rv4"},"source":["# Server Part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymp0-cz21tPf"},"outputs":[],"source":["# Define quick flask server to serve our endpoint\n","from flask import Flask\n","from pyngrok import ngrok\n","from flask import request, jsonify\n","import json\n","\n","app = Flask(__name__)\n","\n","@app.route('/')\n","def hello_world():\n","  return 'Hello World'\n","\n","\n","@app.route('/backprop_actor_critic', methods=['POST'])\n","def backprop_actor_critic():\n","    # Get the JSON payload from the request data\n","    # Parse the received JSON payload\n","    data = request.get_json()\n","\n","    # Extract the serialized loss value from the payload\n","    serialized_loss_value = data['loss_value']\n","\n","    # Deserialize the loss value\n","    loss_value = pickle.loads(serialized_loss_value.encode('latin1'))\n","\n","    optimizer.zero_grad()\n","    print(f'V-MPO Loss value: {loss_value.item()}')\n","\n","    # Create the tensor for backpropagation\n","    loss_tensor = torch.tensor(loss_value.item(), requires_grad=True)\n","\n","    # Perform backpropagation\n","    loss_tensor.backward()\n","    print('Backward step done')\n","\n","    # Perform optimizer step\n","    optimizer.step()\n","    print('Optimizer step done')\n","\n","    return jsonify({'backward': 'ok'})\n","\n","\n","@app.route('/infer_actor_critic')\n","def make_inference_actor_critic():\n","    print(f'Got input actor critic')\n","    data = request.get_json()\n","    instruction_input = data['instruction_input']\n","    cc_net5_payload_numpy = json.loads(instruction_input)\n","    cc_net5_payload = {\n","        key: torch.tensor(value) if isinstance(value, list) else value\n","        for key, value in cc_net5_payload_numpy.items()\n","    }\n","    rgb_data = cc_net5_payload['rgb_input'].to(device)\n","    language_data = cc_net5_payload['language_input'].to(device)\n","    previous_action = cc_net5_payload['previous_action'].to(device)\n","    t5_output = cc_net5_payload['t5_output'].to(device)\n","\n","    prediction = make_inference_model(actor_critic, rgb_data, language_data, previous_action, t5_output)\n","    print(f'prediction: {prediction.shape}')\n","\n","    return jsonify({'prediction': f'{prediction}'})\n","\n","@app.route('/infer_actor_normal', methods=['POST'])\n","def infer_actor_normal():\n","    print(f'Got input actor normal')\n","    data = request.get_json()\n","    instruction_input = data['instruction_input']\n","    cc_net5_payload_numpy = json.loads(instruction_input)\n","    cc_net5_payload = {\n","        key: torch.tensor(value) if isinstance(value, list) else value\n","        for key, value in cc_net5_payload_numpy.items()\n","    }\n","    rgb_data = cc_net5_payload['rgb_input'].to(device)\n","    language_data = cc_net5_payload['language_input'].to(device)\n","    previous_action = cc_net5_payload['previous_action'].to(device)\n","    t5_output = cc_net5_payload['t5_output'].to(device)\n","\n","    prediction = make_inference_model(multimodal, rgb_data, language_data, previous_action, t5_output)\n","    print(f'prediction: {prediction.shape}')\n","\n","    serialized_loss_value = pickle.dumps(prediction).decode('latin1')\n","\n","    # Convert the payload to JSON\n","    json_payload = json.dumps(serialized_loss_value)\n","\n","    return jsonify({'prediction': f'{prediction}'})\n","\n","\n","@app.route('/', methods=['POST'])\n","def make_inference():\n","    print('got input')\n","    data = request.get_json()\n","    #parsed_data = json.loads(data)\n","    instruction_input = data['instruction_input']\n","    cc_net5_payload_numpy = json.loads(instruction_input)\n","    cc_net5_payload = {\n","        key: torch.tensor(value) if isinstance(value, list) else value\n","        for key, value in cc_net5_payload_numpy.items()\n","    }\n","    rgb_data = cc_net5_payload['rgb_input'].to(device)\n","    language_data = cc_net5_payload['language_input'].to(device)\n","    previous_action = cc_net5_payload['previous_action'].to(device)\n","    t5_output = cc_net5_payload['t5_output'].to(device)\n","\n","    print(f'rgb_data: {rgb_data.shape}, language_data: {language_data.shape}, previous_action: {previous_action.shape}, t5_output: {t5_output.shape}')\n","    print(f'rgb_data: {rgb_data.dtype}, language_data: {language_data.dtype}, previous_action: {previous_action.dtype}, t5_output: {t5_output.dtype}')\n","\n","    prediction = make_inference_model(multimodal, rgb_data, language_data, previous_action, t5_output)\n","    print(f'prediction: {prediction.shape}')\n","\n","    formatted_output = decode_output(tokenizer, prediction.to('cpu').detach())\n","    print(f'formatted_output: {formatted_output}')\n","\n","    print('about to return request')\n","    #prediction = \"e\"\n","    return jsonify({'prediction': f'{prediction}'})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"711GtY403yIY","outputId":"515ecc84-7ede-4427-af73-2cb24a0e9e2b"},"outputs":[{"name":"stdout","output_type":"stream","text":[" * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:80\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:13:49] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:14:19] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:16:09] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:16:37] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:18:01] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:29:08] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:29:51] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:30:24] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:30:47] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:31:17] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:33:05] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:35:25] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:35:29] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:35:33] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:35:43] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:35:48] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:35:52] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:01] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:06] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:11] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:15] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:20] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:24] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:50] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:36:54] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:04] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:12] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:16] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:25] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:29] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:37] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:41] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:45] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:50] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:54] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n","Got input actor normal\n","torch.float32 torch.float32 torch.float32 torch.float32\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [12/Jun/2023 17:37:58] \"POST /infer_actor_normal HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["action_output: torch.Size([1, 577]) - torch.float32\n","prediction: torch.Size([1, 577])\n"]}],"source":["app.run(port=80)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686395573650,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"Im77D8XZ_UJY","outputId":"490e3445-f4ac-4d46-d889-bd2f5f6fa343"},"outputs":[{"data":{"text/plain":["(1235, '100', '\u003ctr')"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.stoi['\u003ctr'], tokenizer.itos[85], tokenizer.itos[1235]"]},{"cell_type":"markdown","metadata":{"id":"rIcXfko0hwnc"},"source":["# Test encoding / decoding with embeddings"]},{"cell_type":"markdown","metadata":{"id":"Zt9XU617WK69"},"source":["Tokenize input string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EXvJeMahxuA"},"outputs":[],"source":["input_string = '999'\n","stoi_value = tokenizer.stoi[input_string]"]},{"cell_type":"markdown","metadata":{"id":"8O_IsCEDhXuv"},"source":["Embed ref test from Index of Vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686350924096,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"blTQSI2LgcB7","outputId":"bb94fca6-7551-4219-b30c-a88d11c10da4"},"outputs":[{"data":{"text/plain":["((64,),\n"," array([ 7.24650562e-01,  1.55465722e+00,  9.79292035e-01, -7.00678170e-01,\n","        -2.37068677e+00, -9.89872515e-01, -6.16731167e-01,  1.13634884e+00,\n","         5.96476018e-01,  1.28716552e+00, -1.10430336e-02, -1.47615170e+00,\n","         1.65801823e-01, -9.02195334e-01, -1.90470129e-01, -1.43564057e+00,\n","         7.20837116e-01,  1.77453950e-01, -1.34255123e+00, -1.64361286e+00,\n","        -1.30364799e+00, -1.52187538e+00,  3.07258189e-01,  1.15209270e+00,\n","        -1.48790979e+00,  1.99890882e-01,  1.24265063e+00, -1.74484581e-01,\n","        -1.32448256e+00,  1.70079029e+00, -2.98048168e-01, -5.27164459e-01,\n","         8.16283524e-01,  1.14198291e+00,  8.19505036e-01, -1.40216017e+00,\n","        -3.02055657e-01, -6.24877691e-01, -2.18009889e-01,  1.17287941e-01,\n","        -1.02686810e+00, -3.83700972e-04,  1.25882804e+00, -2.72618860e-01,\n","        -1.12034810e+00, -1.29008055e+00, -5.15826881e-01, -9.08787847e-01,\n","         3.38234842e-01, -1.05625235e-01,  1.46444297e+00, -5.51725745e-01,\n","         1.69028997e+00, -4.83456373e-01, -2.17049694e+00,  7.05672920e-01,\n","         4.53648716e-02,  3.85464668e-01, -3.33081521e-02,  9.47335958e-01,\n","        -5.26930451e-01,  2.02208114e+00,  1.31387246e+00,  3.64151627e-01],\n","       dtype=float32))"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["embedded_ref = torch.flatten(tokenizer.embedding_fn(torch.from_numpy(np.array(stoi_value)))).detach().numpy()\n","# Here is our emebeding\n","embedded_ref.shape, embedded_ref"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325,"status":"ok","timestamp":1686350925819,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"0xhZ3lq4hDkz","outputId":"aa57f330-c42b-4daa-edbd-57493847e690"},"outputs":[{"data":{"text/plain":["1148"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Recover the original index from the embeddings\n","decoded_ref_test, similarity_scores = find_closest_embedding(embedded_ref, tokenizer.embedding_fn.embedding.weight.detach().numpy())\n","decoded_ref_test"]},{"cell_type":"markdown","metadata":{"id":"h9TNhBJVhkeX"},"source":["Now decode it with itos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1686350927688,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"DmXGHlz9hizM","outputId":"d9935a0b-b667-42a9-b1c6-26b8873b3eba"},"outputs":[{"data":{"text/plain":["('999', True)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.itos[decoded_ref_test], input_string == tokenizer.itos[decoded_ref_test]"]},{"cell_type":"markdown","metadata":{"id":"UpuFytGV61M2"},"source":["# Test payload"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"13Y9c8mOc21iS4PTJisGF-Tm8USik9cAi"},"executionInfo":{"elapsed":4854,"status":"ok","timestamp":1686394442037,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"SyKYlzGANlPI","outputId":"6a2b4eeb-2167-41ff-9561-7f9ee7604ca2"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["import json\n","\n","cc_net5_payload = {\n","    'rgb_input': torch.randn(1, 3, 160, 210),\n","    'language_input': torch.randn(512, 64),\n","    'previous_action': torch.randn(1, 577),\n","    't5_output': torch.randn(1, 577),\n","}\n","\n","cc_net5_payload_numpy = {\n","        key: value.detach().numpy().tolist() if isinstance(value, torch.Tensor) else value\n","        for key, value in cc_net5_payload.items()\n","    }\n","\n","json_data = json.dumps(cc_net5_payload_numpy)\n","\n","instruction_input = {'instruction_input': json_data}\n","request = json.dumps(instruction_input)\n","print(request)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5yBwL_67PIt"},"outputs":[],"source":["parsed_data = json.loads(request)\n","instruction_input = parsed_data['instruction_input']\n","cc_net5_payload_numpy = json.loads(instruction_input)\n","cc_net5_payload = {\n","    key: torch.tensor(value) if isinstance(value, list) else value\n","    for key, value in cc_net5_payload_numpy.items()\n","}\n","rgb_input = cc_net5_payload['rgb_input']\n","language_input = cc_net5_payload['language_input']\n","previous_action = cc_net5_payload['previous_action']\n","t5_output = cc_net5_payload['t5_output']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nTwnC198BE6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMYkZVp2napcB+7WaJfujrp","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"031d50ddb91049bfa50b50552b0abaeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20164f5b55024d439ee32851321c38c2","placeholder":"​","style":"IPY_MODEL_dfc56d2fd0d8475a812cccc5f56cd2d8","value":"Downloading data files: 100%"}},"0867d6de351a43589eb69f2cb3d94903":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"086ccc99145b46f78438251a4018bedc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1139461e4c604536ba08e3ff2f88ce3f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1df07edc51e24ad39651cf12432ae825":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d5f3dfe85cf43aea41577ec2ef919f7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6202a347211945459a1a3800a9f05ae7","value":1}},"1eb12ddf7cc64a7d82eb46ed702b3fdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"20164f5b55024d439ee32851321c38c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22109a456e604971ac7b3ebab1630e47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61e84e13d0434feeb3d934617dd1640d","IPY_MODEL_90fe3e8de3994bf9b061226a9a07f49b","IPY_MODEL_f353ba6333344259bea9d28d9c468dea"],"layout":"IPY_MODEL_fb9c7425ccd04ad6884f484e7b4bdc33"}},"22a4765d15ba4f058ee712b88ee3db5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_748b060322cc4b7cb6abf26ac79c6fe7","placeholder":"​","style":"IPY_MODEL_b7d8b98fb61c48318d7ee0c0b4ca6168","value":"Downloading readme: 100%"}},"24385d31069b4f579ca29ff930d0df5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_524c831f9f4f4a5c97865e08116a6a2d","max":55056820,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c39884c11a7c443a977deb4fa86542bb","value":55056820}},"2b9545e1d3094c32a3e005379e3843b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d5f3dfe85cf43aea41577ec2ef919f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37f8bbfc687948ea984736fb8b9f3392":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"381d57ea75314454a47444134a06ded2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3825687c97bf4f98847ac2376592dc4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"384415a8a891415e8155e98472adeda0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0a85cbec4ab470abee96bc77f83ffce","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5f8fd43d06649a5b1b5652303bffc32","value":1}},"4080ab74e01f4520ba6658d7b2ffdbfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfee9163952a425d804382b3a6dc501f","max":13412,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3825687c97bf4f98847ac2376592dc4e","value":13412}},"420ced3fc1a444d09de92da24e25bac2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea337072105e4bb7b7b30fa81a00ddc7","IPY_MODEL_24385d31069b4f579ca29ff930d0df5c","IPY_MODEL_f866f14331974fff85758d16abf1aeba"],"layout":"IPY_MODEL_ba11da47f75041e383cca8df2147bca0"}},"524c831f9f4f4a5c97865e08116a6a2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"591f24a156f1407a974d39d798131882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bc13fb3e1854d15be07600cbbef810e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db118fd1a82248de8da8980403f21134","placeholder":"​","style":"IPY_MODEL_6a2027072a4f49a39c6ccee2c80a416f","value":"Extracting data files: 100%"}},"5c590922803f4449995a0732de5dfca4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cbfe9e3efcf4940b9bc63669cfb361b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed2bea756ee4436d8f1cdb681d940c97","placeholder":"​","style":"IPY_MODEL_591f24a156f1407a974d39d798131882","value":" 1/1 [00:00\u0026lt;00:00, 33.99it/s]"}},"5e5ef954a1454038aa93643f9e4c4543":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1139461e4c604536ba08e3ff2f88ce3f","placeholder":"​","style":"IPY_MODEL_e1b9d548dd584c1a8062e53c4814186a","value":"Generating train split: 100%"}},"5f1015f696d54d1f99f7c1954a0e673a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61e84e13d0434feeb3d934617dd1640d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c590922803f4449995a0732de5dfca4","placeholder":"​","style":"IPY_MODEL_086ccc99145b46f78438251a4018bedc","value":"100%"}},"6202a347211945459a1a3800a9f05ae7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a2027072a4f49a39c6ccee2c80a416f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"748b060322cc4b7cb6abf26ac79c6fe7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"765975c08afa4d42907122af9e9f954a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"807da2eac6d84e2a8b76250d818b7cbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_031d50ddb91049bfa50b50552b0abaeb","IPY_MODEL_1df07edc51e24ad39651cf12432ae825","IPY_MODEL_b7b42ca490b1430fa68af8217c2ed858"],"layout":"IPY_MODEL_b79ccf7c365e464b8620baf112234e7b"}},"8142fae0c2b544198dcf97c8bb243ef5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82bea603aaaa40138c2b4e7889378c0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85ad0f1cf7e44870939edfbbd85ff4e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e5ef954a1454038aa93643f9e4c4543","IPY_MODEL_4080ab74e01f4520ba6658d7b2ffdbfd","IPY_MODEL_a3ed0049b8b9418c858b2349ac56ecb4"],"layout":"IPY_MODEL_1eb12ddf7cc64a7d82eb46ed702b3fdd"}},"8c5d6144abeb45fc8bcf41481e93d890":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90fe3e8de3994bf9b061226a9a07f49b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8142fae0c2b544198dcf97c8bb243ef5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82bea603aaaa40138c2b4e7889378c0a","value":1}},"980518712e9447dc8f33ad6f6159d4e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0867d6de351a43589eb69f2cb3d94903","max":548,"min":0,"orientation":"horizontal","style":"IPY_MODEL_381d57ea75314454a47444134a06ded2","value":548}},"9e81bb50790c4847851d228b95be8d20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_765975c08afa4d42907122af9e9f954a","placeholder":"​","style":"IPY_MODEL_d3c3f2e06afe47d1888db71c0c1b1606","value":" 548/548 [00:00\u0026lt;00:00, 27.2kB/s]"}},"9eea499bde17490ab5c3890324ef9eef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3ed0049b8b9418c858b2349ac56ecb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b9545e1d3094c32a3e005379e3843b1","placeholder":"​","style":"IPY_MODEL_9eea499bde17490ab5c3890324ef9eef","value":" 13412/13412 [00:03\u0026lt;00:00, 3987.50 examples/s]"}},"a5f8fd43d06649a5b1b5652303bffc32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b753ff506df848d1941ae6f48b0588cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b79ccf7c365e464b8620baf112234e7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7b42ca490b1430fa68af8217c2ed858":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c5d6144abeb45fc8bcf41481e93d890","placeholder":"​","style":"IPY_MODEL_5f1015f696d54d1f99f7c1954a0e673a","value":" 1/1 [00:02\u0026lt;00:00,  2.59s/it]"}},"b7d8b98fb61c48318d7ee0c0b4ca6168":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba11da47f75041e383cca8df2147bca0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be0fdbdd9c3d4d1c8a154174c5515cb1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5bc13fb3e1854d15be07600cbbef810e","IPY_MODEL_384415a8a891415e8155e98472adeda0","IPY_MODEL_5cbfe9e3efcf4940b9bc63669cfb361b"],"layout":"IPY_MODEL_37f8bbfc687948ea984736fb8b9f3392"}},"c0a85cbec4ab470abee96bc77f83ffce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c39884c11a7c443a977deb4fa86542bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce5034e52f1d42f081ccbd1df0eda5d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfee9163952a425d804382b3a6dc501f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3c3f2e06afe47d1888db71c0c1b1606":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5a1dc866e4c46ebb8bff9f313c71908":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db118fd1a82248de8da8980403f21134":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc56d2fd0d8475a812cccc5f56cd2d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1b9d548dd584c1a8062e53c4814186a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e267e07358c5414989f70acb31cf9016":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e86b849380114500894892d33f4e3da1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea2423728a2d402a834f71a77a6083e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea337072105e4bb7b7b30fa81a00ddc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b753ff506df848d1941ae6f48b0588cc","placeholder":"​","style":"IPY_MODEL_fd9317022dd94a20815c15b9797e9500","value":"Downloading data: 100%"}},"ed2bea756ee4436d8f1cdb681d940c97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0ef7fd5c0d14ab8b8a960b4ebdc0f5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22a4765d15ba4f058ee712b88ee3db5e","IPY_MODEL_980518712e9447dc8f33ad6f6159d4e8","IPY_MODEL_9e81bb50790c4847851d228b95be8d20"],"layout":"IPY_MODEL_e86b849380114500894892d33f4e3da1"}},"f353ba6333344259bea9d28d9c468dea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce5034e52f1d42f081ccbd1df0eda5d8","placeholder":"​","style":"IPY_MODEL_ea2423728a2d402a834f71a77a6083e7","value":" 1/1 [00:00\u0026lt;00:00,  6.37it/s]"}},"f866f14331974fff85758d16abf1aeba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5a1dc866e4c46ebb8bff9f313c71908","placeholder":"​","style":"IPY_MODEL_e267e07358c5414989f70acb31cf9016","value":" 55.1M/55.1M [00:01\u0026lt;00:00, 57.9MB/s]"}},"fb9c7425ccd04ad6884f484e7b4bdc33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd9317022dd94a20815c15b9797e9500":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}