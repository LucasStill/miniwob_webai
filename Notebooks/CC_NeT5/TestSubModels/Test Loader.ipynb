{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":555,"referenced_widgets":["11775b1adab44da6972430ef0c04af39","f9fc446c500945c4a547141986e53641","6315119121ec41819a364ac5f3ba1c00","031694b3b0104a77832a46fb1748fdac","b1440da631ef447cacb5dd4947bf3e62","79a2022bdb614c3abc515491b91f9c66","3c1c149ddbc24f05b29a2a46de43dca4","8047da4b7bdb4afd864fde7fb1d2c640","3bbeef06874d46dd8282bb7473da123e","94a11a90bab449d9bfd07df094893ced","1a93e87881394d9ab38e490616573712","b0b15b7ff7c2407e82c07d85a8e727ec","31734fdde65a4899a418140a3fdf125d","d4ccc3ece19e4615ad74ba3d0a85e96b","6f375bdc78e44c2891a6624949377453","cd0506911029466ca4a92471565f2fc0","74d225ac23ff4a9a94288ea7bd801615"]},"executionInfo":{"elapsed":32389,"status":"ok","timestamp":1685715450219,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"BAaOnOvsOtCY","outputId":"ac9cba7d-3067-4fa7-88d7-ec2c6bc4375e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11775b1adab44da6972430ef0c04af39"}},"metadata":{}}],"source":["!pip install -q transformers datasets\n","from datasets import load_dataset\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233,"referenced_widgets":["17d48d8680304637aa8e51f69b6c2931","b53a3bde80fe4db1af60dffba8c72a28","3d919abbfa7e4815ae6ac1b24bda29cd","badbd6d51aac44428947775c2d57909e","2e37e20454e24b928ba61bdfd975fad2","1bcf1f95e062478f83d5107da0c5721e","4ab152a019dd426095563c39c673e097","304c6bb29a2748d8be9be2e798eab702","132cfb8f7cf14f928cdffc83ba3e088c","2b6bbbf2e93441f296e7118771396e2a","c4cd41ae0b164d1791c9a49425b99d52","8fe3e3a530b94b0994723a0c8aebb80d","e03a6229e28d48e88facf3a522ce78f9","b1e09b05fbaf4968929fea77038f9163","8003dc4039374812b427b46652668ab3","8ad232d03db842da986adc7eb81dda07","c7642ec345354e838e091252ec0f5951","b59c8e100f3a46e2bf92ec8ef85a2b13","c673630246cd4386a0cedb84958f84b7","5ac55d11598b4b00b6e45db7b299e41b","11a2abbd921746a89ab4f568693de7b4","3747bc89bcdd41dd8c030f68ad7f21db","1f84295e7ce94ee8809d519ce1597665","9f0e9731c1524d16a0190d6dbef5d9ac","24be3dc82dd44f5baf60a07a4d825f68","119bfddeeb0b418c8f88a90907b211f6","edbe6c40566044409532032c1d5f0253","248e7b6b48f340b5bd20731e89561a56","3755638837cc41f3a6ad3980c9f469f0","24d7075adaf34c20b2fa1f8d6257ceeb","2f9ef33c77334828a022b69afd1a85ee","e4c19511eb784624b3e4404812324dbc","789669b20b964686a4c43e6981b23758","5858d2b7ec2444da9f9b5bf090757b98","d25e129c3d4d43c8a716bfa22b8c4d09","46201f31dace4c45b7cc4adfa5cc3942","ffa849f6f3d647ca90b4c9adfe2af29b","9231f35c02c545eebbf82d7df217a9d9","ebe18a35312545a98b4d0ce68f6b982e","c4766104664949faa65387a0f867b005","4039c0b7a1504c739ce37472ba741ebe","f51c05b3062a414a85e699a76cc5a12c","c3f1b0a2dcf448cf8b56cc8676dce04f","beaffea6ad60474ea5d5e1e5db49ab43","f34da6ad772344b891dd3f0f52bc45c4","314c6d4c0e08450ba9ffefa95ad86193","4a1d6ee1ac0548d5870810b074c8b26c","92d6642a9d104e399be41c2d2412e43e","6aeef4679c0c4c5389f0f2317b174f4b","3ad5260b093b4232b7e1c0fbe57c2c4a","b344c5ae536b45359969c879477ed19d","ebd8baf26fb3407a9db15e4848306edd","c7fb28d4fed54b29ae10be6509b1baff","360ca6b9657845ca9a1391f7355393a9","3f8f77d4c3234a7485c664242ed6aada","ab20a523c45f4970be6cce7e05ce98f5","7f62dd01613e4801a543e1203805dc18","f5fec2008849450ab3845a23a99991e0","7c64056cdb21443ca2f39bd673698347","26dd5d60dab448ef958dc3f82534ade9","264319373cef4f38ab3c991f856a2afe","cb746be1db56474daba6c9ba953c4498","959159e432b14a89a87f44782f3f03eb","e2bb1c90364142aba20e91cb622793f0","a4c2932ff2a24cb0849606502fa47f5c","38a971863fdc437ca7ef96565c07930b"]},"executionInfo":{"elapsed":7480,"status":"ok","timestamp":1685715457692,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"8mQr1tt-O2gN","outputId":"41ad634a-7ad7-4869-ac4e-16aac14470b1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/548 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d48d8680304637aa8e51f69b6c2931"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/LucasThil___parquet/LucasThil--randomized_clean_miniwob_episodes_v2-49501296fc0102ba/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe3e3a530b94b0994723a0c8aebb80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/55.1M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f84295e7ce94ee8809d519ce1597665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5858d2b7ec2444da9f9b5bf090757b98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/13412 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f34da6ad772344b891dd3f0f52bc45c4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/LucasThil___parquet/LucasThil--randomized_clean_miniwob_episodes_v2-49501296fc0102ba/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab20a523c45f4970be6cce7e05ce98f5"}},"metadata":{}}],"source":["import numpy as np\n","from datasets import load_dataset\n","dataset = load_dataset(\"LucasThil/randomized_clean_miniwob_episodes_v2\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91437,"status":"ok","timestamp":1685715549126,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"ixG9bMy4PoE_","outputId":"b01d736a-fffa-4fba-9a36-570b14b622ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7787,"status":"ok","timestamp":1685715559570,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"621pErKvPpfo","outputId":"eccb1ac2-faae-4711-f025-a7ec1e0692bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["pad: 1591\n","Loaded CC_NeT5 Tokenizer with vocabulary size being 1592.\n"]}],"source":["# 1 - Get out Tokenizer Class\n","css_fields = ['top', 'left', 'width', 'height']\n","special_characters = ['.', ',', '#', ':', '-', '/', '(', ')', 'https://', '@', '&', '\"', \"'\", '!', '?', ';', '+', '=',\n","                      '*', '$', '€', '*', '`']\n","\n","\n","def round_to_nearest_ten(number):\n","    return round(number / 10) * 10\n","\n","import torch.nn as nn\n","import torch\n","\n","# Embedding Function\n","class EmbeddingFunction(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(EmbeddingFunction, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","    def forward(self, input_tokens):\n","        embedded = self.embedding(input_tokens)\n","        return embedded\n","\n","    # Provide a tensor and de-embbeds it to retrieve the correct index\n","    def get_embedding_index(self, x):\n","        results = torch.where(torch.sum((self.embedding.weight == x), axis=1))\n","        if len(results[0]) == len(x):\n","            return None\n","        else:\n","            return results[0][0]\n","\n","# Turn it into a class\n","class CCNeT5Tokenizer:\n","    def __init__(self, vocab_path):\n","        stoi = {}\n","        itos = {}\n","        self.padding_char = '<PAD>'\n","        self.special_characters = ['.', ',', '#', ':', '-', '/', '(', ')', 'https://', '@', '&', '\"', \"'\", '!', '?',\n","                                   ';', '+', '=', '*', '$', '€', '*', '`']\n","\n","        with open(vocab_path, 'r') as file:\n","            for index, line in enumerate(file):\n","                line = line.strip()\n","                stoi[line] = index\n","                itos[index] = line\n","\n","        stoi[' '] = stoi['']\n","        itos[stoi[' ']] = ' '\n","        self.stoi = stoi\n","        self.stoi[self.padding_char] = len(stoi.keys())  # Add PADDING character\n","        # We do not need itos as we don't implement a de-tokinezing function.\n","        self.itos = itos\n","        self.itos[len(stoi.keys()) - 1] = self.padding_char  # Add PADDING character\n","        print(f'pad: {len(stoi.keys()) - 1}')\n","        print(f'Loaded CC_NeT5 Tokenizer with vocabulary size being {len(self.stoi)}.')\n","\n","        # Instantiate the embedding function\n","        vocab_size = len(stoi)\n","        self.embedding_dim = 64\n","        self.embedding_fn = EmbeddingFunction(vocab_size, self.embedding_dim)\n","\n","    # Provide a string and tokenize it: utterance or task name.\n","    def tokenize_string(self, string):\n","        tokenized_string = []\n","        for w in str(string).lower().split(' '):\n","            for sc in self.special_characters:\n","                if sc in w:\n","                    w = w.replace(sc, 'Ø' + sc + 'Ø')\n","            p1_words = [s for s in w.split('Ø') if s != '']\n","            # Check if the found words exist in our vocab, else subdivide\n","            for fw in p1_words:\n","                if fw not in self.stoi.keys():\n","                    for c in fw:\n","                        tokenized_string.append(self.stoi[c])\n","                else:\n","                    tokenized_string.append(self.stoi[fw])\n","        return tokenized_string\n","\n","    def get_tokens_from_embeddings(self, embedded_tokens):\n","        indices = torch.Tensor(list(map(self.embedding_fn.get_embedding_index, embedded_tokens)))\n","        return indices\n","\n","    # Turns an array into a string\n","    def detokenize_array(self, array):\n","        reconstruted_string = []\n","        #pad_index = self.itos[self.stoi[self.padding_char]]\n","        for v in array:\n","            v = int(v)\n","            s = self.itos[v]\n","            if s != self.padding_char:\n","                reconstruted_string.append(s)\n","\n","        #reconstruted_string = ' '.join(reconstruted_string)\n","        cleaned_string = ''\n","        last_single = False\n","        for s in reconstruted_string:\n","            if len(s) > 1:\n","                if last_single:\n","                    cleaned_string += ' '\n","                    last_single = False\n","                cleaned_string += s + ' '\n","            else:\n","                # We do this to treat single length characters\n","                cleaned_string += s\n","                last_single = True\n","        return cleaned_string\n","\n","    # Truncate and pad the incoming sentences based on a max_size argument\n","    def truncate_pad_entry(self, tokenized_array, max_size):\n","        if len(tokenized_array) < max_size:\n","            while len(tokenized_array) < max_size:\n","                tokenized_array.append(self.stoi[self.padding_char])\n","            return tokenized_array\n","        elif len(tokenized_array) > max_size:\n","            # Too long, so we automatically truncate\n","            return tokenized_array[:max_size]\n","        else:\n","            # Just avoid iterating under the hood\n","            return tokenized_array\n","\n","    # Tokenize a DOM dictionary\n","    def tokenize_dom(self, dom):\n","        tokenized_dom = []\n","\n","        # Add opening tag\n","        element_tag = dom['tag'].lower()\n","        if 'input' in dom.keys():\n","            # if tag is input, reformat it\n","            element_tag = dom['input'].lower() + '_' + dom['type'].lower()\n","            tokenized_dom.append(self.stoi['<' + element_tag])\n","        else:\n","            # add normal tag\n","            tokenized_dom.append(self.stoi['<' + dom['tag'].lower()])\n","\n","        for field in dom:\n","            if field != 'tag' and field != 'children' and field != 'type' and field != 'text' and field not in css_fields:  # and field != 'value':\n","\n","                tokenized_dom.append(self.stoi[field.lower()])\n","\n","                # Ensure we don't have float values, we'll stick with integers\n","                if isinstance(dom[field], float):\n","                    tokenized_dom.append(self.stoi[str(int(round_to_nearest_ten(dom[field])))])\n","                else:\n","                    words = str(dom[field]).lower().split(' ')\n","                    for word in words:\n","                        for sc in special_characters:\n","                            if sc in word:\n","                                word = word.replace(sc, 'Ø' + sc + 'Ø')\n","                        p1_words = [s for s in word.split('Ø') if s != '']\n","\n","                        # decides whether we keep the full word, or just the letters:\n","                        if field == 'value' or (False and (field == 'label' or field == 'button')):\n","                            for w in p1_words:\n","                                # Take individual characters of the value string\n","                                processed_words = [self.stoi[w[i:i + 1]] for i in range(0, len(w), 1)]\n","                                tokenized_dom += processed_words\n","\n","                        elif field == 'ref':\n","                            for w in p1_words:\n","                                # Take individual characters of the value string\n","                                processed_words = [self.stoi[w[i:i + 3]] for i in range(0, len(w), 3)]\n","                                tokenized_dom += processed_words\n","                        else:\n","                            # Use the full word:\n","                            for w in p1_words:\n","                                tokenized_dom.append(self.stoi[str(w)])\n","\n","            elif field == 'text':\n","                tokenized_dom.append(self.stoi['text'])\n","                for c in dom[field]:\n","                    tokenized_dom.append(self.stoi[c])\n","            elif field in css_fields:\n","                # Cast field\n","                tokenized_dom.append(self.stoi[field])\n","                css_value = int(round_to_nearest_ten(float(dom[field])))\n","                tokenized_dom.append(self.stoi[str(css_value)])\n","\n","        if 'children' in dom.keys():\n","            tokenized_dom.append(self.stoi['children'])\n","            for child in dom['children']:\n","                tokenized_dom += self.tokenize_dom(child)\n","                # for v in found_vocab:\n","                #    tokenized_dom.append(self.stoi[v])\n","\n","        # add closing tag\n","        tokenized_dom.append(self.stoi['</' + element_tag + '>'])\n","\n","        return tokenized_dom\n","\n","vocab_path=\"/content/drive/MyDrive/WebAI/Notebooks/CC_NeT5/vocab.txt\"\n","tokenizer = CCNeT5Tokenizer(vocab_path=vocab_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685715559570,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"pA1qYvKkPszm"},"outputs":[],"source":["\n","\n","# Load some of the images we have here and try to save them to the dataset\n","import os\n","from typing import List, Tuple\n","import numpy as np\n","from PIL import Image\n","import torch\n","import pandas as pd\n","import re\n","import json\n","from torch.utils.data import DataLoader\n","import random\n","\n","class DatasetLoader:\n","    def __init__(self, screenshots_path, dataset, start_index, end_index, dom_tokenizer, batch_size):\n","        # Get the Subset of the dataset\n","        self.start_index = start_index\n","        self.end_index = end_index\n","        self.df = dataset['train'].to_pandas()[self.start_index:self.end_index]\n","        # When creating the class, parse JSON files into a state-based list structure\n","        self.df['processed_states'] = self.df['processed_states'].apply(lambda x: json.loads(re.sub(r'\\b(True|False)\\b', lambda m: m.group(0).lower(), x.replace(\"'\", '\"'))))\n","        print(f'Dataset subset has {len(self.df)} rows')\n","\n","        self.screenshots_path = screenshots_path\n","        # If want to load them from the directory\n","        #self.indices, found_screenshots = self.get_file_indices()\n","        # Or load the previously saved indices\n","        self.indices = self.read_indices_from_file('/content/drive/MyDrive/WebAI/file_indices.txt')\n","        found_screenshots = len(self.indices)\n","        print(f'Found a total of {found_screenshots} state screenshots for {len(self.indices)} episodes.')\n","\n","        self.add_images_to_episodes()\n","\n","        self.dom_tokenizer = dom_tokenizer\n","        self.dom_seq_length = 492\n","        self.utterance_seq_length = 16\n","        self.task_name_seq_length = 4\n","\n","        self.target_action_type_seq_length = 1\n","        self.target_ref_seq_length = 1\n","        self.target_keydown_seq_length = 8 #32\n","\n","        # Size of the final layer output being composed of\n","        # 1 (action),\n","        # + 64 (ref) (\n","        # + 64x8 (keydown text times target_keydown_seq_length)\n","        self.model_output_size = 577\n","\n","        self.batch_size = batch_size\n","\n","    # Add the images to the different episodes of the dataset\n","    def add_images_to_episodes_old(self):\n","        found_images = []\n","        i = self.start_index\n","        image_counter = 0\n","        while i < self.end_index:\n","            if i % 100 == 0:\n","              print(f'Done: {i}')\n","            # Get K indices\n","            images = []\n","            try:\n","              state_indexes = self.indices[i]\n","              for state_index in state_indexes:\n","                  images.append(self.read_image(i, state_index))\n","                  image_counter += 1\n","              found_images.append(images)\n","            except:\n","              print(f'error {i}')\n","            i += 1\n","        # Add new column\n","        self.df['episode_images'] = found_images\n","        print(f'Found total of {image_counter} images in the loaded episodes.')\n","\n","    # Load the images in batches\n","    def add_images_to_episodes(self):\n","      found_images = []\n","      image_counter = 0\n","\n","      for i in range(self.start_index, self.end_index):\n","        if i%100 == 0:\n","          print(f'Done :{i}')\n","        try:\n","            state_indexes = self.indices[i]\n","            images = [self.read_image(i, state_index) for state_index in state_indexes]\n","            found_images.append(images)\n","            image_counter += len(images)\n","        except Exception as e:\n","            print(f'Error at index {i}: {str(e)}')\n","\n","      self.df['episode_images'] = found_images\n","      print(f'Found a total of {image_counter} images in the loaded episodes.')\n","\n","\n","    def get_file_indices(self) -> List[Tuple[int, int]]:\n","        indices = []\n","        for filename in os.listdir(self.screenshots_path):\n","            if filename.endswith('.png'):\n","                # Extract the N and K values from the filename\n","                N, K = filename.replace('sc_', '').replace('st_', '').replace('.png', '').split('_')\n","                N = int(N)\n","                K = int(K)\n","                indices.append((N, K))\n","\n","        # Turn the indices into a dictionary\n","        indices_dict = {}\n","        for (N, K) in indices:\n","            if N not in indices_dict:\n","                indices_dict[N] = [K]\n","            else:\n","                indices_dict[N].append(K)\n","                indices_dict[N] = sorted(indices_dict[N])\n","\n","        return indices_dict, len(indices)\n","\n","    def save_file_indices_to_file(self, dictionary, filename):\n","      with open(filename, 'w') as file:\n","          for key, value in dictionary.items():\n","              line = f\"{key}: {value}\\n\"\n","              file.write(line)\n","\n","    def read_indices_from_file(self, filename):\n","        dictionary = {}\n","        with open(filename, 'r') as file:\n","            for line in file:\n","                key, value = line.strip().split(': ')\n","                key = int(key)\n","                value = [int(num) for num in value.strip('[]').split(', ')]\n","                dictionary[key] = value\n","        return dictionary\n","\n","\n","    import torch\n","\n","    # Reads images into tensor\n","    def read_image_old(self, N, K) -> torch.Tensor:\n","        filename = f'sc_{N}_st_{K}.png'\n","        filepath = os.path.join(self.screenshots_path, filename)\n","        image = Image.open(filepath).convert('RGB')\n","        tensor = torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) / 255.0\n","        return tensor\n","\n","    # Pure NP instead of tensor\n","    def read_image(self, N, K) -> np.array:\n","        filename = f'sc_{N}_st_{K}.png'\n","        filepath = os.path.join(self.screenshots_path, filename)\n","        image = Image.open(filepath).convert('RGB')\n","        array = np.transpose(np.array(image) / 255.0, (2, 0, 1))\n","        return array\n","\n","\n","    def save_image_png(self, N, K):\n","        loaded_image = self.read_image(N, K).numpy()\n","        print(f'Image has shape {loaded_image.shape}')\n","\n","        # scale the pixel values from [0,1] to [0, 255]\n","        array = np.clip(loaded_image * 255, 0, 255).astype('uint8')\n","        img = Image.fromarray(array.transpose(1, 2, 0), mode='RGB')\n","        img.save('screenshots/test_img.png')\n","\n","    # Turn our dataset into a series of state tensors ready for training.\n","    # We though ensure to clearly separate our different instances:\n","    # - todo: T5-Data (for later, depends between SL or RL current approach)\n","    # - RBG\n","    # - todo: Tokenized DOM\n","    # - todo: Task Instruction\n","    # They are separated because they are not going to be fed in the same manner into the model.\n","    # Basically into one single row.\n","    # todo: skip rows that have negative rewards\n","    def process_dataset(self):\n","        rbg_data = []\n","        dom_data = []\n","        utterance_data = []\n","        task_name_data = []\n","\n","        target_action = []\n","        target_refs = []\n","        target_keydown = []\n","\n","        episode_previous_actions = []\n","\n","        # Some metrics\n","        duplicated_images = 0\n","        cut_images = 0\n","\n","        for (index, row) in self.df.iterrows():\n","            if float(row['reward']) <= 0: # Skips failed episodes\n","                continue\n","            if len(row['episode_images']) != len(row['processed_states']): # Skips episodes with no matching pictures amount and states\n","                len_processed_states = len(row['processed_states'])\n","                len_episode_images = len(row['episode_images'])\n","                print(f'Error: episode_images and processed_states don\\' have the same length for index {index}: len_episode_images={len_episode_images}, len_processed_states={len_processed_states}')\n","                # We just discard it, fuck it if it's just an individual row and only one image missing out of n states.\n","                # Data processing showed that only few states were missing from the zip and got fixed except a couple of them\n","\n","                # Ok actually fix it by cloning the last rgb\n","                if len_episode_images < len_processed_states:\n","                  last_img = row['episode_images']\n","                  while len(row['episode_images']) < len_processed_states:\n","                    row['episode_images'].append(last_img)\n","                    duplicated_images += 1\n","                else:\n","                  # Too few processed states for pictures, cut last pic.\n","                  # We keep the processed states over the generated screenshots\n","                  row['episode_images'] = row['episode_images'][:len_processed_states]\n","                  cut_images += len_episode_images - len_processed_states\n","                continue\n","            for rbg in row['episode_images']:\n","                rbg = np.array(rbg, dtype='float64')\n","                #print(f'rbg shape: {rbg.shape}')\n","                rbg_data.append(rbg)\n","\n","\n","            # Process DOM Data\n","            for (index_state, state) in enumerate(row['processed_states']):\n","\n","                # Output Tokens\n","                if state['action_type'] == 'click': # Append boolean for action click or keydown\n","                    t_action = np.array([0])\n","                    target_action.append(t_action)\n","                else:\n","                    t_action = np.array([1])\n","                    target_action.append(t_action)\n","\n","                # Tokenize ref and appends to target\n","                t_ref = np.array(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(str(state['refs'])), self.target_ref_seq_length))\n","                target_refs.append(t_ref)\n","                # Tokenize target text and appends to target\n","                t_keydown = np.array(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(state['keydown_text']), self.target_keydown_seq_length))\n","                target_keydown.append(t_keydown)\n","\n","                # Input Tokens\n","                # Tokenize dom\n","                tokenized_dom = self.dom_tokenizer.tokenize_dom(state['dom'])\n","                tokenized_dom = self.dom_tokenizer.truncate_pad_entry(tokenized_dom, self.dom_seq_length)\n","                dom_data.append(tokenized_dom)\n","\n","                # Tokenize utterance\n","                tokenized_utterance = self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(row['utterance']), self.utterance_seq_length)\n","                utterance_data.append(tokenized_utterance)\n","\n","                # Tokenize task name\n","                tokenized_task_name = self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(row['task_name']), self.task_name_seq_length)\n","                task_name_data.append(tokenized_task_name)\n","\n","                # Deal with the previous action\n","                if index_state < len(row['processed_states'])-1:\n","                    # Default one so empty for the first action\n","                    if index_state == 0:\n","                        episode_previous_actions.append([])\n","                        #episode_previous_actions.append(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(''), self.model_output_size))\n","\n","                    # Add previous actions one by one\n","                    episode_previous_actions.append([t_action, t_ref, t_keydown])\n","                elif index_state >= len(row['processed_states'])-1 and index_state == 0:\n","                    episode_previous_actions.append([])\n","                # Else no action to add\n","\n","\n","\n","\n","        rbg_data = np.stack(rbg_data)\n","        dom_data = np.array(dom_data)\n","        utterance_data = np.array(utterance_data)\n","        task_name_data = np.array(task_name_data)\n","        target_action = np.array(target_action)\n","        target_refs = np.array(target_refs)\n","        target_keydown = np.array(target_keydown)\n","\n","        print(f'duplicated_images: {duplicated_images}, cut_images: {cut_images}')\n","\n","        return rbg_data, dom_data, utterance_data, task_name_data, target_action, target_refs, target_keydown, episode_previous_actions\n","\n","\n","    # Batchify the dataset\n","    def get_dataloder(self, dataset):\n","        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n","        return train_loader\n","\n","    # Creates random indexes to split and shuffle the test and training datasets\n","    def create_train_test_dataset(self, length):\n","        # Calculate the number of random indexes (10% of length)\n","        num_random_indexes = int(length * 0.1)\n","\n","        # Generate random indexes without replacement\n","        random_indexes = random.sample(range(length), num_random_indexes)\n","\n","        # Create train and test datasets\n","        train_dataset = []\n","        test_dataset = []\n","\n","        for i in range(length):\n","            if i in random_indexes:\n","                test_dataset.append(i)\n","            else:\n","                train_dataset.append(i)\n","\n","        # Shuffle the train and test datasets\n","        random.shuffle(train_dataset)\n","        random.shuffle(test_dataset)\n","\n","        return train_dataset, test_dataset\n","\n","    # Devise datasets\n","    def split_datasets(self, rbg_data, dom_data, utterance_data, task_name_data, target_action, target_refs, target_keydown, episode_previous_actions):\n","        train_indexes, test_indexes = self.create_train_test_dataset(length=len(rbg_data))\n","\n","        train_rgb = rbg_data[train_indexes]\n","        train_dom = dom_data[train_indexes]\n","        train_utterance = utterance_data[train_indexes]\n","        train_task_name = task_name_data[train_indexes]\n","        train_target_action = target_action[train_indexes]\n","        train_target_ref = target_refs[train_indexes]\n","        train_target_keydown = target_keydown[train_indexes]\n","        train_episode_previous_action = []\n","        for index in train_indexes:\n","            train_episode_previous_action.append(episode_previous_actions[index])\n","\n","        # Create a boolean mask of the indexes\n","        test_rgb = rbg_data[test_indexes]\n","        test_dom = dom_data[test_indexes]\n","        test_utterance = utterance_data[test_indexes]\n","        test_task_name = task_name_data[test_indexes]\n","        test_target_action = target_action[test_indexes]\n","        test_target_ref = target_refs[test_indexes]\n","        test_target_keydown = target_keydown[test_indexes]\n","        test_episode_previous_action = []\n","        for index in test_indexes:\n","            test_episode_previous_action.append(episode_previous_actions[index])\n","\n","        return train_rgb, train_dom, train_utterance, train_task_name, train_target_action, train_target_ref, \\\n","               train_target_keydown, train_episode_previous_action, test_rgb, test_dom, test_utterance, \\\n","               test_task_name, test_target_action, test_target_ref, test_target_keydown, test_episode_previous_action\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2anAKvOyPu0H","outputId":"5e1c48f8-4e2e-482d-a852-f129ebed0fb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset subset has 8000 rows\n","Found a total of 7266 state screenshots for 7266 episodes.\n","Done :0\n","Error at index 0: [Errno 5] Input/output error: '/content/drive/MyDrive/WebAI/screenshot_indexes/sc_0_st_0.png'\n"]}],"source":["screenshots_path = '/content/drive/MyDrive/WebAI/screenshot_indexes'\n","batch_size = 12\n","# NOTE!!!: Check if need to rerun the indices on all pictures instead of range\n","dataset_loader = DatasetLoader(screenshots_path=screenshots_path, dataset=dataset, start_index=0, end_index=8000, dom_tokenizer=tokenizer, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5638,"status":"ok","timestamp":1685715072192,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"Wu14OdOfPxXj","outputId":"170118ce-ece6-4707-afce-059e218eb181"},"outputs":[{"output_type":"stream","name":"stdout","text":["Error: episode_images and processed_states don' have the same length for index 6: len_episode_images=4, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 374: len_episode_images=5, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 396: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 504: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 586: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 588: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 594: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 596: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 598: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 600: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 609: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 611: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 618: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 624: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 626: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 627: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 650: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 652: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 655: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 657: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 658: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 660: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 661: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 662: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 664: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 665: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 666: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 668: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 669: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 672: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 673: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 674: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 675: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 679: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 684: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 686: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 687: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 688: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 692: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 693: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 696: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 697: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 698: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 699: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 701: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 703: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 706: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 708: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 709: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 710: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 712: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 713: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 717: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 718: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 719: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 721: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 722: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 724: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 725: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 726: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 728: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 729: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 732: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 734: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 736: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 737: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 742: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 744: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 747: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 749: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 750: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 752: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 753: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 755: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 757: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 758: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 759: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 760: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 763: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 764: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 765: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 768: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 769: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 772: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 775: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 776: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 777: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 780: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 782: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 785: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 787: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 788: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 789: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 791: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 792: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 794: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 797: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 798: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 799: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 800: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 801: len_episode_images=19, len_processed_states=22\n","Error: episode_images and processed_states don' have the same length for index 802: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 803: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 804: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 808: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 811: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 812: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 813: len_episode_images=7, len_processed_states=8\n","Error: episode_images and processed_states don' have the same length for index 814: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 815: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 817: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 818: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 820: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 824: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 826: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 828: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 830: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 831: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 833: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 834: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 836: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 837: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 839: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 844: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 845: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 849: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 852: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 853: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 855: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 863: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 865: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 866: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 868: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 871: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 872: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 874: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 876: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 877: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 878: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 881: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 883: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 885: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 886: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 887: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 888: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 890: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 891: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 893: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 895: len_episode_images=5, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 897: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 898: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 900: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 901: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 902: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 903: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 907: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 908: len_episode_images=5, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 910: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 912: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 914: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 915: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 918: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 919: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 921: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 925: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 927: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 933: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 934: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 936: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 940: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 941: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 943: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 944: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 946: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 951: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 952: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 953: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 957: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 962: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 964: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 965: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 967: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 969: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 970: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 971: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 973: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 974: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 976: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 979: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 980: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 981: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 983: len_episode_images=5, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 987: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 988: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 989: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 991: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 992: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 995: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 997: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1006: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1007: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1008: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1009: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1010: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1012: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 1015: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1018: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1019: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1020: len_episode_images=7, len_processed_states=8\n","Error: episode_images and processed_states don' have the same length for index 1021: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1025: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1030: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1032: len_episode_images=5, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 1034: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1037: len_episode_images=15, len_processed_states=19\n","Error: episode_images and processed_states don' have the same length for index 1038: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1040: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1041: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1042: len_episode_images=8, len_processed_states=9\n","Error: episode_images and processed_states don' have the same length for index 1043: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 1044: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1046: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1047: len_episode_images=5, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 1049: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1050: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1052: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1054: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1056: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 1059: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1062: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1063: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 1064: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1067: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1069: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1071: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1074: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1076: len_episode_images=1, len_processed_states=2\n","Error: episode_images and processed_states don' have the same length for index 1079: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1082: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1086: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1087: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1092: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1096: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1097: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1098: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 1099: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1100: len_episode_images=3, len_processed_states=4\n","Error: episode_images and processed_states don' have the same length for index 1103: len_episode_images=6, len_processed_states=7\n","Error: episode_images and processed_states don' have the same length for index 1105: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1107: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1108: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1109: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1111: len_episode_images=4, len_processed_states=5\n","Error: episode_images and processed_states don' have the same length for index 1115: len_episode_images=5, len_processed_states=6\n","Error: episode_images and processed_states don' have the same length for index 1117: len_episode_images=2, len_processed_states=3\n","Error: episode_images and processed_states don' have the same length for index 1119: len_episode_images=4, len_processed_states=5\n","duplicated_images: 266, cut_images: 1\n"]}],"source":["rgb_data, dom_data, utterance_data, task_data, target_action, target_refs, target_keydown, episode_previous_actions = dataset_loader.process_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1685715077464,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"DsjcIwrqPy1S","outputId":"905b7c6f-ce5e-4f97-f886-9f0d6aea7d12"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4358, 4358, 4358, 4358, 4358, 4358, 4358, 4358)"]},"metadata":{},"execution_count":8}],"source":["len(rgb_data), len(dom_data), len(utterance_data), len(task_data), len(target_action), len(target_refs), len(target_keydown), len(episode_previous_actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1585,"status":"ok","timestamp":1685715080523,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"4nkOCXwNP0Ul","outputId":"c4b22c00-1539-41e7-9826-f7e634c87338"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3923, 3, 210, 160),\n"," (3923, 492),\n"," (3923, 16),\n"," (3923, 4),\n"," (3923, 1),\n"," (3923, 1),\n"," (3923, 8),\n"," (435, 3, 210, 160),\n"," (435, 492),\n"," (435, 16),\n"," (435, 4),\n"," (435, 1),\n"," (435, 1),\n"," (435, 8))"]},"metadata":{},"execution_count":9}],"source":["\n","# Create Train/Test Splits\n","train_rgb, train_dom, train_utterance, train_task_name, train_target_action, train_target_ref, \\\n","               train_target_keydown, train_episode_previous_action, test_rgb, test_dom, test_utterance, test_task_name, test_target_action, \\\n","               test_target_ref, test_target_keydown, test_episode_previous_action = dataset_loader.split_datasets(rgb_data, dom_data, utterance_data, task_data, target_action, target_refs, target_keydown, episode_previous_actions)\n","\n","train_rgb.shape, train_dom.shape, train_utterance.shape, train_task_name.shape, train_target_action.shape, train_target_ref.shape, \\\n","               train_target_keydown.shape, \\\n","test_rgb.shape, test_dom.shape, test_utterance.shape, test_task_name.shape, test_target_action.shape, test_target_ref.shape, \\\n","    test_target_keydown.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1685715082838,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"-wYfpbaXP1Oh","outputId":"8dfca967-8cad-4a1d-dfc6-2c6ed0a47a30"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3923, 512)\n","rgb_data: torch.Size([3923, 3, 210, 160]), language_input: torch.Size([3923, 512, 64])\n","(435, 512)\n","rgb_data: torch.Size([435, 3, 210, 160]), language_input: torch.Size([435, 512, 64])\n"]}],"source":["\n","import numpy as np\n","import torch\n","\n","def format_language_rgb_input(rgb_data, dom_data, utterance_data, task_data):\n","    # Put language task together\n","    language_input = np.concatenate((dom_data, utterance_data, task_data), axis=1)\n","    #language_input = dom_data\n","    print(language_input.shape)\n","\n","    rgb_data = torch.from_numpy(rgb_data).type(torch.float32)\n","    language_input = torch.from_numpy(language_input).type(torch.long)\n","    language_input = tokenizer.embedding_fn(language_input) # Create embeddings for language\n","    print(f'rgb_data: {rgb_data.shape}, language_input: {language_input.shape}')\n","    return rgb_data, language_input\n","\n","train_rgb_data, train_language_input = format_language_rgb_input(rgb_data=train_rgb, dom_data=train_dom, utterance_data=train_utterance, task_data=train_task_name)\n","test_rgb_data, test_language_input = format_language_rgb_input(rgb_data=test_rgb, dom_data=test_dom, utterance_data=test_utterance, task_data=test_task_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1310,"status":"ok","timestamp":1685715086792,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"eQnskJFbP3Ig","outputId":"dde70feb-b7c7-4178-ac1d-d38b30ad4941"},"outputs":[{"output_type":"stream","name":"stdout","text":["Got TRAIN 3923 formatted_previous_actions, type: float64\n","formatted_previous_actions: torch.Size([3923, 577]), torch.float64\n","Got TEST 435 formatted_previous_actions, type: float64\n","formatted_previous_actions: torch.Size([435, 577]), torch.float64\n"]}],"source":["\n","# Test formatting of previous action sequence data.\n","# TODO: need to update the dataloader with them\n","def format_previous_actions(episode_previous_actions, tokenizer, dataset_loader):\n","    formatted_previous_actions = []\n","\n","    # Process one by one the different entries to have them correctly matching the dataset\n","    for entry in episode_previous_actions:\n","        # No previous action, have default empty sequence\n","        if len(entry) == 0:\n","            empty_action = np.array(tokenizer.truncate_pad_entry(tokenizer.tokenize_string(''), dataset_loader.model_output_size))\n","            formatted_previous_actions.append(empty_action)\n","        else:\n","            action_type = torch.from_numpy(entry[0]).type(torch.long)\n","            ref = torch.from_numpy(entry[1]).type(torch.long)\n","            ref = torch.flatten(tokenizer.embedding_fn(ref))\n","            keydown = torch.flatten(tokenizer.embedding_fn(torch.from_numpy(entry[2]).type(torch.long)))\n","            together = np.concatenate((action_type.detach().numpy(), ref.detach().numpy(), keydown.detach().numpy()))\n","            formatted_previous_actions.append(together)\n","\n","    formatted_previous_actions = np.array(formatted_previous_actions)\n","    return formatted_previous_actions\n","\n","# These are the formatted actions to use when concatenating after the multimodal layers.\n","train_formatted_previous_actions = format_previous_actions(episode_previous_actions=train_episode_previous_action, tokenizer=tokenizer, dataset_loader=dataset_loader)\n","print(f'Got TRAIN {len(train_formatted_previous_actions)} formatted_previous_actions, type: {train_formatted_previous_actions.dtype}')\n","# into tensor\n","train_formatted_previous_actions = torch.from_numpy(train_formatted_previous_actions)\n","print(f'formatted_previous_actions: {train_formatted_previous_actions.shape}, {train_formatted_previous_actions.dtype}')\n","\n","# These are the formatted actions to use when concatenating after the multimodal layers.\n","test_formatted_previous_actions = format_previous_actions(episode_previous_actions=test_episode_previous_action, tokenizer=tokenizer, dataset_loader=dataset_loader)\n","print(f'Got TEST {len(test_formatted_previous_actions)} formatted_previous_actions, type: {test_formatted_previous_actions.dtype}')\n","# into tensor\n","test_formatted_previous_actions = torch.from_numpy(test_formatted_previous_actions)\n","print(f'formatted_previous_actions: {test_formatted_previous_actions.shape}, {test_formatted_previous_actions.dtype}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1685715088871,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"AgM16DtCP49C","outputId":"c71bbd3a-f3cc-4088-c138-a3126ad8d32b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([3923, 577]), torch.Size([435, 577]))"]},"metadata":{},"execution_count":12}],"source":["\n","# Produces target labels.\n","# TODO: also use it to format the T5 output text to feed CC-NeT5\n","def produce_labels(target_action, target_refs, target_keydown, tokenizer):\n","    i = 0\n","    labels = []\n","\n","    while i<len(target_action):\n","        # Tokenize refs and keydowns\n","        embedded_action = torch.from_numpy(target_action[i]).detach().numpy()\n","        embedded_ref = torch.flatten(tokenizer.embedding_fn(torch.from_numpy(target_refs[i]))).detach().numpy()\n","        embedded_keydown = torch.flatten(tokenizer.embedding_fn(torch.from_numpy(target_keydown[i]))).detach().numpy()\n","\n","        label = np.concatenate((embedded_action, embedded_ref, embedded_keydown), axis=0)\n","        labels.append(label)\n","        i += 1\n","\n","    labels = np.array(labels)\n","    labels = torch.from_numpy(labels).type(torch.float32)\n","    return labels\n","\n","train_labels = produce_labels(train_target_action, train_target_ref, train_target_keydown, tokenizer)\n","test_labels = produce_labels(test_target_action, test_target_ref, test_target_keydown, tokenizer)\n","train_labels.shape, test_labels.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lDY6me_P6pe"},"outputs":[],"source":["# Batchify data\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#batched_rgb = dataset_loader.get_dataloder(dataset=rgb_data.to(torch.device(device)))\n","#batched_language = dataset_loader.get_dataloder(dataset=language_input.to(torch.device(device)))\n","#batched_previous_actions = dataset_loader.get_dataloder(dataset=formatted_previous_actions.to(torch.device(device)))\n","#batched_labels = dataset_loader.get_dataloder(dataset=labels.to(torch.device(device)))\n","\n","# Batchify data\n","train_batched_rgb = dataset_loader.get_dataloder(dataset=train_rgb_data.to(torch.device(device)))\n","train_batched_language = dataset_loader.get_dataloder(dataset=train_language_input.to(torch.device(device)))\n","train_batched_previous_actions = dataset_loader.get_dataloder(dataset=train_formatted_previous_actions.to(torch.device(device)))\n","train_batched_labels = dataset_loader.get_dataloder(dataset=train_labels.to(torch.device(device)))\n","\n","test_batched_rgb = dataset_loader.get_dataloder(dataset=test_rgb_data.to(torch.device(device)))\n","test_batched_language = dataset_loader.get_dataloder(dataset=test_language_input.to(torch.device(device)))\n","test_batched_previous_actions = dataset_loader.get_dataloder(dataset=test_formatted_previous_actions.to(torch.device(device)))\n","test_batched_labels = dataset_loader.get_dataloder(dataset=test_labels.to(torch.device(device)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8CHEl02bP8MI"},"outputs":[],"source":["# Some model classes\n","\n","'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3,3),\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=14):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 3\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=2)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        #self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        #out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(x)\n","        dl1 = out.shape\n","        out = self.layer2(out)\n","        dl2 = out.shape\n","        out = self.layer3(out)\n","        dl3 = out.shape\n","        out = self.layer4(out)\n","        dl4 = out.shape\n","        #out = F.avg_pool2d(out, 4)\n","        #out = out.view(out.size(0), -1)\n","\n","        # Flatten (batch size, channels, features)\n","        out = out.view(batch_size, 512, 140)\n","        out_1_s = out.shape\n","        # Might have to change the last layer to obtain our 14x11 feature vector?\n","        #out = self.linear(out)\n","        #print(f'out_1_s: {out_1_s}, out: {out.shape}')\n","        #print(f'dl1: {dl1}, dl2: {dl2}, dl3: {dl3}, dl4: {dl4}, out: {out_1_s}')\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","# ----------------------------------\n","# Cross Attention Model for Language\n","class CrossAttentionModelLanguage(nn.Module):\n","    # Changed hidden_dim from 140 to 1\n","    def __init__(self, input_dim=64, hidden_dim=1, num_heads=4):\n","        super(CrossAttentionModelLanguage, self).__init__()\n","\n","        self.attention = nn.MultiheadAttention(input_dim, num_heads)\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","\n","    def forward(self, input):\n","        # Reshape the input to (sequence_length, batch_size, input_dim)\n","        input = input.permute(1, 0, 2)\n","\n","        # Apply cross-attention\n","        output, _ = self.attention(input, input, input)\n","\n","        # Reshape the output to (batch_size, sequence_length, input_dim)\n","        output = output.permute(1, 0, 2)\n","\n","        # Apply linear transformation\n","        output = self.linear(output)\n","\n","        return output\n","\n","\n","#--------------------------------\n","# Multimodal Transformer Network \n","from torch.nn import Transformer\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self):\n","        super(TransformerNetwork, self).__init__()\n","\n","        self.embedding_dim = 512\n","        self.num_layers = 8\n","        self.num_heads = 8\n","\n","        self.embedding = nn.Linear(142, self.embedding_dim)\n","        self.transformer = Transformer(\n","            d_model=self.embedding_dim,\n","            nhead=self.num_heads,\n","            num_encoder_layers=self.num_layers,\n","            num_decoder_layers=self.num_layers\n","        )\n","        #self.output_layer = nn.Linear(self.embedding_dim, 1)\n","\n","    def forward(self, input_tensor):\n","        batch_size = input_tensor.size(0)\n","\n","        embedded = self.embedding(input_tensor)\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        output = self.transformer(embedded, embedded)\n","        output = output.permute(1, 0, 2)\n","        #output = self.output_layer(output)\n","\n","        return output.squeeze(2)\n","\n","#-------------------\n","# LSTM Netowkr Part\n","class TwoLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(TwoLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        # Note: we use a Linear layer for dimension adjustment.\n","        # Indeed, the previous multi-modal transformer outputs a 1024 + prev_action length tensor,\n","        # and here the LSTM layers have a 512 dim where we also need to bypass residual connections.\n","        # We use the linear layer below to have a matching size with the input/output of these LSTMs\n","        # in order to concatenate them.\n","        self.linear = nn.Linear(input_size, hidden_size)\n","\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2, batch_first=True)\n","\n","        # This linear layer is for the final mapping\n","        #self.fc = nn.Linear(hidden_size, output_size) #fully connected last layer\n","\n","    def forward(self, input):\n","        # Perform dimension adjustment using the linear layer\n","        adjusted_input = self.linear(input)\n","        #print(f'adjusted_input: {adjusted_input.shape}')\n","\n","        # Perform the forward pass through each LSTM layer\n","        output, _hidden_state = self.lstm(adjusted_input)\n","\n","        # Perform residual connections between LSTM layers\n","        residual_output = adjusted_input + output\n","\n","        # Extract the hidden state of the last LSTM layer after residual connections\n","        last_hidden_state = residual_output[:, -1, :]\n","\n","        #out = self.fc(last_hidden_state)\n","        return last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjYkRg6pP9Vh"},"outputs":[],"source":["\n","import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","class CCNeT5(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        #self.rgb_model = nn.Sequential(*resnet_blocks)\n","        self.rgb_model = ResNet18()\n","        self.language_model = CrossAttentionModelLanguage()\n","\n","        # Multi Modal Part Combining RGB + Language\n","        #self.multimodal_transformer = MultiModalTransformer()\n","        self.multimodal_transformer = TransformerNetwork()\n","\n","        # Here we should concatenate with the previous Action\n","        # when doing the forward pass\n","\n","        # Two-Layer LSTM\n","        self.lstm = TwoLSTM(1089, 513)\n","\n","        # Last output Layer to create a proper action space from the original language embeddings\n","        self.fc = nn.Linear(513, 577) #fully connected last layer\n","\n","        # T5 Decreased\n","        # Linear layer to reduce the size of the T5 output frm 577 to 512\n","        self.fc_t5 = nn.Linear(577, 512)\n","\n","\n","    # todo: pass previous action\n","    def forward(self, rgb_input, language_input, previous_action, t5_output):\n","        # Process RGB input\n","        rgb_output = self.rgb_model(rgb_input)\n","\n","        # Process language input\n","        language_output = self.language_model(language_input)\n","\n","        #print(f'shape rbg_output: {rgb_output.shape}, shape language_output: {language_output.shape}')\n","        flattened_tensor1 = rgb_output.view(batch_size, 512*140)\n","        flattened_tensor2 = language_output.view(batch_size, -1)\n","\n","        # Concatenate tensor1 and flattened_tensor2 along the last dimension\n","        fused_output = torch.cat((flattened_tensor1, flattened_tensor2), dim=1).view(batch_size, 512, 141)\n","\n","        # Add T5 output to image and language tensor\n","        t5_output = self.fc_t5(t5_output).unsqueeze(dim=2) # Change shape from (batch, 577) to (batch, 512, 1)\n","        # Concatenate with fused output\n","        fused_output = torch.cat((fused_output, t5_output), dim=2)\n","\n","        # Further processing or fusion of modalities\n","        #print(f'fused_output: {fused_output.shape} - {fused_output.dtype}')\n","        # Additional processing steps...\n","\n","        # Permute\n","        #fused_output = fused_output.permute(0, 2, 1)\n","\n","        # Feed into Multimodal Transformer\n","        multimodal_output = self.multimodal_transformer(fused_output)\n","        #print(f'multimodal_output: {multimodal_output.shape} - type: {multimodal_output.dtype}')\n","\n","        # TODO: concatenate with previous action\n","        #print(f'previous_action: {previous_action.shape}')\n","        #tensor2_reshaped = F.pad(previous_action, (0, 447), mode='constant')\n","        #tensor2_expanded = torch.unsqueeze(tensor2_reshaped, dim=2)  # Shape: [8, 1024, 577]\n","        #combined_tensor = torch.cat((multimodal_output, tensor2_expanded), dim=2).type(torch.float32) # Shape: [8, 1024, 1089]\n","\n","        tensor2 = previous_action.unsqueeze(1)\n","        # Expand tensor2 dimensions to match tensor1\n","        tensor2 = tensor2.expand(-1, 512, -1)\n","        # Combine the tensors\n","        combined_tensor = torch.cat((multimodal_output, tensor2), dim=2).type(torch.float32)\n","        #print(f'concatenation previous action: {combined_tensor.shape}')\n","\n","        # Feed into LSTM\n","        # + Permute to fit the right format\n","        #lstm_output = self.lstm(combined_tensor.permute(0, 2, 1))\n","        lstm_output = self.lstm(combined_tensor)\n","        #print(f'lstm_output: {lstm_output.shape}')\n","\n","        # Last layer to target action space\n","        action_output = self.fc(lstm_output)\n","\n","        return action_output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":616,"status":"ok","timestamp":1685715107605,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-60},"id":"V8dNDQ7cQrp9","outputId":"e7392699-f024-4dc6-fada-017a328cc678"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CCNeT5(\n","  (rgb_model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","  )\n","  (language_model): CrossAttentionModelLanguage(\n","    (attention): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","    )\n","    (linear): Linear(in_features=64, out_features=1, bias=True)\n","  )\n","  (multimodal_transformer): TransformerNetwork(\n","    (embedding): Linear(in_features=142, out_features=512, bias=True)\n","    (transformer): Transformer(\n","      (encoder): TransformerEncoder(\n","        (layers): ModuleList(\n","          (0-7): 8 x TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (decoder): TransformerDecoder(\n","        (layers): ModuleList(\n","          (0-7): 8 x TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (lstm): TwoLSTM(\n","    (linear): Linear(in_features=1089, out_features=513, bias=True)\n","    (lstm): LSTM(513, 513, num_layers=2, batch_first=True)\n","  )\n","  (fc): Linear(in_features=513, out_features=577, bias=True)\n","  (fc_t5): Linear(in_features=577, out_features=512, bias=True)\n",")"]},"metadata":{},"execution_count":16}],"source":["multimodal = CCNeT5()\n","multimodal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QhnGZqmSQyIh"},"outputs":[],"source":["\n","import torch.optim as optim\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","multimodal.to(device)\n","loss_function = nn.MSELoss()\n","optimizer = optim.Adam(multimodal.parameters(), lr=1e-4)\n","losses = []\n","\n","def train_model(model, batched_rgb, batched_language, batched_previous_actions, batched_labels):\n","    i = 0\n","    total_iters = len(train_batched_rgb)\n","    print(f'total_iters/', end='')\n","    cumulated_loss = 0\n","    for rgb_data, language_data, previous_action, targets in zip(batched_rgb, batched_language, batched_previous_actions, batched_labels):\n","        optimizer.zero_grad()\n","\n","        print('#', end='')\n","\n","        # T5 Input\n","        # Here we put the input from the T5 model, for now the targets in the first offline patch\n","        t5_output = targets.clone()\n","\n","        #print(rgb_data.dtype, language_data.dtype, previous_action.dtype, targets.dtype)\n","\n","        #print(f'iteration {i}')\n","        if rgb_data.shape[0] != batch_size or previous_action.shape[0] != batch_size:\n","            # If we have a smaller batch, discard it as we don't handle it currently\n","            #print(f'current batch size: {rgb_data.shape[0]}')\n","            continue\n","        #print(rgb_data.shape)\n","        #print(language_data.shape)\n","        rgb_data = rgb_data.to(device)\n","        language_data = language_data.to(device)\n","        action_output = model.forward(rgb_data, language_data, previous_action, t5_output)\n","        #print(f'action_output: {action_output.shape} - {action_output.dtype}')\n","        #print(f'targets: {targets.shape} - {targets.dtype}')\n","\n","        loss = loss_function(action_output, targets)\n","        loss.backward(retain_graph=True)\n","        optimizer.step()\n","        cumulated_loss += loss.item()\n","        #print(f'{i}/{len(total_iters)}')\n","        i += 1\n","\n","# Test Model\n","def test_model(model, batched_rgb, batched_language, batched_previous_actions, batched_labels):\n","    i = 0\n","    total_iters = len(batched_rgb)\n","    print(f'Running Evaluation Test: {total_iters}/', end='')\n","    cumulated_loss = 0\n","\n","\n","    for rgb_data, language_data, previous_action, targets in zip(batched_rgb, batched_language, batched_previous_actions, batched_labels):\n","        optimizer.zero_grad()\n","\n","        print('#', end='')\n","\n","        # T5 Input\n","        # Here we put the input from the T5 model, for now the targets in the first offline patch\n","        t5_output = targets.clone()\n","\n","        #print(f'iteration {i}')\n","        if rgb_data.shape[0] != batch_size:\n","            # If we have a smaller batch, discard it as we don't handle it currently\n","            #print(f'current batch size: {rgb_data.shape[0]}')\n","            continue\n","        #print(rgb_data.shape)\n","        #print(language_data.shape)\n","        rgb_data = rgb_data.to(device)\n","        language_data = language_data.to(device)\n","        action_output = model.forward(rgb_data, language_data, previous_action, t5_output)\n","        #print(f'action_output: {action_output.shape} - {action_output.dtype}')\n","        #print(f'targets: {targets.shape} - {targets.dtype}')\n","        #print(f'Eval {i} / {total_iters}')\n","\n","        cumulated_loss += loss_function(action_output, targets).item()\n","        i += 1\n","    return cumulated_loss / i\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fC1R4yESQ0YP","outputId":"13f8b6e7-8ddc-406c-a18c-aaf49812cb53"},"outputs":[{"output_type":"stream","name":"stdout","text":["total_iters/##########################################################################################################################################################################################################################################################################################"]}],"source":["epochs = 10\n","i = 0\n","losses = []\n","while i < epochs:\n","    train_model(multimodal, train_batched_rgb, train_batched_language, train_batched_previous_actions, train_batched_labels)\n","    epoch_loss = test_model(multimodal, test_batched_rgb, test_batched_language, test_batched_previous_actions, test_batched_labels)\n","\n","    print(f'Epoch {i} has loss {epoch_loss}')\n","\n","    # Saves the entire model\n","    torch.save(multimodal, f'/content/drive/MyDrive/WebAI/Model Weights/CCNeT5_OffLine1/CCNeT5_{i}')\n","    losses.append(epoch_loss)\n","    i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yD3dVbMtbIy-"},"outputs":[],"source":["#import gc\n","#del multimodal\n","#gc.collect()\n","#torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":764},"executionInfo":{"elapsed":31759,"status":"error","timestamp":1685568358757,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"UrKqaOY-ZDL-","outputId":"6ce81325-6a6f-4525-f8d6-5b43ca0041c5"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-3a9fd5ea76a9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             len(x.shape) < 1):\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","plt.plot(losses)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xkzBaB2Q3-n"},"outputs":[],"source":["# Saves the entire model\n","torch.save(multimodal, '/content/drive/MyDrive/WebAI/Model Weights/CCNeT5_Smaller_1_500.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-41KxwMV10M"},"outputs":[],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FC7jehKX5xB"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNVyJae/1Sb2tKlvARAJOvv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"11775b1adab44da6972430ef0c04af39":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_f9fc446c500945c4a547141986e53641","IPY_MODEL_6315119121ec41819a364ac5f3ba1c00","IPY_MODEL_031694b3b0104a77832a46fb1748fdac","IPY_MODEL_b1440da631ef447cacb5dd4947bf3e62","IPY_MODEL_79a2022bdb614c3abc515491b91f9c66"],"layout":"IPY_MODEL_3c1c149ddbc24f05b29a2a46de43dca4"}},"f9fc446c500945c4a547141986e53641":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8047da4b7bdb4afd864fde7fb1d2c640","placeholder":"​","style":"IPY_MODEL_3bbeef06874d46dd8282bb7473da123e","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"6315119121ec41819a364ac5f3ba1c00":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_94a11a90bab449d9bfd07df094893ced","placeholder":"​","style":"IPY_MODEL_1a93e87881394d9ab38e490616573712","value":""}},"031694b3b0104a77832a46fb1748fdac":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_b0b15b7ff7c2407e82c07d85a8e727ec","style":"IPY_MODEL_31734fdde65a4899a418140a3fdf125d","value":true}},"b1440da631ef447cacb5dd4947bf3e62":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_d4ccc3ece19e4615ad74ba3d0a85e96b","style":"IPY_MODEL_6f375bdc78e44c2891a6624949377453","tooltip":""}},"79a2022bdb614c3abc515491b91f9c66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd0506911029466ca4a92471565f2fc0","placeholder":"​","style":"IPY_MODEL_74d225ac23ff4a9a94288ea7bd801615","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"3c1c149ddbc24f05b29a2a46de43dca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"8047da4b7bdb4afd864fde7fb1d2c640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bbeef06874d46dd8282bb7473da123e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94a11a90bab449d9bfd07df094893ced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a93e87881394d9ab38e490616573712":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0b15b7ff7c2407e82c07d85a8e727ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31734fdde65a4899a418140a3fdf125d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4ccc3ece19e4615ad74ba3d0a85e96b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f375bdc78e44c2891a6624949377453":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"cd0506911029466ca4a92471565f2fc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74d225ac23ff4a9a94288ea7bd801615":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17d48d8680304637aa8e51f69b6c2931":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b53a3bde80fe4db1af60dffba8c72a28","IPY_MODEL_3d919abbfa7e4815ae6ac1b24bda29cd","IPY_MODEL_badbd6d51aac44428947775c2d57909e"],"layout":"IPY_MODEL_2e37e20454e24b928ba61bdfd975fad2"}},"b53a3bde80fe4db1af60dffba8c72a28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bcf1f95e062478f83d5107da0c5721e","placeholder":"​","style":"IPY_MODEL_4ab152a019dd426095563c39c673e097","value":"Downloading readme: 100%"}},"3d919abbfa7e4815ae6ac1b24bda29cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_304c6bb29a2748d8be9be2e798eab702","max":548,"min":0,"orientation":"horizontal","style":"IPY_MODEL_132cfb8f7cf14f928cdffc83ba3e088c","value":548}},"badbd6d51aac44428947775c2d57909e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b6bbbf2e93441f296e7118771396e2a","placeholder":"​","style":"IPY_MODEL_c4cd41ae0b164d1791c9a49425b99d52","value":" 548/548 [00:00&lt;00:00, 11.8kB/s]"}},"2e37e20454e24b928ba61bdfd975fad2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bcf1f95e062478f83d5107da0c5721e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ab152a019dd426095563c39c673e097":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"304c6bb29a2748d8be9be2e798eab702":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"132cfb8f7cf14f928cdffc83ba3e088c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b6bbbf2e93441f296e7118771396e2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4cd41ae0b164d1791c9a49425b99d52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fe3e3a530b94b0994723a0c8aebb80d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e03a6229e28d48e88facf3a522ce78f9","IPY_MODEL_b1e09b05fbaf4968929fea77038f9163","IPY_MODEL_8003dc4039374812b427b46652668ab3"],"layout":"IPY_MODEL_8ad232d03db842da986adc7eb81dda07"}},"e03a6229e28d48e88facf3a522ce78f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7642ec345354e838e091252ec0f5951","placeholder":"​","style":"IPY_MODEL_b59c8e100f3a46e2bf92ec8ef85a2b13","value":"Downloading data files: 100%"}},"b1e09b05fbaf4968929fea77038f9163":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c673630246cd4386a0cedb84958f84b7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ac55d11598b4b00b6e45db7b299e41b","value":1}},"8003dc4039374812b427b46652668ab3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11a2abbd921746a89ab4f568693de7b4","placeholder":"​","style":"IPY_MODEL_3747bc89bcdd41dd8c030f68ad7f21db","value":" 1/1 [00:01&lt;00:00,  1.78s/it]"}},"8ad232d03db842da986adc7eb81dda07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7642ec345354e838e091252ec0f5951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b59c8e100f3a46e2bf92ec8ef85a2b13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c673630246cd4386a0cedb84958f84b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ac55d11598b4b00b6e45db7b299e41b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11a2abbd921746a89ab4f568693de7b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3747bc89bcdd41dd8c030f68ad7f21db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f84295e7ce94ee8809d519ce1597665":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f0e9731c1524d16a0190d6dbef5d9ac","IPY_MODEL_24be3dc82dd44f5baf60a07a4d825f68","IPY_MODEL_119bfddeeb0b418c8f88a90907b211f6"],"layout":"IPY_MODEL_edbe6c40566044409532032c1d5f0253"}},"9f0e9731c1524d16a0190d6dbef5d9ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_248e7b6b48f340b5bd20731e89561a56","placeholder":"​","style":"IPY_MODEL_3755638837cc41f3a6ad3980c9f469f0","value":"Downloading data: 100%"}},"24be3dc82dd44f5baf60a07a4d825f68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24d7075adaf34c20b2fa1f8d6257ceeb","max":55056820,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f9ef33c77334828a022b69afd1a85ee","value":55056820}},"119bfddeeb0b418c8f88a90907b211f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4c19511eb784624b3e4404812324dbc","placeholder":"​","style":"IPY_MODEL_789669b20b964686a4c43e6981b23758","value":" 55.1M/55.1M [00:00&lt;00:00, 79.5MB/s]"}},"edbe6c40566044409532032c1d5f0253":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"248e7b6b48f340b5bd20731e89561a56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3755638837cc41f3a6ad3980c9f469f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24d7075adaf34c20b2fa1f8d6257ceeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f9ef33c77334828a022b69afd1a85ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4c19511eb784624b3e4404812324dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"789669b20b964686a4c43e6981b23758":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5858d2b7ec2444da9f9b5bf090757b98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d25e129c3d4d43c8a716bfa22b8c4d09","IPY_MODEL_46201f31dace4c45b7cc4adfa5cc3942","IPY_MODEL_ffa849f6f3d647ca90b4c9adfe2af29b"],"layout":"IPY_MODEL_9231f35c02c545eebbf82d7df217a9d9"}},"d25e129c3d4d43c8a716bfa22b8c4d09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebe18a35312545a98b4d0ce68f6b982e","placeholder":"​","style":"IPY_MODEL_c4766104664949faa65387a0f867b005","value":"Extracting data files: 100%"}},"46201f31dace4c45b7cc4adfa5cc3942":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4039c0b7a1504c739ce37472ba741ebe","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f51c05b3062a414a85e699a76cc5a12c","value":1}},"ffa849f6f3d647ca90b4c9adfe2af29b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3f1b0a2dcf448cf8b56cc8676dce04f","placeholder":"​","style":"IPY_MODEL_beaffea6ad60474ea5d5e1e5db49ab43","value":" 1/1 [00:00&lt;00:00, 21.85it/s]"}},"9231f35c02c545eebbf82d7df217a9d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebe18a35312545a98b4d0ce68f6b982e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4766104664949faa65387a0f867b005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4039c0b7a1504c739ce37472ba741ebe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f51c05b3062a414a85e699a76cc5a12c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3f1b0a2dcf448cf8b56cc8676dce04f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"beaffea6ad60474ea5d5e1e5db49ab43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f34da6ad772344b891dd3f0f52bc45c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_314c6d4c0e08450ba9ffefa95ad86193","IPY_MODEL_4a1d6ee1ac0548d5870810b074c8b26c","IPY_MODEL_92d6642a9d104e399be41c2d2412e43e"],"layout":"IPY_MODEL_6aeef4679c0c4c5389f0f2317b174f4b"}},"314c6d4c0e08450ba9ffefa95ad86193":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ad5260b093b4232b7e1c0fbe57c2c4a","placeholder":"​","style":"IPY_MODEL_b344c5ae536b45359969c879477ed19d","value":"Generating train split: 100%"}},"4a1d6ee1ac0548d5870810b074c8b26c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebd8baf26fb3407a9db15e4848306edd","max":13412,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7fb28d4fed54b29ae10be6509b1baff","value":13412}},"92d6642a9d104e399be41c2d2412e43e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_360ca6b9657845ca9a1391f7355393a9","placeholder":"​","style":"IPY_MODEL_3f8f77d4c3234a7485c664242ed6aada","value":" 13412/13412 [00:02&lt;00:00, 6105.73 examples/s]"}},"6aeef4679c0c4c5389f0f2317b174f4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"3ad5260b093b4232b7e1c0fbe57c2c4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b344c5ae536b45359969c879477ed19d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebd8baf26fb3407a9db15e4848306edd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7fb28d4fed54b29ae10be6509b1baff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"360ca6b9657845ca9a1391f7355393a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f8f77d4c3234a7485c664242ed6aada":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab20a523c45f4970be6cce7e05ce98f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f62dd01613e4801a543e1203805dc18","IPY_MODEL_f5fec2008849450ab3845a23a99991e0","IPY_MODEL_7c64056cdb21443ca2f39bd673698347"],"layout":"IPY_MODEL_26dd5d60dab448ef958dc3f82534ade9"}},"7f62dd01613e4801a543e1203805dc18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_264319373cef4f38ab3c991f856a2afe","placeholder":"​","style":"IPY_MODEL_cb746be1db56474daba6c9ba953c4498","value":"100%"}},"f5fec2008849450ab3845a23a99991e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_959159e432b14a89a87f44782f3f03eb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2bb1c90364142aba20e91cb622793f0","value":1}},"7c64056cdb21443ca2f39bd673698347":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4c2932ff2a24cb0849606502fa47f5c","placeholder":"​","style":"IPY_MODEL_38a971863fdc437ca7ef96565c07930b","value":" 1/1 [00:00&lt;00:00, 17.17it/s]"}},"26dd5d60dab448ef958dc3f82534ade9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"264319373cef4f38ab3c991f856a2afe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb746be1db56474daba6c9ba953c4498":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"959159e432b14a89a87f44782f3f03eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2bb1c90364142aba20e91cb622793f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4c2932ff2a24cb0849606502fa47f5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38a971863fdc437ca7ef96565c07930b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}