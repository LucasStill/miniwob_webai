{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xt80bCxE8R2s","outputId":"495aed1d-4a80-4463-f236-d161fb96d3d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers datasets\n","from datasets import load_dataset\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":340},"collapsed":true,"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1685471156297,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"lk8x45iT8IQk"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-97c1b12d350b\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LucasThil/randomized_clean_miniwob_episodes_v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import numpy as np\n","from datasets import load_dataset\n","dataset = load_dataset(\"LucasThil/randomized_clean_miniwob_episodes_v2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1685471156297,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"Y8Aakc9nhFav","outputId":"3f59ffee-10f5-4e37-f740-e0bed5b99759"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-3075b36c09c8\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"]}],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RFnIrebQ2Aw"},"outputs":[],"source":["#drive.flush_and_unmount()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23568,"status":"ok","timestamp":1685450863808,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"ZSFmsXxh8gAd","outputId":"2a3c589e-a04c-47b3-e798-416c32f87ce5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"bP1waJ389zdy"},"source":["# Language Tokenizer Class + Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5770,"status":"ok","timestamp":1685450872101,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"zWATMBZ-9Gge","outputId":"742ea6a2-9fc7-403f-b6e1-77b1df904d77"},"outputs":[{"name":"stdout","output_type":"stream","text":["pad: 1590\n","Loaded CC_NeT5 Tokenizer with vocabulary size being 1591.\n"]}],"source":["# 1 - Get out Tokenizer Class\n","css_fields = ['top', 'left', 'width', 'height']\n","special_characters = ['.', ',', '#', ':', '-', '/', '(', ')', 'https://', '@', '\u0026', '\"', \"'\", '!', '?', ';', '+', '=',\n","                      '*', '$', '€', '*', '`']\n","\n","\n","def round_to_nearest_ten(number):\n","    return round(number / 10) * 10\n","\n","import torch.nn as nn\n","import torch\n","\n","# Embedding Function\n","class EmbeddingFunction(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(EmbeddingFunction, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","    def forward(self, input_tokens):\n","        embedded = self.embedding(input_tokens)\n","        return embedded\n","\n","    # Provide a tensor and de-embbeds it to retrieve the correct index\n","    def get_embedding_index(self, x):\n","        results = torch.where(torch.sum((self.embedding.weight == x), axis=1))\n","        if len(results[0]) == len(x):\n","            return None\n","        else:\n","            return results[0][0]\n","\n","# Turn it into a class\n","class CCNeT5Tokenizer:\n","    def __init__(self, vocab_path):\n","        stoi = {}\n","        itos = {}\n","        self.padding_char = '\u003cPAD\u003e'\n","        self.special_characters = ['.', ',', '#', ':', '-', '/', '(', ')', 'https://', '@', '\u0026', '\"', \"'\", '!', '?',\n","                                   ';', '+', '=', '*', '$', '€', '*', '`']\n","\n","        with open(vocab_path, 'r') as file:\n","            for index, line in enumerate(file):\n","                line = line.strip()\n","                stoi[line] = index\n","                itos[index] = line\n","\n","        stoi[' '] = stoi['']\n","        itos[stoi[' ']] = ' '\n","        self.stoi = stoi\n","        self.stoi[self.padding_char] = len(stoi.keys())  # Add PADDING character\n","        # We do not need itos as we don't implement a de-tokinezing function.\n","        self.itos = itos\n","        self.itos[len(stoi.keys()) - 1] = self.padding_char  # Add PADDING character\n","        print(f'pad: {len(stoi.keys()) - 1}')\n","        print(f'Loaded CC_NeT5 Tokenizer with vocabulary size being {len(self.stoi)}.')\n","\n","        # Instantiate the embedding function\n","        vocab_size = len(stoi)\n","        self.embedding_dim = 64\n","        self.embedding_fn = EmbeddingFunction(vocab_size, self.embedding_dim)\n","\n","    # Provide a string and tokenize it: utterance or task name.\n","    def tokenize_string(self, string):\n","        tokenized_string = []\n","        for w in string.lower().split(' '):\n","            for sc in self.special_characters:\n","                if sc in w:\n","                    w = w.replace(sc, 'Ø' + sc + 'Ø')\n","            p1_words = [s for s in w.split('Ø') if s != '']\n","            # Check if the found words exist in our vocab, else subdivide\n","            for fw in p1_words:\n","                if fw not in self.stoi.keys():\n","                    for c in fw:\n","                        tokenized_string.append(self.stoi[c])\n","                else:\n","                    tokenized_string.append(self.stoi[fw])\n","        return tokenized_string\n","\n","    def get_tokens_from_embeddings(self, embedded_tokens):\n","        indices = torch.Tensor(list(map(self.embedding_fn.get_embedding_index, embedded_tokens)))\n","        return indices\n","\n","    # Turns an array into a string\n","    def detokenize_array(self, array):\n","        reconstruted_string = []\n","        #pad_index = self.itos[self.stoi[self.padding_char]]\n","        for v in array:\n","            v = int(v)\n","            s = self.itos[v]\n","            if s != self.padding_char:\n","                reconstruted_string.append(s)\n","\n","        #reconstruted_string = ' '.join(reconstruted_string)\n","        cleaned_string = ''\n","        last_single = False\n","        for s in reconstruted_string:\n","            if len(s) \u003e 1:\n","                if last_single:\n","                    cleaned_string += ' '\n","                    last_single = False\n","                cleaned_string += s + ' '\n","            else:\n","                # We do this to treat single length characters\n","                cleaned_string += s\n","                last_single = True\n","        return cleaned_string\n","\n","    # Truncate and pad the incoming sentences based on a max_size argument\n","    def truncate_pad_entry(self, tokenized_array, max_size):\n","        if len(tokenized_array) \u003c max_size:\n","            while len(tokenized_array) \u003c max_size:\n","                tokenized_array.append(self.stoi[self.padding_char])\n","            return tokenized_array\n","        elif len(tokenized_array) \u003e max_size:\n","            # Too long, so we automatically truncate\n","            return tokenized_array[:max_size]\n","        else:\n","            # Just avoid iterating under the hood\n","            return tokenized_array\n","\n","    # Tokenize a DOM dictionary\n","    def tokenize_dom(self, dom):\n","        tokenized_dom = []\n","\n","        # Add opening tag\n","        element_tag = dom['tag'].lower()\n","        if 'input' in dom.keys():\n","            # if tag is input, reformat it\n","            element_tag = dom['input'].lower() + '_' + dom['type'].lower()\n","            tokenized_dom.append(self.stoi['\u003c' + element_tag])\n","        else:\n","            # add normal tag\n","            tokenized_dom.append(self.stoi['\u003c' + dom['tag'].lower()])\n","\n","        for field in dom:\n","            if field != 'tag' and field != 'children' and field != 'type' and field != 'text' and field not in css_fields:  # and field != 'value':\n","\n","                tokenized_dom.append(self.stoi[field.lower()])\n","\n","                # Ensure we don't have float values, we'll stick with integers\n","                if isinstance(dom[field], float):\n","                    tokenized_dom.append(self.stoi[str(int(round_to_nearest_ten(dom[field])))])\n","                else:\n","                    words = str(dom[field]).lower().split(' ')\n","                    for word in words:\n","                        for sc in special_characters:\n","                            if sc in word:\n","                                word = word.replace(sc, 'Ø' + sc + 'Ø')\n","                        p1_words = [s for s in word.split('Ø') if s != '']\n","\n","                        # decides whether we keep the full word, or just the letters:\n","                        if field == 'value' or (False and (field == 'label' or field == 'button')):\n","                            for w in p1_words:\n","                                # Take individual characters of the value string\n","                                processed_words = [self.stoi[w[i:i + 1]] for i in range(0, len(w), 1)]\n","                                tokenized_dom += processed_words\n","\n","                        elif field == 'ref':\n","                            for w in p1_words:\n","                                # Take individual characters of the value string\n","                                processed_words = [self.stoi[w[i:i + 3]] for i in range(0, len(w), 3)]\n","                                tokenized_dom += processed_words\n","                        else:\n","                            # Use the full word:\n","                            for w in p1_words:\n","                                tokenized_dom.append(self.stoi[str(w)])\n","\n","            elif field == 'text':\n","                tokenized_dom.append(self.stoi['text'])\n","                for c in dom[field]:\n","                    tokenized_dom.append(self.stoi[c])\n","            elif field in css_fields:\n","                # Cast field\n","                tokenized_dom.append(self.stoi[field])\n","                css_value = int(round_to_nearest_ten(float(dom[field])))\n","                tokenized_dom.append(self.stoi[str(css_value)])\n","\n","        if 'children' in dom.keys():\n","            tokenized_dom.append(self.stoi['children'])\n","            for child in dom['children']:\n","                tokenized_dom += self.tokenize_dom(child)\n","                # for v in found_vocab:\n","                #    tokenized_dom.append(self.stoi[v])\n","\n","        # add closing tag\n","        tokenized_dom.append(self.stoi['\u003c/' + element_tag + '\u003e'])\n","\n","        return tokenized_dom\n","\n","vocab_path=\"/content/drive/MyDrive/WebAI/Notebooks/CC_NeT5/vocab.txt\"\n","tokenizer = CCNeT5Tokenizer(vocab_path=vocab_path)"]},{"cell_type":"markdown","metadata":{"id":"h1IOM-jX-NVI"},"source":["# Prepare DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vNtiuCC-C63"},"outputs":[],"source":["\n","\n","# Load some of the images we have here and try to save them to the dataset\n","import os\n","from typing import List, Tuple\n","import numpy as np\n","from PIL import Image\n","import torch\n","import pandas as pd\n","import re\n","import json\n","from torch.utils.data import DataLoader\n","\n","\n","class DatasetLoader:\n","    def __init__(self, screenshots_path, dataset, start_index, end_index, dom_tokenizer, batch_size):\n","        # Get the Subset of the dataset\n","        self.start_index = start_index\n","        self.end_index = end_index\n","        self.df = dataset['train'].to_pandas()[self.start_index:self.end_index]\n","        # When creating the class, parse JSON files into a state-based list structure\n","        self.df['processed_states'] = self.df['processed_states'].apply(lambda x: json.loads(re.sub(r'\\b(True|False)\\b', lambda m: m.group(0).lower(), x.replace(\"'\", '\"'))))\n","        print(f'Dataset subset has {len(self.df)} rows')\n","\n","        self.screenshots_path = screenshots_path\n","        self.indices, found_screenshots = self.get_file_indices()\n","        print(f'Found a total of {found_screenshots} state screenshots for {len(self.indices)} episodes.')\n","\n","        self.add_images_to_episodes()\n","\n","        self.dom_tokenizer = dom_tokenizer\n","        self.dom_seq_length = 492\n","        self.utterance_seq_length = 16\n","        self.task_name_seq_length = 4\n","\n","        self.target_action_type_seq_length = 1\n","        self.target_ref_seq_length = 1\n","        self.target_keydown_seq_length = 8 #32\n","\n","        # Size of the final layer output being composed of\n","        # 1 (action),\n","        # + 64 (ref) (\n","        # + 64x8 (keydown text times target_keydown_seq_length)\n","        self.model_output_size = 577\n","\n","        self.batch_size = batch_size\n","\n","    # Add the images to the different episodes of the dataset\n","    def add_images_to_episodes(self):\n","        found_images = []\n","        i = self.start_index\n","        image_counter = 0\n","        while i \u003c self.end_index:\n","            # Get K indices\n","            images = []\n","            try:\n","              state_indexes = self.indices[i]\n","              for state_index in state_indexes:\n","                  images.append(self.read_image(i, state_index))\n","                  image_counter += 1\n","              found_images.append(images)\n","            except:\n","              print(f'error {i}')\n","            i += 1\n","        # Add new column\n","        self.df['episode_images'] = found_images\n","        print(f'Found total of {image_counter} images in the loaded episodes.')\n","\n","    def get_file_indices(self) -\u003e List[Tuple[int, int]]:\n","        indices = []\n","        for filename in os.listdir(self.screenshots_path):\n","            if filename.endswith('.png'):\n","                # Extract the N and K values from the filename\n","                N, K = filename.replace('sc_', '').replace('st_', '').replace('.png', '').split('_')\n","                N = int(N)\n","                K = int(K)\n","                indices.append((N, K))\n","\n","        # Turn the indices into a dictionary\n","        indices_dict = {}\n","        for (N, K) in indices:\n","            if N not in indices_dict:\n","                indices_dict[N] = [K]\n","            else:\n","                indices_dict[N].append(K)\n","                indices_dict[N] = sorted(indices_dict[N])\n","\n","        return indices_dict, len(indices)\n","\n","    import torch\n","\n","    # Reads images into tensor\n","    def read_image(self, N, K) -\u003e torch.Tensor:\n","        filename = f'sc_{N}_st_{K}.png'\n","        filepath = os.path.join(self.screenshots_path, filename)\n","        image = Image.open(filepath).convert('RGB')\n","        tensor = torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) / 255.0\n","        return tensor\n","\n","    def save_image_png(self, N, K):\n","        loaded_image = self.read_image(N, K).numpy()\n","        print(f'Image has shape {loaded_image.shape}')\n","\n","        # scale the pixel values from [0,1] to [0, 255]\n","        array = np.clip(loaded_image * 255, 0, 255).astype('uint8')\n","        img = Image.fromarray(array.transpose(1, 2, 0), mode='RGB')\n","        img.save('screenshots/test_img.png')\n","\n","    # Turn our dataset into a series of state tensors ready for training.\n","    # We though ensure to clearly separate our different instances:\n","    # - todo: T5-Data (for later, depends between SL or RL current approach)\n","    # - RBG\n","    # - todo: Tokenized DOM\n","    # - todo: Task Instruction\n","    # They are separated because they are not going to be fed in the same manner into the model.\n","    # Basically into one single row.\n","    # todo: skip rows that have negative rewards\n","    def process_dataset(self):\n","        rbg_data = []\n","        dom_data = []\n","        utterance_data = []\n","        task_name_data = []\n","\n","        target_action = []\n","        target_refs = []\n","        target_keydown = []\n","\n","        episode_previous_actions = []\n","\n","        for (index, row) in self.df.iterrows():\n","            if float(row['reward']) \u003c= 0: # Skips failed episodes\n","                continue\n","            if len(row['episode_images']) != len(row['processed_states']): # Skips episodes with no matching pictures amount and states\n","                print(f'Error: episode_images and processed_states don\\' have the same length for index {index}')\n","                continue\n","            for rbg in row['episode_images']:\n","                rbg = np.array(rbg, dtype='float64')\n","                print(f'rbg shape: {rbg.shape}')\n","                rbg_data.append(rbg)\n","\n","\n","            # Process DOM Data\n","            for (index_state, state) in enumerate(row['processed_states']):\n","\n","                # Output Tokens\n","                if state['action_type'] == 'click': # Append boolean for action click or keydown\n","                    t_action = np.array([0])\n","                    target_action.append(t_action)\n","                else:\n","                    t_action = np.array([1])\n","                    target_action.append(t_action)\n","\n","                # Tokenize ref and appends to target\n","                t_ref = np.array(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(str(state['refs'])), self.target_ref_seq_length))\n","                target_refs.append(t_ref)\n","                # Tokenize target text and appends to target\n","                t_keydown = np.array(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(state['keydown_text']), self.target_keydown_seq_length))\n","                target_keydown.append(t_keydown)\n","\n","                # Input Tokens\n","                # Tokenize dom\n","                tokenized_dom = self.dom_tokenizer.tokenize_dom(state['dom'])\n","                tokenized_dom = self.dom_tokenizer.truncate_pad_entry(tokenized_dom, self.dom_seq_length)\n","                dom_data.append(tokenized_dom)\n","\n","                # Tokenize utterance\n","                tokenized_utterance = self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(row['utterance']), self.utterance_seq_length)\n","                utterance_data.append(tokenized_utterance)\n","\n","                # Tokenize task name\n","                tokenized_task_name = self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(row['task_name']), self.task_name_seq_length)\n","                task_name_data.append(tokenized_task_name)\n","\n","                # Deal with the previous action\n","                if index_state \u003c len(row['processed_states'])-1:\n","                    # Default one so empty for the first action\n","                    if index_state == 0:\n","                        episode_previous_actions.append([])\n","                        #episode_previous_actions.append(self.dom_tokenizer.truncate_pad_entry(self.dom_tokenizer.tokenize_string(''), self.model_output_size))\n","\n","                    # Add previous actions one by one\n","                    episode_previous_actions.append([t_action, t_ref, t_keydown])\n","                # Else no action to add\n","\n","\n","\n","\n","        rbg_data = np.stack(rbg_data)\n","        dom_data = np.array(dom_data)\n","        utterance_data = np.array(utterance_data)\n","        task_name_data = np.array(task_name_data)\n","        target_action = np.array(target_action)\n","        target_refs = np.array(target_refs)\n","        target_keydown = np.array(target_keydown)\n","\n","\n","        return rbg_data, dom_data, utterance_data, task_name_data, target_action, target_refs, target_keydown, episode_previous_actions\n","\n","\n","    # Batchify the dataset\n","    def get_dataloder(self, dataset):\n","        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n","        return train_loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43826,"status":"ok","timestamp":1685450919106,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"WQkn6Qxe8IQn","outputId":"442ed2a6-e3bd-4abb-8c65-8db02aa59c17"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset subset has 10 rows\n","Found a total of 5048 state screenshots for 2270 episodes.\n","Found total of 33 images in the loaded episodes.\n"]}],"source":["screenshots_path = '/content/drive/MyDrive/WebAI/screenshot_indexes'\n","batch_size = 8\n","dataset_loader = DatasetLoader(screenshots_path=screenshots_path, dataset=dataset, start_index=0, end_index=10, dom_tokenizer=tokenizer, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1685450922336,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"31NSyJJQKqKF","outputId":"bbf5024f-041d-468a-f006-ee812fd4e619"},"outputs":[{"data":{"text/plain":["{9344: [0, 1],\n"," 9345: [0, 1],\n"," 9367: [4],\n"," 9370: [2, 3],\n"," 9371: [2, 3],\n"," 9394: [2, 3],\n"," 9395: [2, 3],\n"," 9400: [0, 1, 2],\n"," 9401: [0, 1, 2],\n"," 9416: [4, 6],\n"," 9434: [0, 2, 3],\n"," 9435: [0, 2, 3],\n"," 9442: [2, 3],\n"," 9443: [2, 3],\n"," 9448: [0, 1, 2],\n"," 9449: [0, 1, 2],\n"," 9455: [4],\n"," 9461: [5, 6],\n"," 9476: [0, 1],\n"," 9477: [0, 1],\n"," 9485: [4],\n"," 9492: [0, 1],\n"," 9493: [0, 1],\n"," 9498: [1, 2, 3],\n"," 9499: [1, 2, 3],\n"," 9502: [1, 2, 3],\n"," 9503: [1, 2, 3],\n"," 9508: [0, 1],\n"," 9509: [0, 1],\n"," 9536: [0, 1, 3],\n"," 9537: [0, 1, 3],\n"," 9540: [0, 1, 2],\n"," 9541: [0, 1, 2],\n"," 9563: [4],\n"," 9568: [5],\n"," 9574: [0, 2, 3],\n"," 9575: [0, 2, 3],\n"," 9586: [4, 5],\n"," 9587: [4, 5, 7],\n"," 9590: [1, 2, 3],\n"," 9591: [1, 2, 3],\n"," 9595: [8, 9],\n"," 9606: [5],\n"," 9607: [5, 6, 7],\n"," 9610: [0, 1],\n"," 9611: [0, 1],\n"," 9615: [9],\n"," 9616: [10, 11],\n"," 9620: [8, 9],\n"," 9624: [2, 3],\n"," 9625: [2, 3],\n"," 9632: [4, 5, 7],\n"," 9633: [4, 5, 7],\n"," 9638: [6, 7],\n"," 9639: [6, 7],\n"," 9640: [12, 13],\n"," 9644: [4, 5, 6],\n"," 9645: [4],\n"," 9652: [0, 2, 3],\n"," 9653: [0, 2, 3],\n"," 9654: [10],\n"," 9656: [8, 9],\n"," 9658: [0, 1, 3],\n"," 9659: [0, 1],\n"," 9666: [0, 1, 2],\n"," 9667: [0, 1, 2],\n"," 9668: [8],\n"," 9669: [8, 9],\n"," 9670: [4, 6, 7],\n"," 9671: [4, 6, 7],\n"," 9675: [11, 12],\n"," 9678: [10, 11, 12],\n"," 9682: [0, 1],\n"," 9683: [0, 1],\n"," 9688: [0, 2, 3],\n"," 9689: [0, 2, 3],\n"," 9704: [4, 5],\n"," 9705: [4, 5],\n"," 9706: [12],\n"," 9712: [0, 2, 3],\n"," 9713: [0],\n"," 9714: [14, 15],\n"," 9716: [8, 9],\n"," 9717: [8, 9],\n"," 9718: [0, 1, 3],\n"," 9719: [0, 1, 3],\n"," 9723: [8],\n"," 9726: [0, 1, 2],\n"," 9727: [0, 1, 2, 11, 12],\n"," 9729: [8, 9],\n"," 9730: [4, 6, 7],\n"," 9731: [4, 6, 7],\n"," 9746: [6, 7],\n"," 9747: [6, 7],\n"," 9750: [0, 1, 3],\n"," 9751: [0, 1, 3],\n"," 9764: [1, 2, 3],\n"," 9765: [1, 2, 3, 10, 11],\n"," 9771: [10],\n"," 9772: [4, 5],\n"," 9779: [4, 6, 7],\n"," 9780: [0, 2, 3],\n"," 9781: [0, 2, 3],\n"," 9785: [8],\n"," 9796: [4, 5],\n"," 9797: [4, 5],\n"," 9802: [0, 1],\n"," 9803: [0, 1],\n"," 9808: [1, 2, 3],\n"," 9809: [1, 2, 3],\n"," 9814: [4, 6, 7],\n"," 9815: [4, 6, 7],\n"," 9820: [4, 5, 6],\n"," 9821: [4, 5, 6],\n"," 9832: [8, 9, 10, 11],\n"," 9836: [2, 3],\n"," 9837: [2, 3],\n"," 9840: [1, 2, 3],\n"," 9841: [1, 2, 3],\n"," 9845: [8, 9],\n"," 9856: [4, 5],\n"," 9857: [4, 5],\n"," 9862: [5, 6],\n"," 9863: [5, 6, 7],\n"," 9864: [11],\n"," 9868: [4, 5, 6],\n"," 9869: [4, 5, 6],\n"," 9874: [0, 1],\n"," 9875: [0, 1],\n"," 9884: [20, 22],\n"," 9886: [4, 6, 7],\n"," 9887: [4, 6, 7],\n"," 9890: [0, 1, 2],\n"," 9891: [0, 1, 2],\n"," 9898: [10],\n"," 9900: [1, 2, 3],\n"," 9901: [1, 2, 3],\n"," 9904: [8, 9],\n"," 9905: [8],\n"," 9916: [4, 5, 7],\n"," 9917: [4, 5, 7],\n"," 9922: [5, 6, 7],\n"," 9923: [5, 6, 7],\n"," 9928: [4, 5, 6],\n"," 9929: [4],\n"," 9930: [9],\n"," 9934: [0, 1, 3],\n"," 9935: [0, 1, 3],\n"," 9938: [18, 19],\n"," 9941: [10, 11, 12],\n"," 9942: [0, 1, 2],\n"," 9943: [0, 1, 2],\n"," 9946: [8],\n"," 9948: [2, 3],\n"," 9949: [2, 3],\n"," 9954: [6],\n"," 9955: [6],\n"," 9960: [4, 5],\n"," 9973: [8, 9],\n"," 9976: [0, 2, 3],\n"," 9977: [0, 2, 3],\n"," 9979: [9, 10, 12, 13],\n"," 9984: [4, 5],\n"," 9985: [4, 5],\n"," 9988: [10, 11],\n"," 9992: [1, 2, 3],\n"," 9993: [1, 2, 3],\n"," 9997: [8],\n"," 9998: [0, 1],\n"," 9999: [0, 1],\n"," 10006: [4, 5],\n"," 10010: [1, 2, 3],\n"," 10011: [1, 2, 3, 10, 11, 12],\n"," 10015: [8, 9],\n"," 10021: [9],\n"," 10024: [0, 1, 3],\n"," 10025: [0, 1, 3],\n"," 10032: [6, 7],\n"," 10033: [6],\n"," 10038: [4, 5, 6],\n"," 10039: [4, 5, 6],\n"," 10044: [4, 6, 7],\n"," 10045: [4, 6, 7],\n"," 10047: [10],\n"," 10052: [0, 1, 2],\n"," 10053: [0, 1, 2],\n"," 10056: [8],\n"," 10057: [8],\n"," 10058: [1, 2, 3],\n"," 10059: [1],\n"," 10066: [0, 2, 3],\n"," 11519: [4],\n"," 11524: [11],\n"," 11526: [4, 5, 7],\n"," 11527: [4, 5, 7],\n"," 11546: [0, 2],\n"," 11547: [0],\n"," 11572: [0, 1, 2],\n"," 11573: [0, 1, 2],\n"," 11578: [1, 2],\n"," 11579: [1],\n"," 11597: [0, 1, 3],\n"," 11596: [0, 1],\n"," 11600: [4, 5],\n"," 11616: [1, 2, 3],\n"," 11617: [1, 2],\n"," 11622: [0, 1, 3],\n"," 11623: [0, 1],\n"," 11654: [0],\n"," 11655: [0],\n"," 11660: [0, 2],\n"," 11661: [0, 2],\n"," 11684: [1, 2],\n"," 11685: [1, 2],\n"," 11715: [0, 1],\n"," 11714: [0],\n"," 11720: [0],\n"," 11721: [0],\n"," 11762: [0, 1],\n"," 11763: [0, 1],\n"," 11768: [0],\n"," 11769: [0, 2],\n"," 11786: [0, 1],\n"," 11787: [0, 1],\n"," 11804: [0],\n"," 11805: [0],\n"," 11830: [0, 1],\n"," 11831: [0, 1],\n"," 11846: [0, 1],\n"," 11847: [0, 1],\n"," 11872: [1, 2, 3],\n"," 11873: [1, 2, 3],\n"," 11878: [0, 1, 2],\n"," 11879: [0, 1, 2],\n"," 11896: [0, 2],\n"," 11897: [0, 2, 3],\n"," 11906: [0, 1, 3],\n"," 11907: [0, 1, 3],\n"," 11910: [6],\n"," 11924: [4],\n"," 11925: [4],\n"," 11938: [0],\n"," 11939: [0, 1],\n"," 11967: [4],\n"," 11971: [0, 1],\n"," 11970: [0, 1],\n"," 11988: [4],\n"," 11989: [4, 5],\n"," 11994: [0, 1],\n"," 11995: [0, 1],\n"," 12003: [4],\n"," 12014: [0, 1, 2],\n"," 12015: [0, 1, 2],\n"," 12020: [2, 3],\n"," 12021: [2, 3],\n"," 12037: [4, 5, 6],\n"," 12040: [4],\n"," 12041: [4, 5, 7],\n"," 12056: [2],\n"," 12057: [2],\n"," 12062: [0, 1],\n"," 12063: [0, 1],\n"," 12068: [0],\n"," 12069: [0, 2, 3],\n"," 12086: [0, 1],\n"," 12087: [0, 1],\n"," 12090: [4],\n"," 12116: [1],\n"," 12117: [1],\n"," 12123: [0, 1],\n"," 12122: [0],\n"," 12128: [2],\n"," 12129: [2],\n"," 12154: [0, 1, 2],\n"," 12155: [0, 1, 2],\n"," 12160: [0, 2],\n"," 12161: [0, 2],\n"," 12184: [1, 2],\n"," 12185: [1, 2],\n"," 12193: [4],\n"," 12199: [4],\n"," 12204: [0, 1],\n"," 12205: [0, 1],\n"," 12265: [4],\n"," 12272: [0, 1],\n"," 12273: [0, 1, 2],\n"," 12278: [1, 2, 3],\n"," 12279: [1, 2],\n"," 12296: [0, 1],\n"," 12297: [0, 1],\n"," 12306: [0, 2, 3],\n"," 12307: [0, 2],\n"," 12324: [4, 6, 7],\n"," 12325: [4, 6, 7],\n"," 12332: [0, 1, 2],\n"," 12333: [0, 1, 2],\n"," 12336: [8],\n"," 12337: [8],\n"," 12338: [1, 2, 3],\n"," 12339: [1],\n"," 12341: [9],\n"," 12344: [0, 1, 3],\n"," 12345: [0, 1, 3],\n"," 12348: [10, 12, 13],\n"," 12359: [4],\n"," 12367: [4, 5],\n"," 12371: [1],\n"," 12370: [1],\n"," 12394: [0],\n"," 12395: [0],\n"," 12401: [0],\n"," 12400: [0],\n"," 12416: [4],\n"," 12417: [4],\n"," 12428: [5],\n"," 12434: [0, 2],\n"," 12435: [0, 2, 3],\n"," 12437: [10, 11],\n"," 12442: [1, 2, 3],\n"," 12443: [1, 2, 3],\n"," 12446: [8, 9],\n"," 12448: [0, 1],\n"," 12449: [0, 1],\n"," 12454: [4, 5],\n"," 12455: [4, 5],\n"," 12460: [6, 7],\n"," 12461: [6, 7],\n"," 12476: [0, 1],\n"," 12477: [0, 1],\n"," 12492: [0, 1, 2],\n"," 12493: [0, 1],\n"," 12498: [2],\n"," 12499: [2, 3],\n"," 12503: [2],\n"," 12508: [0, 1, 2],\n"," 12509: [0, 1],\n"," 12556: [4],\n"," 12557: [4],\n"," 12562: [4],\n"," 12563: [4],\n"," 12574: [0, 2],\n"," 12575: [0, 2],\n"," 12590: [1, 2],\n"," 12591: [1, 2],\n"," 12610: [0, 1],\n"," 12611: [0, 1],\n"," 12653: [0],\n"," 12652: [0],\n"," 12658: [0, 1],\n"," 12659: [0],\n"," 12667: [0, 1],\n"," 12666: [0],\n"," 12682: [0, 1, 3],\n"," 12683: [0, 1],\n"," 12688: [0, 2],\n"," 12689: [0],\n"," 12712: [0, 2, 3],\n"," 12713: [0, 2, 3],\n"," 12718: [0, 1, 3],\n"," 12719: [0, 1, 3],\n"," 12726: [0, 1, 2],\n"," 12727: [0, 1, 2],\n"," 12730: [4, 6, 7],\n"," 12731: [4],\n"," 12750: [0, 1],\n"," 12751: [0, 1],\n"," 12765: [2, 3],\n"," 12764: [3],\n"," 12802: [0, 1],\n"," 12803: [0, 1],\n"," 12836: [0, 2],\n"," 12837: [0, 2],\n"," 12840: [1, 2, 3],\n"," 12841: [1, 2],\n"," 12874: [0, 1],\n"," 12875: [0],\n"," 12890: [0, 1, 2],\n"," 12891: [0, 1, 2],\n"," 12900: [1, 2],\n"," 12901: [1, 2],\n"," 12934: [0, 1],\n"," 12935: [0, 1],\n"," 12942: [0, 1],\n"," 12943: [0, 1],\n"," 12948: [1, 2],\n"," 12949: [1, 2],\n"," 12954: [4],\n"," 12976: [2, 3],\n"," 12977: [2],\n"," 12984: [4],\n"," 12985: [4, 5, 7],\n"," 12992: [2, 3],\n"," 12993: [2, 3],\n"," 12998: [0, 1, 2],\n"," 12999: [0, 1, 2],\n"," 13000: [6, 7],\n"," 13016: [0, 1, 3],\n"," 13017: [0, 1, 3],\n"," 13022: [1, 2, 3],\n"," 13023: [1, 2, 3],\n"," 13028: [0, 1],\n"," 13029: [0, 1],\n"," 13054: [0, 2, 3],\n"," 13055: [0, 2],\n"," 13060: [0, 1],\n"," 13061: [0, 1, 2],\n"," 13084: [0, 1],\n"," 13085: [0, 1],\n"," 13098: [4],\n"," 13099: [4],\n"," 13102: [4],\n"," 13114: [0, 2, 3],\n"," 13115: [0, 2, 3],\n"," 13120: [0],\n"," 13121: [0],\n"," 13156: [0, 1],\n"," 13157: [0, 1],\n"," 13161: [10, 11],\n"," 13162: [2],\n"," 13163: [2, 3],\n"," 13168: [0, 1, 2],\n"," 13169: [0, 1, 2],\n"," 13187: [2],\n"," 13191: [4],\n"," 13207: [0],\n"," 13206: [0],\n"," 13224: [4],\n"," 13225: [4],\n"," 13232: [0, 2, 3],\n"," 13233: [0, 2, 3],\n"," 13238: [0, 1, 3],\n"," 13239: [0, 1, 3],\n"," 13244: [1, 2, 3],\n"," 13245: [1, 2, 3],\n"," 13252: [4],\n"," 13259: [4],\n"," 13270: [0, 1, 3],\n"," 13271: [0, 1, 3],\n"," 13294: [0, 1, 2],\n"," 13295: [0, 1, 2],\n"," 13304: [2, 3],\n"," 13305: [2, 3],\n"," 13312: [4],\n"," 13313: [4],\n"," 13330: [0, 1],\n"," 13331: [0, 1],\n"," 13346: [0, 1],\n"," 13347: [0],\n"," 13365: [4, 5, 6],\n"," 13372: [0, 2, 3],\n"," 13373: [0, 2, 3],\n"," 13378: [0, 1, 3],\n"," 13379: [0, 1, 3],\n"," 13380: [4],\n"," 13381: [4],\n"," 13396: [1],\n"," 13397: [1],\n"," 13402: [0, 1],\n"," 13403: [0, 1],\n"," 13408: [0],\n"," 13409: [0],\n"," 36: [0, 1, 2],\n"," 37: [0, 1],\n"," 40: [0, 1, 2],\n"," 41: [0],\n"," 68: [0, 1, 2, 3, 4, 5],\n"," 74: [0, 1, 2],\n"," 75: [0, 1, 2],\n"," 90: [0, 1, 2],\n"," 91: [0, 1, 2],\n"," 101: [0, 1, 2],\n"," 6642: [0, 1, 10],\n"," 6643: [0, 1],\n"," 6648: [1, 2, 3],\n"," 6649: [1, 2, 3],\n"," 6654: [4, 6],\n"," 6655: [4, 6, 7],\n"," 6660: [4, 5, 6],\n"," 6661: [4, 5],\n"," 6672: [8],\n"," 6676: [2, 3],\n"," 6677: [2, 3],\n"," 6684: [4, 5, 7],\n"," 6685: [4, 5, 7],\n"," 6686: [10, 12],\n"," 6692: [2, 3],\n"," 6693: [2, 3],\n"," 6697: [8],\n"," 6698: [0, 1, 2],\n"," 6699: [0, 1],\n"," 6702: [0, 1, 2],\n"," 6703: [0, 1, 2],\n"," 6704: [10],\n"," 6708: [2, 3],\n"," 6709: [2, 3],\n"," 6714: [6],\n"," 6715: [6],\n"," 6720: [4, 5],\n"," 6728: [11],\n"," 6732: [8],\n"," 6733: [8, 9],\n"," 6737: [0, 2, 3],\n"," 6736: [0],\n"," 6738: [9],\n"," 6740: [1, 2, 3],\n"," 6741: [1, 2, 3],\n"," 6745: [8, 9],\n"," 6746: [12, 13],\n"," 6756: [4, 5, 7],\n"," 6757: [4, 5, 7],\n"," 6762: [5, 6, 7],\n"," 6763: [5, 6, 7],\n"," 6767: [10, 12, 13],\n"," 6768: [4, 5, 6],\n"," 6769: [4, 5, 6],\n"," 6770: [9],\n"," 6774: [0, 1, 3],\n"," 6775: [0, 1, 3],\n"," 6787: [4, 6, 7],\n"," 6790: [0, 1, 2],\n"," 6791: [0, 1, 2],\n"," 6804: [4, 5, 6],\n"," 6805: [4, 5],\n"," 6808: [10],\n"," 6812: [0, 2, 3],\n"," 6813: [0, 2, 3],\n"," 6816: [8, 9],\n"," 6818: [0, 1, 3],\n"," 6819: [0, 1, 3],\n"," 6826: [0, 1, 2],\n"," 6827: [0, 1, 2],\n"," 6828: [8],\n"," 6830: [4, 6],\n"," 6831: [4],\n"," 6846: [5, 6, 7],\n"," 6847: [5, 6],\n"," 6850: [0, 1],\n"," 6851: [0, 1],\n"," 6853: [10, 11],\n"," 6854: [9],\n"," 6864: [2, 3],\n"," 6865: [2, 3],\n"," 6872: [4, 5, 7],\n"," 6873: [4, 5, 7],\n"," 6878: [6],\n"," 6879: [6],\n"," 6880: [2, 3],\n"," 6881: [2, 3],\n"," 6884: [8],\n"," 6885: [8, 9],\n"," 6896: [4, 5, 6],\n"," 6897: [4, 5, 6],\n"," 6906: [6, 7],\n"," 6907: [6, 7],\n"," 6910: [0, 1, 3],\n"," 6911: [0, 1, 3],\n"," 6914: [9],\n"," 6915: [9],\n"," 6920: [8],\n"," 6924: [1, 2, 3],\n"," 6925: [1, 2, 3],\n"," 6932: [4, 5],\n"," 6933: [4, 5],\n"," 6938: [4, 6, 7],\n"," 6939: [4],\n"," 6944: [4, 5],\n"," 6945: [4, 5],\n"," 6952: [0, 2, 3],\n"," 6953: [0, 2, 3],\n"," 6957: [8, 9],\n"," 6958: [0, 1, 3],\n"," 6959: [0, 1, 3],\n"," 6962: [8, 10],\n"," 6966: [0, 1, 2],\n"," 6967: [0, 1, 2],\n"," 6968: [8, 9],\n"," 6969: [8, 9],\n"," 6971: [4, 6, 7],\n"," 6976: [12, 13],\n"," 6982: [0, 1, 3],\n"," 6983: [0, 1, 3],\n"," 6986: [9],\n"," 6988: [0, 2, 3],\n"," 6989: [0, 2, 3],\n"," 6994: [5, 6],\n"," 6995: [5, 6],\n"," 7002: [8, 9],\n"," 7006: [1, 2, 3],\n"," 7007: [1, 2, 3],\n"," 7010: [4, 5, 7],\n"," 7011: [4, 5, 7],\n"," 7025: [5, 6, 7],\n"," 7032: [0, 1, 3],\n"," 7033: [0, 1, 3],\n"," 7038: [0, 2, 3],\n"," 7039: [0, 2, 3],\n"," 7041: [8],\n"," 7045: [0, 1],\n"," 7044: [0, 1],\n"," 7052: [4, 6, 7],\n"," 7053: [4, 6, 7],\n"," 7058: [4, 5],\n"," 7059: [4, 5],\n"," 7061: [20, 22, 23],\n"," 7066: [4, 5, 6],\n"," 7067: [4, 5, 6],\n"," 7069: [10, 12],\n"," 7070: [2, 3],\n"," 7071: [2, 3],\n"," 7075: [8],\n"," 7082: [4, 5],\n"," 7083: [4, 5, 7],\n"," 7088: [6],\n"," 7089: [6, 7],\n"," 7090: [8, 9],\n"," 7094: [2, 3],\n"," 7095: [2, 3],\n"," 7100: [8],\n"," 7104: [0, 1, 2],\n"," 7105: [0, 1, 2],\n"," 7112: [6, 7],\n"," 7113: [6, 7],\n"," 7118: [4, 5],\n"," 7119: [4, 5],\n"," 7126: [4, 5],\n"," 7127: [4, 5, 20],\n"," 7129: [14, 15],\n"," 7130: [0, 2, 3],\n"," 7131: [0, 2, 3],\n"," 7134: [8, 9],\n"," 7141: [12],\n"," 7142: [8, 9],\n"," 7143: [8, 9],\n"," 7146: [1, 2, 3],\n"," 7147: [1, 2, 3],\n"," 7150: [4, 5],\n"," 7151: [4, 5],\n"," 7164: [5, 6],\n"," 7172: [0, 1],\n"," 7173: [0, 1, 3],\n"," 7177: [9],\n"," 7178: [2, 3],\n"," 7179: [2, 3, 10],\n"," 7180: [4, 6, 7],\n"," 7181: [4, 6],\n"," 7196: [0, 1],\n"," 7197: [0, 1],\n"," 7216: [3],\n"," 7217: [3],\n"," 7222: [0, 1],\n"," 7223: [0, 1],\n"," 7228: [1, 3],\n"," 7229: [1, 3],\n"," 7254: [0, 1, 3],\n"," 7255: [0, 1, 3],\n"," 7260: [1, 3],\n"," 7261: [1],\n"," 7284: [0, 3],\n"," 7285: [0],\n"," 7314: [0, 1, 3],\n"," 7315: [0, 1, 3],\n"," 7320: [1, 3],\n"," 7321: [1, 2, 3],\n"," 7355: [26],\n"," 7356: [0, 3],\n"," 7357: [0, 3],\n"," 7362: [0, 1, 2],\n"," 7363: [0, 1],\n"," 7368: [3],\n"," 7369: [3],\n"," 7386: [0, 1, 3],\n"," 7387: [0, 1, 3],\n"," 7411: [12, 13],\n"," 7412: [1, 3],\n"," 7413: [1, 3],\n"," 7418: [0, 1],\n"," 7419: [0, 1],\n"," 7426: [0, 1, 3],\n"," 7427: [0, 1, 3],\n"," 7450: [0, 1],\n"," 7451: [0, 1],\n"," 7464: [0, 3],\n"," 7465: [0, 3],\n"," 7480: [1, 3],\n"," 7481: [1],\n"," 7510: [0, 1],\n"," 7511: [0, 1],\n"," 7524: [2],\n"," 7525: [3],\n"," 7533: [5],\n"," 7552: [3],\n"," 7553: [3],\n"," 7558: [0, 1],\n"," 7559: [0, 1],\n"," 7566: [0, 1, 3],\n"," 7567: [0, 1, 3],\n"," 7582: [0, 1],\n"," 7583: [0, 1],\n"," 7589: [1, 2, 3],\n"," 7602: [0, 3],\n"," 7603: [0, 3],\n"," 7608: [0, 1],\n"," 7609: [0, 1],\n"," 7615: [5],\n"," 7636: [0, 1],\n"," 7637: [0, 1],\n"," 7640: [0, 1, 3],\n"," 7641: [0, 1, 3],\n"," 7674: [1, 3],\n"," 7675: [1, 3],\n"," 7690: [0, 3],\n"," 7691: [0, 2, 3],\n"," 7700: [0, 1, 3],\n"," 7701: [0, 1, 3],\n"," 7734: [3],\n"," 7735: [3],\n"," 7742: [3],\n"," 7743: [3],\n"," 7748: [0, 1],\n"," 7749: [0, 1, 3],\n"," 7776: [0, 1],\n"," 7777: [0, 1],\n"," 7792: [0, 1],\n"," 7793: [0, 1],\n"," 7798: [0, 3],\n"," 7799: [0, 3],\n"," 7807: [5],\n"," 7810: [1, 3],\n"," 7811: [1, 3],\n"," 7824: [0, 1, 3],\n"," 7825: [0, 1, 3],\n"," 7852: [0, 1],\n"," 7853: [0],\n"," 7858: [1, 3],\n"," 7859: [1, 3],\n"," 7866: [0, 3],\n"," 7867: [0, 3],\n"," 7882: [1, 3],\n"," 7883: [1, 3],\n"," 7888: [0, 1],\n"," 7889: [0, 1],\n"," 7894: [5],\n"," 7912: [0, 1],\n"," 7913: [0, 1],\n"," 7918: [1, 3],\n"," 7919: [1, 2, 3],\n"," 7926: [0, 3],\n"," 7927: [0, 3],\n"," 7951: [3],\n"," 7964: [0, 1],\n"," 7965: [0, 1],\n"," 7980: [0, 1],\n"," 7981: [0, 1],\n"," 8016: [0, 1],\n"," 8017: [0, 1],\n"," 8022: [2, 3],\n"," 8023: [3],\n"," 8028: [0, 1],\n"," 8029: [0, 1, 2],\n"," 8054: [0, 2, 3],\n"," 8055: [0, 3],\n"," 8060: [0, 1],\n"," 8061: [0, 1],\n"," 8084: [0, 1, 3],\n"," 8085: [0, 1, 3],\n"," 8114: [0, 3],\n"," 8115: [0, 3],\n"," 8120: [0, 1, 2],\n"," 8121: [0, 1],\n"," 8156: [0, 1],\n"," 8157: [0, 1, 3],\n"," 8163: [1, 3],\n"," 8162: [1, 3],\n"," 8168: [0, 1],\n"," 8169: [0, 1],\n"," 8186: [0, 3],\n"," 8187: [0, 3],\n"," 8206: [0],\n"," 8207: [0, 1],\n"," 8232: [0, 3],\n"," 8233: [0, 2, 3],\n"," 8238: [0, 1, 3],\n"," 8239: [0, 1, 3],\n"," 8244: [3],\n"," 8245: [3],\n"," 8266: [5],\n"," 8270: [0, 1],\n"," 8271: [0, 1],\n"," 8294: [0, 1],\n"," 8295: [0, 1],\n"," 8304: [1, 3],\n"," 8305: [1, 3],\n"," 8330: [0, 1, 3],\n"," 8331: [0, 1, 3],\n"," 8346: [0, 1, 2],\n"," 8347: [0, 1],\n"," 8372: [0, 2, 3],\n"," 8373: [0, 3],\n"," 8378: [0, 1, 3],\n"," 8379: [0, 1, 3],\n"," 8396: [1, 3],\n"," 8397: [1, 3],\n"," 8402: [0, 1, 3],\n"," 8403: [0, 1, 3],\n"," 8408: [3],\n"," 8409: [3],\n"," 8436: [1, 3],\n"," 8437: [1, 3],\n"," 8440: [0, 3],\n"," 8441: [0, 3],\n"," 8474: [0, 1, 2],\n"," 8475: [0],\n"," 8490: [0],\n"," 8491: [0, 1],\n"," 8500: [0, 2],\n"," 8501: [0, 2],\n"," 8534: [0, 1, 2],\n"," 8535: [0, 1, 2],\n"," 8542: [0],\n"," 8543: [0, 1],\n"," 8548: [0, 3],\n"," 8549: [0, 2],\n"," 8576: [2],\n"," 8577: [2],\n"," 8585: [4, 5],\n"," 8592: [2, 3],\n"," 8598: [0, 1, 3],\n"," 8599: [0, 1],\n"," 8604: [6],\n"," 8612: [0, 1, 2],\n"," 8613: [0, 1, 2],\n"," 8618: [1, 2, 3],\n"," 8619: [1, 2],\n"," 8626: [0, 2, 3],\n"," 8627: [0, 2, 3],\n"," 8630: [4, 5],\n"," 8650: [1, 2, 3],\n"," 8651: [1, 2],\n"," 8664: [0, 1, 3],\n"," 8665: [0, 1, 3],\n"," 8680: [0, 1, 2],\n"," 8681: [0, 1, 2],\n"," 8706: [4],\n"," 8711: [2, 3],\n"," 8710: [2],\n"," 8724: [0, 1],\n"," 8725: [0, 1],\n"," 8739: [4],\n"," 8753: [0, 1, 2],\n"," 8752: [0, 1],\n"," 8758: [1],\n"," 8759: [1, 2],\n"," 8766: [0, 2, 3],\n"," 8767: [0, 2],\n"," 8771: [4, 5],\n"," 8782: [2, 3],\n"," 8783: [2, 3],\n"," 8788: [0, 1],\n"," 8789: [0, 1, 2],\n"," 8794: [4, 5],\n"," 8795: [4],\n"," 8800: [0, 1],\n"," 8801: [0, 1],\n"," 8823: [4],\n"," 8828: [4],\n"," 8834: [1, 2],\n"," 8835: [1, 2, 3],\n"," 8842: [0, 2, 3],\n"," 8843: [0, 2],\n"," 8848: [0, 1],\n"," 8849: [0, 1],\n"," 8855: [4],\n"," 8876: [0, 1, 2],\n"," 8877: [0, 1, 2],\n"," 8892: [0, 1],\n"," 8893: [0, 1],\n"," 8898: [2, 3],\n"," 8899: [2, 3],\n"," 8902: [2, 3],\n"," 8903: [2, 3],\n"," 8908: [0, 1, 3],\n"," 8909: [0, 1, 3],\n"," 8936: [0, 1],\n"," 8937: [0, 1],\n"," 8940: [0, 1, 3],\n"," 8941: [0, 1, 3],\n"," 8957: [5, 6],\n"," 8974: [2, 3],\n"," 8975: [2, 3],\n"," 8990: [0, 2, 3],\n"," 8991: [0],\n"," 9014: [0, 1],\n"," 9015: [0, 1, 2],\n"," 9020: [0, 2, 3],\n"," 9021: [0, 2, 3],\n"," 9040: [4, 5],\n"," 9056: [1, 2],\n"," 9057: [1, 2],\n"," 9062: [0, 1, 3],\n"," 9063: [0, 1],\n"," 9068: [2, 3],\n"," 9069: [2, 3],\n"," 9086: [0, 1, 2],\n"," 9087: [0, 1, 2],\n"," 9100: [4, 5],\n"," 9101: [4],\n"," 9117: [2, 3],\n"," 9122: [0, 1],\n"," 9123: [0, 1],\n"," 9129: [0],\n"," 9128: [0],\n"," 9154: [0, 1],\n"," 9155: [0],\n"," 9160: [2, 3],\n"," 9185: [1],\n"," 9184: [1],\n"," 9204: [0, 1],\n"," 9205: [0, 1, 3],\n"," 9231: [1, 2],\n"," 9246: [0],\n"," 9247: [0, 2, 3],\n"," 9265: [4],\n"," 9272: [0, 1, 2],\n"," 9273: [0, 1, 2],\n"," 9279: [1],\n"," 9296: [0, 1],\n"," 9297: [0, 1],\n"," 9306: [2],\n"," 9307: [2, 3],\n"," 9332: [0, 1, 2],\n"," 9333: [0, 1, 2],\n"," 9338: [1, 2, 3],\n"," 9339: [1, 2, 3],\n"," 5213: [0, 1, 3],\n"," 5218: [2, 3],\n"," 5219: [2, 3],\n"," 5220: [10, 12, 13],\n"," 5226: [1, 2, 3],\n"," 5227: [1, 2, 3],\n"," 5230: [4, 5],\n"," 5231: [4, 5],\n"," 5243: [10],\n"," 5246: [4, 5],\n"," 5247: [4, 5],\n"," 5250: [0, 2, 3],\n"," 5251: [0, 2, 3],\n"," 5257: [10],\n"," 5261: [8],\n"," 5262: [10, 11],\n"," 5264: [0, 1, 2],\n"," 5265: [0, 1, 2],\n"," 5272: [6, 7],\n"," 5273: [6, 7],\n"," 5278: [4, 5],\n"," 5279: [4, 5],\n"," 5280: [0, 1],\n"," 5281: [0, 1, 3],\n"," 5284: [9],\n"," 5296: [6, 7],\n"," 5297: [6, 7],\n"," 5305: [10, 11],\n"," 5306: [4, 5],\n"," 5307: [4, 5, 6],\n"," 5310: [2, 3],\n"," 5311: [2, 3],\n"," 5314: [8, 9],\n"," 5315: [8],\n"," 5324: [0, 1],\n"," 5325: [0, 1],\n"," 5329: [12, 13],\n"," 5332: [4],\n"," 5333: [4, 6, 7],\n"," 5338: [4, 5],\n"," 5339: [4, 5],\n"," 5344: [5],\n"," 5345: [5, 6],\n"," 5352: [0, 1, 3],\n"," 5353: [0, 1, 3, 11, 12],\n"," 5358: [0, 2, 3],\n"," 5359: [0, 2, 3],\n"," 5366: [1, 2, 3],\n"," 5367: [1, 2, 3],\n"," 5370: [4, 5, 7],\n"," 5372: [12],\n"," 5382: [0, 2, 3],\n"," 5383: [0, 2, 3],\n"," 5388: [0, 1, 3],\n"," 5389: [0, 1],\n"," 5394: [4, 5, 6],\n"," 5395: [4, 5, 6],\n"," 5400: [4, 6, 7, 10, 12, 13],\n"," 5401: [4, 6, 7],\n"," 5413: [8],\n"," 5416: [0, 1],\n"," 5417: [0, 1],\n"," 5422: [2, 3],\n"," 5423: [2, 3],\n"," 5427: [8, 9],\n"," 5428: [0, 1, 3],\n"," 5429: [0, 1, 3],\n"," ...}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset_loader.indices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685450924495,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"kSJLS9Rg8IQn","outputId":"f0bc6c29-15bf-4e93-f621-668e6214f822"},"outputs":[{"name":"stdout","output_type":"stream","text":["rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","Error: episode_images and processed_states don' have the same length for index 6\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n","rbg shape: (3, 210, 160)\n"]}],"source":["rgb_data, dom_data, utterance_data, task_data, target_action, target_refs, target_keydown, episode_previous_actions = dataset_loader.process_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685450926638,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"z__V8x268IQn","outputId":"b8b02a65-1156-4a23-df33-f62aa70b47a5"},"outputs":[{"data":{"text/plain":["(14, 14, 14, 14, 14, 14, 14, 14)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(rgb_data), len(dom_data), len(utterance_data), len(task_data), len(target_action), len(target_refs), len(target_keydown), len(episode_previous_actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1685450927356,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"UIhWVARu8IQo","outputId":"aab19bdd-1030-4627-abc1-9aa9c3aa92a4"},"outputs":[{"data":{"text/plain":["((14, 3, 210, 160), (14, 492), (14, 16), (14, 4), (14, 1), (14, 1), (14, 8))"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["rgb_data.shape, dom_data.shape, utterance_data.shape, task_data.shape, target_action.shape, target_refs.shape, target_keydown.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1685450928375,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"nCF7tgwL8IQo","outputId":"e1bba436-6cd6-472c-ba7c-3f7f0be60294"},"outputs":[{"name":"stdout","output_type":"stream","text":["(14, 512)\n","rgb_data: torch.Size([14, 3, 210, 160]), language_input: torch.Size([14, 512, 64])\n"]}],"source":["import numpy as np\n","import torch\n","\n","# Put language task together\n","language_input = np.concatenate((dom_data, utterance_data, task_data), axis=1)\n","#language_input = dom_data\n","print(language_input.shape)\n","\n","rgb_data = torch.from_numpy(rgb_data).type(torch.float32)\n","language_input = torch.from_numpy(language_input).type(torch.long)\n","language_input = tokenizer.embedding_fn(language_input) # Create embeddings for language\n","print(f'rgb_data: {rgb_data.shape}, language_input: {language_input.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685450929834,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"4kCY16TA8IQo","outputId":"e517b9fa-ae4f-41cb-84a3-7dbf69237a02"},"outputs":[{"data":{"text/plain":["torch.Size([14, 512, 64])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["language_input.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685450932272,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"Y-ynZd_N8IQo","outputId":"26aa40f7-210e-4002-bae2-be806a45fa7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","(577,)\n","Got 14 formatted_previous_actions, type: float64\n","formatted_previous_actions: torch.Size([14, 577]), torch.float64\n"]}],"source":["# Test formatting of previous action sequence data.\n","# TODO: need to update the dataloader with them\n","def format_previous_actions(episode_previous_actions, tokenizer, dataset_loader):\n","    formatted_previous_actions = []\n","\n","    # Process one by one the different entries to have them correctly matching the dataset\n","    for entry in episode_previous_actions:\n","        # No previous action, have default empty sequence\n","        if len(entry) == 0:\n","            empty_action = np.array(tokenizer.truncate_pad_entry(tokenizer.tokenize_string(''), dataset_loader.model_output_size))\n","            print(empty_action.shape)\n","            formatted_previous_actions.append(empty_action)\n","        else:\n","            action_type = torch.from_numpy(entry[0]).type(torch.long)\n","            ref = torch.from_numpy(entry[1]).type(torch.long)\n","            ref = torch.flatten(tokenizer.embedding_fn(ref))\n","            keydown = torch.flatten(tokenizer.embedding_fn(torch.from_numpy(entry[2]).type(torch.long)))\n","            together = np.concatenate((action_type.detach().numpy(), ref.detach().numpy(), keydown.detach().numpy()))\n","            print(together.shape)\n","            formatted_previous_actions.append(together)\n","\n","    formatted_previous_actions = np.array(formatted_previous_actions)\n","    return formatted_previous_actions\n","\n","# These are the formatted actions to use when concatenating after the multimodal layers.\n","formatted_previous_actions = format_previous_actions(episode_previous_actions=episode_previous_actions, tokenizer=tokenizer, dataset_loader=dataset_loader)\n","print(f'Got {len(formatted_previous_actions)} formatted_previous_actions, type: {formatted_previous_actions.dtype}')\n","\n","# into tensor\n","formatted_previous_actions = torch.from_numpy(formatted_previous_actions)\n","print(f'formatted_previous_actions: {formatted_previous_actions.shape}, {formatted_previous_actions.dtype}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685450935213,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"PuBLSndv8IQp","outputId":"bf79cba6-f8bd-4c35-cbe4-733a5a49c0b5"},"outputs":[{"data":{"text/plain":["torch.Size([14, 577])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["def produce_labels(target_action, target_refs, target_keydown, tokenizer):\n","    i = 0\n","    labels = []\n","\n","    while i\u003clen(target_action):\n","        # Tokenize refs and keydowns\n","        embedded_action = torch.from_numpy(target_action[i]).detach().numpy()\n","        embedded_ref = torch.flatten(tokenizer.embedding_fn(torch.from_numpy(target_refs[i]))).detach().numpy()\n","        embedded_keydown = torch.flatten(tokenizer.embedding_fn(torch.from_numpy(target_keydown[i]))).detach().numpy()\n","\n","        label = np.concatenate((embedded_action, embedded_ref, embedded_keydown), axis=0)\n","        labels.append(label)\n","        i += 1\n","\n","    labels = np.array(labels)\n","    labels = torch.from_numpy(labels).type(torch.float32)\n","    return labels\n","\n","labels = produce_labels(target_action, target_refs, target_keydown, tokenizer)\n","labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":368,"status":"error","timestamp":1685450938800,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"Yl4G7FE8rc_E","outputId":"065211c1-5da1-483a-db3d-2fead3328c06"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-17-087a69eb8e21\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmultimodal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'multimodal' is not defined"]}],"source":["import gc\n","del multimodal\n","gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fg1vn4tq8IQp"},"outputs":[],"source":["# Batchify data\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","batched_rgb = dataset_loader.get_dataloder(dataset=rgb_data.to(torch.device(device)))\n","batched_language = dataset_loader.get_dataloder(dataset=language_input.to(torch.device(device)))\n","batched_previous_actions = dataset_loader.get_dataloder(dataset=formatted_previous_actions.to(torch.device(device)))\n","batched_labels = dataset_loader.get_dataloder(dataset=labels.to(torch.device(device)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkSlAg6xCLOI"},"outputs":[],"source":["# Some model classes\n","\n","'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3,3),\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=14):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 3\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=2)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        #self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        #out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(x)\n","        dl1 = out.shape\n","        out = self.layer2(out)\n","        dl2 = out.shape\n","        out = self.layer3(out)\n","        dl3 = out.shape\n","        out = self.layer4(out)\n","        dl4 = out.shape\n","        #out = F.avg_pool2d(out, 4)\n","        #out = out.view(out.size(0), -1)\n","\n","        # Flatten (batch size, channels, features)\n","        out = out.view(8, 512, 140)\n","        out_1_s = out.shape\n","        # Might have to change the last layer to obtain our 14x11 feature vector?\n","        #out = self.linear(out)\n","        #print(f'out_1_s: {out_1_s}, out: {out.shape}')\n","        print(f'dl1: {dl1}, dl2: {dl2}, dl3: {dl3}, dl4: {dl4}, out: {out_1_s}')\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","# ----------------------------------\n","# Cross Attention Model for Language\n","class CrossAttentionModelLanguage(nn.Module):\n","    def __init__(self, input_dim=64, hidden_dim=140, num_heads=4):\n","        super(CrossAttentionModelLanguage, self).__init__()\n","\n","        self.attention = nn.MultiheadAttention(input_dim, num_heads)\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","\n","    def forward(self, input):\n","        # Reshape the input to (sequence_length, batch_size, input_dim)\n","        input = input.permute(1, 0, 2)\n","\n","        # Apply cross-attention\n","        output, _ = self.attention(input, input, input)\n","\n","        # Reshape the output to (batch_size, sequence_length, input_dim)\n","        output = output.permute(1, 0, 2)\n","\n","        # Apply linear transformation\n","        output = self.linear(output)\n","\n","        return output\n","\n","\n","#--------------------------------\n","# Multimodal Transformer Network \n","from torch.nn import Transformer\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self):\n","        super(TransformerNetwork, self).__init__()\n","\n","        self.embedding_dim = 512\n","        self.num_layers = 8\n","        self.num_heads = 8\n","\n","        self.embedding = nn.Linear(140, self.embedding_dim)\n","        self.transformer = Transformer(\n","            d_model=self.embedding_dim,\n","            nhead=self.num_heads,\n","            num_encoder_layers=self.num_layers,\n","            num_decoder_layers=self.num_layers\n","        )\n","        #self.output_layer = nn.Linear(self.embedding_dim, 1)\n","\n","    def forward(self, input_tensor):\n","        batch_size = input_tensor.size(0)\n","\n","        embedded = self.embedding(input_tensor)\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        output = self.transformer(embedded, embedded)\n","        output = output.permute(1, 0, 2)\n","        #output = self.output_layer(output)\n","\n","        return output.squeeze(2)\n","\n","#-------------------\n","# LSTM Netowkr Part\n","class TwoLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(TwoLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        # Note: we use a Linear layer for dimension adjustment.\n","        # Indeed, the previous multi-modal transformer outputs a 1024 + prev_action length tensor,\n","        # and here the LSTM layers have a 512 dim where we also need to bypass residual connections.\n","        # We use the linear layer below to have a matching size with the input/output of these LSTMs\n","        # in order to concatenate them.\n","        self.linear = nn.Linear(input_size, hidden_size)\n","\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2, batch_first=True)\n","\n","        # This linear layer is for the final mapping\n","        #self.fc = nn.Linear(hidden_size, output_size) #fully connected last layer\n","\n","    def forward(self, input):\n","        # Perform dimension adjustment using the linear layer\n","        adjusted_input = self.linear(input)\n","        print(f'adjusted_input: {adjusted_input.shape}')\n","\n","        # Perform the forward pass through each LSTM layer\n","        output, _hidden_state = self.lstm(adjusted_input)\n","\n","        # Perform residual connections between LSTM layers\n","        residual_output = adjusted_input + output\n","\n","        # Extract the hidden state of the last LSTM layer after residual connections\n","        last_hidden_state = residual_output[:, -1, :]\n","\n","        #out = self.fc(last_hidden_state)\n","        return last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5eucGQ78IQp"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","#from rgb_model import ResNet18\n","#from language_model import CrossAttentionModelLanguage\n","#from multimodal_aggregate import MultiModalTransformer\n","#from multimodal_aggregate import TransformerNetwork\n","#from lstm_model import TwoLSTM\n","import torch.nn.functional as F\n","\n","class CCNeT5(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        #self.rgb_model = nn.Sequential(*resnet_blocks)\n","        self.rgb_model = ResNet18()\n","        self.language_model = CrossAttentionModelLanguage()\n","\n","        # Multi Modal Part Combining RGB + Language\n","        #self.multimodal_transformer = MultiModalTransformer()\n","        self.multimodal_transformer = TransformerNetwork()\n","\n","        # Here we should concatenate with the previous Action\n","        # when doing the forward pass\n","\n","        # Two-Layer LSTM\n","        self.lstm = TwoLSTM(1024, 513)\n","\n","        # Last output Layer to create a proper action space from the original language embeddings\n","        self.fc = nn.Linear(513, 577) #fully connected last layer\n","\n","\n","    # todo: pass previous action\n","    def forward(self, rgb_input, language_input, previous_action):\n","        # Process RGB input\n","        rgb_output = self.rgb_model(rgb_input)\n","\n","        # Process language input\n","        language_output = self.language_model(language_input)\n","\n","        print(f'shape rbg_output: {rgb_output.shape}, shape language_output: {language_output.shape}')\n","\n","        # Further processing or fusion of modalities\n","        fused_output = torch.cat((rgb_output, language_output), dim=1)\n","        print(f'fused_output: {fused_output.shape} - {fused_output.dtype}')\n","        # Additional processing steps...\n","\n","        # Permute\n","        #fused_output = fused_output.permute(0, 2, 1)\n","\n","        # Feed into Multimodal Transformer\n","        multimodal_output = self.multimodal_transformer(fused_output)\n","        print(f'multimodal_output: {multimodal_output.shape} - type: {multimodal_output.dtype}')\n","\n","        # TODO: concatenate with previous action\n","        print(f'previous_action: {previous_action.shape}')\n","        tensor2_reshaped = F.pad(previous_action, (0, 447), mode='constant')\n","        tensor2_expanded = torch.unsqueeze(tensor2_reshaped, dim=2)  # Shape: [8, 1024, 577]\n","        combined_tensor = torch.cat((multimodal_output, tensor2_expanded), dim=2).type(torch.float32) # Shape: [8, 1024, 1089]\n","        print(f'concatenation previous action: {combined_tensor.shape}')\n","\n","        # Feed into LSTM\n","        # + Permute to fit the right format\n","        lstm_output = self.lstm(combined_tensor.permute(0, 2, 1))\n","        print(f'lstm_output: {lstm_output.shape}')\n","\n","        # Last layer to target action space\n","        action_output = self.fc(lstm_output)\n","\n","        return action_output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":823,"status":"ok","timestamp":1685450944926,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"},"user_tz":-120},"id":"Qi3l2P_m8IQq","outputId":"55146acb-c67f-4236-aa52-9069d55859dc"},"outputs":[{"data":{"text/plain":["CCNeT5(\n","  (rgb_model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (shortcut): Sequential()\n","      )\n","    )\n","  )\n","  (language_model): CrossAttentionModelLanguage(\n","    (attention): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","    )\n","    (linear): Linear(in_features=64, out_features=140, bias=True)\n","  )\n","  (multimodal_transformer): TransformerNetwork(\n","    (embedding): Linear(in_features=140, out_features=512, bias=True)\n","    (transformer): Transformer(\n","      (encoder): TransformerEncoder(\n","        (layers): ModuleList(\n","          (0-7): 8 x TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (decoder): TransformerDecoder(\n","        (layers): ModuleList(\n","          (0-7): 8 x TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","            )\n","            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (lstm): TwoLSTM(\n","    (linear): Linear(in_features=1024, out_features=513, bias=True)\n","    (lstm): LSTM(513, 513, num_layers=2, batch_first=True)\n","  )\n","  (fc): Linear(in_features=513, out_features=577, bias=True)\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["multimodal = CCNeT5()\n","multimodal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOCGOJ4H8IQq"},"outputs":[],"source":["import torch.optim as optim\n","multimodal.to(device)\n","loss_function = nn.MSELoss()\n","optimizer = optim.Adam(multimodal.parameters(), lr=0.1)\n","\n","def train_model(model, batched_rgb, batched_language, batched_previous_actions, batched_labels, epochs):\n","    i = 0\n","    while i \u003c epochs:\n","        for rgb_data, language_data, previous_action, targets in zip(batched_rgb, batched_language, batched_previous_actions, batched_labels):\n","            optimizer.zero_grad()\n","\n","            print(f'iteration {i}')\n","            if rgb_data.shape[0] != batch_size:\n","                # If we have a smaller batch, discard it as we don't handle it currently\n","                print(f'current batch size: {rgb_data.shape[0]}')\n","                continue\n","            print(rgb_data.shape)\n","            print(language_data.shape)\n","            rgb_data = rgb_data#.to(device)\n","            language_data = language_data#.to(device)\n","            action_output = model.forward(rgb_data, language_data, previous_action)\n","            print(f'action_output: {action_output.shape} - {action_output.dtype}')\n","            print(f'targets: {targets.shape} - {targets.dtype}')\n","\n","            loss = loss_function(action_output, targets)\n","            loss.backward(retain_graph=True)\n","            optimizer.step()\n","\n","        #if i % 100 == 0 or True:\n","        print(\"Epoch: %d, loss: %1.5f\" % (i, loss.item()))\n","\n","        i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONCEtB6R8IQq","outputId":"d430e3cf-8a89-4b34-ce11-4dc130d31525"},"outputs":[{"name":"stdout","output_type":"stream","text":["iteration 0\n","torch.Size([8, 3, 210, 160])\n","torch.Size([8, 512, 64])\n","dl1: torch.Size([8, 32, 105, 80]), dl2: torch.Size([8, 128, 53, 40]), dl3: torch.Size([8, 256, 27, 20]), dl4: torch.Size([8, 512, 14, 10]), out: torch.Size([8, 512, 140])\n","shape rbg_output: torch.Size([8, 512, 140]), shape language_output: torch.Size([8, 512, 140])\n","fused_output: torch.Size([8, 1024, 140]) - torch.float32\n"]}],"source":["epochs = 2\n","train_model(multimodal, batched_rgb, batched_language, batched_previous_actions, batched_labels, epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbojuJAk8IQq"},"outputs":[],"source":["# Attempts saving model state\n","#torch.save(multimodal.state_dict(), 'model_save_test.pth')\n","\n","# Saves the entire model\n","torch.save(multimodal, 'model_save_test.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQ-gN-fX8IQq","outputId":"2bc639d6-cc57-497c-bc2d-da46cf4be7dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["iteration 1\n","torch.Size([8, 3, 210, 160])\n","torch.Size([8, 512, 64])\n","dl1: torch.Size([8, 32, 105, 80]), dl2: torch.Size([8, 128, 53, 40]), dl3: torch.Size([8, 256, 27, 20]), dl4: torch.Size([8, 512, 14, 10]), out: torch.Size([8, 512, 140])\n","shape rbg_output: torch.Size([8, 512, 140]), shape language_output: torch.Size([8, 512, 140])\n","fused_output: torch.Size([8, 1024, 140])\n","multimodal_output: torch.Size([8, 1024, 512])\n","lstm_output: torch.Size([8, 512])\n","iteration 2\n","current batch size: 6\n"]}],"source":["# try to load the model and make a forward pass\n","model = torch.load('model_save_test.pth')\n","train_model(model=model, batched_rgb=batched_rgb, batched_language=batched_language)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUvAyF788IQq"},"outputs":[],"source":["# todo:\n","# To have third dimension for language embeddings, use DOM + utterance + task?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSEo_Dmn8IQq"},"outputs":[],"source":["# todo:\n","# pass and concatenate previous action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ij_-eCm8IQq","outputId":"682f107c-dcad-490a-9d84-3354c5aa0bb0"},"outputs":[{"data":{"text/plain":"'\u003cbody left 0 top 0 width 1300 height 210 id classes ref 93 children \u003cdiv left 0 top 0 width 160 height 210 id wrap classes ref 152 children \u003cdiv left 0 top 50 width 160 height 160 id area classes ref 262 children \u003cdiv left 0 top 50 width 160 height 160 id forward classes ref 369 children \u003cdiv left 0 top 50 width 160 height 20 id forward - bar classes ref 33 children \u003cspan left 0 top 60 width 10 height 10 id close - forward classes ref 248 text children \u003c/span\u003e \u003cspan left 140 top 60 width 10 height 10 id send - forward classes ref 299 text children \u003c/span\u003e \u003c/div\u003e \u003cdiv left 0 top 70 width 160 height 40 id classes forward - header ref 160 children \u003cdiv left 0 top 70 width 160 height 20 id classes forward - info ref 74 children \u003clabel left 0 top 80 width 20 height 10 id classes ref 1 text to: children \u003c/label\u003e \u003cinput_text left 20 top 80 width 120 height 20 id classes forward - sender ref 144 value text children \u003c/input_text\u003e \u003c/div\u003e \u003cdiv left 0 top 100 width 160 height 20 id classes forward - subject ref 178 children \u003clabel left 0 top 100 width 40 height 10 id classes ref 254 text subject: children \u003c/label\u003e \u003ct left 50 top 100 width 20 height 10 ref 148 text Dui. children \u003c/t\u003e \u003c/div\u003e \u003c/div\u003e \u003cdiv left 0 top 110 width 160 height 100 id classes forward - body ref 351 children \u003ctextarea left 0 top 110 width 150 height 100 id forward - text classes ref 193 value purusetviverradignissimquisqueipsummetusurna. text children \u003c/textarea\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/body\u003e '"},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.detokenize_array(tokenizer.get_tokens_from_embeddings(language_input[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wV2d_gbK8IQr","outputId":"bf8a4aa0-2f94-4fcf-ed00-c038e1a7ccb0"},"outputs":[{"data":{"text/plain":"(14, 552)"},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["np.concatenate((dom_data, utterance_data, task_data), axis=1).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHiP1s0q8IQr","outputId":"0c11907c-51c3-4094-f843-9d2f720b0368"},"outputs":[{"data":{"text/plain":"((14, 1), (14, 1), (14, 8))"},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Now work in creating targets of shape 577\n","target_action.shape, target_refs.shape, target_keydown.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UH54CxVr8IQr","outputId":"22f062bd-def9-45f8-f46b-6bba20afbb20"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\n"]}],"source":["print(target_action[10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l06zSUlN8IQr","outputId":"ef0b0608-9add-4f1d-f184-65f64bde3004"},"outputs":[{"data":{"text/plain":"'e'"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["'e'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0wSqOpi8IQr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"}},"nbformat":4,"nbformat_minor":0}