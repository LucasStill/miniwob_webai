{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2uIdOd/CNpXqvtAXbzuXu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmlIjSUEZ-CS","executionInfo":{"status":"ok","timestamp":1675249030815,"user_tz":-60,"elapsed":747,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"42eb4dde-1ad4-4eb2-862a-89d21cca2fbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-01 10:57:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt.1’\n","\n","input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n","\n","2023-02-01 10:57:10 (111 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["# read it in to inspect it\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","  text = f.read()"],"metadata":{"id":"FhRNUmHCaWLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6_7RWWqaxHP","executionInfo":{"status":"ok","timestamp":1675249031558,"user_tz":-60,"elapsed":22,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"dddca405-4ca5-424e-dd71-f03a77095811"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  1115394\n"]}]},{"cell_type":"code","source":["print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PX3rZS_Wa10H","executionInfo":{"status":"ok","timestamp":1675249031558,"user_tz":-60,"elapsed":19,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"e85f8ae0-2e7d-4301-d920-380299d9e2e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor citizens, the patricians good.\n","What authority surfeits on would relieve us: if they\n","would yield us but the superfluity, while it were\n","wholesome, we might guess they relieved us humanely;\n","but they think we are too dear: the leanness that\n","afflicts us, the object of our misery, is as an\n","inventory to particularise their abundance; our\n","sufferance is a gain to them Let us revenge this with\n","our pikes, ere we become rakes: for the gods know I\n","speak this in hunger for bread, not in thirst for revenge.\n","\n","\n"]}]},{"cell_type":"code","source":["# The set function gets the set of all characters. Then make it a list and sort.\n","#This is our vocabulary\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9SP3iXE0a4G_","executionInfo":{"status":"ok","timestamp":1675249031558,"user_tz":-60,"elapsed":15,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"f1dcdf37-29d9-4f0c-8763-623619e2eb43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","65\n"]}]},{"cell_type":"code","source":["# Let's build both the encoder and the decoder of the tokenization\n","# Create a mapping from characters to integers\n","stoi = {ch:i for i, ch in enumerate(chars)}\n","itos = {i:ch for i, ch in enumerate(chars)}\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of intergers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlw2oV4hbPjY","executionInfo":{"status":"ok","timestamp":1675249031559,"user_tz":-60,"elapsed":11,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"80778355-8281-41b7-81cb-8de578a2b62c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[46, 47, 47, 1, 58, 46, 43, 56, 43]\n","hii there\n"]}]},{"cell_type":"markdown","source":["Some notes from the lecture regarding tokenization:\n","\n","There are several ways to doing tokenization, one of them is BPE encoding which does not make tokens based on words or characters, but more as unit of subwords. The key idea is that you can trade off the codebook size, and the sequence length. So you can have very long sequence of integers with very small vocabulary, or very short sequences of integer with large vocabulary.\n","\n","In practice people use the sub-word precision, this is what GPT-3 uses. In this example we use a very small codebook, and very simple encode function but we get very long sequences."],"metadata":{"id":"60ZJJ1Wmc0y3"}},{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a torch.Tensor\n","import torch # we use PyTorch: https://pytorch.org\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earlier will to the GPT look like this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMQW1aQtcPq0","executionInfo":{"status":"ok","timestamp":1675249031559,"user_tz":-60,"elapsed":8,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"a9f7fe33-29d5-4192-cbc3-30c43a2e82f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1115394]) torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n","         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n","        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n","         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n","        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n","         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n","        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n","        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n","         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n","         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n","        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n","        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n","         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n","        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n","        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n","        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n","        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n","        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n","        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n","         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n","         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n","         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n","        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n","        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n","        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n","        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n","        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n","        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n","         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n","         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n","        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n","        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n","        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n","         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n","        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n","        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n","         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n","        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n","        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n","        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n","        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n","        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n","        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n","        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n","        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n","        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n","         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n","        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n","        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n","        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n","        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n","        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n","        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"QR4J4Ucsd2T7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To train the transformer, we don't feed the entire dataset at once. Instead we'll sample random chuncks at the time from the dataset and train it into. They all have some kind of length and maximum length. \n","\n","The maximum length is referred as block_size"],"metadata":{"id":"zDGG7jvUegqA"}},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTxfdTUCeM16","executionInfo":{"status":"ok","timestamp":1675249032420,"user_tz":-60,"elapsed":21,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"6b3f37cf-cfac-4ccf-9bc8-380fb7d5c99e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["Above, when we work with 8 elements, we need the context of another one, hence why we have 9. A way to look at it is in the context of 18, we expect 47.\n","In the context of 18, 47, we expect 56.\n","In the context of 18, 47, 56, we can expect 57 to come next."],"metadata":{"id":"qRapNhGBfGB8"}},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","  context = x[:t+1]\n","  target = y[t]\n","  print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHrhR1rKe9Vj","executionInfo":{"status":"ok","timestamp":1675249032421,"user_tz":-60,"elapsed":20,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"83bf0108-89a2-4242-d96a-e6d86eb519a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([18]) the target: 47\n","when input is tensor([18, 47]) the target: 56\n","when input is tensor([18, 47, 56]) the target: 57\n","when input is tensor([18, 47, 56, 57]) the target: 58\n","when input is tensor([18, 47, 56, 57, 58]) the target: 1\n","when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n","when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n","when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4  # how many independent sequences will we process in parallel in every forward/backward pass of the transformer?\n","block_size = 8  # what is the maximum context length for predictions?\n","\n","# generate a small batch of data of inputs x and targets y\n","def get_batch(split):\n","  # // below input if batch should be train or val data. Very nice way of doing it\n","  data = train_data if split == 'train' else val_data\n","  # // when we randomly select some data, we generate random batch_size of random offsets.\n","  # // so ix is going to be 4 numbers generated between 0 and len(data)-block_size\n","  ix = torch.randint(len(data) - block_size, (batch_size,))\n","\n","  # // xs is the first block_size char starting at i, y are the offset of 1 of that. And we'll get all chunkcks by using torch.stack to stack them up. And they become a row in a 4*8 tensor \n","  x = torch.stack([data[i:i+block_size] for i in ix])\n","  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","  return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","print('Explanation: Look at above: when the input is 24, the output is 43. When the input is 24, 43, the target is 58. When the input is 24, 43, 58, the target is 5')\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","  for t in range(block_size): # time dimension\n","    context = xb[b, :t+1]\n","    target = yb[b,t]\n","    print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WHmsATLfmpq","executionInfo":{"status":"ok","timestamp":1675249032421,"user_tz":-60,"elapsed":17,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"5422b526-10f0-4e61-df0f-d81d1a29a8f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","----\n","Explanation: Look at above: when the input is 24, the output is 43. When the input is 24, 43, the target is 58. When the input is 24, 43, 58, the target is 5\n","----\n","when input is [24] the target: 43\n","when input is [24, 43] the target: 58\n","when input is [24, 43, 58] the target: 5\n","when input is [24, 43, 58, 5] the target: 57\n","when input is [24, 43, 58, 5, 57] the target: 1\n","when input is [24, 43, 58, 5, 57, 1] the target: 46\n","when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n","when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n","when input is [44] the target: 53\n","when input is [44, 53] the target: 56\n","when input is [44, 53, 56] the target: 1\n","when input is [44, 53, 56, 1] the target: 58\n","when input is [44, 53, 56, 1, 58] the target: 46\n","when input is [44, 53, 56, 1, 58, 46] the target: 39\n","when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n","when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n","when input is [52] the target: 58\n","when input is [52, 58] the target: 1\n","when input is [52, 58, 1] the target: 58\n","when input is [52, 58, 1, 58] the target: 46\n","when input is [52, 58, 1, 58, 46] the target: 39\n","when input is [52, 58, 1, 58, 46, 39] the target: 58\n","when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n","when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n","when input is [25] the target: 17\n","when input is [25, 17] the target: 27\n","when input is [25, 17, 27] the target: 10\n","when input is [25, 17, 27, 10] the target: 0\n","when input is [25, 17, 27, 10, 0] the target: 21\n","when input is [25, 17, 27, 10, 0, 21] the target: 1\n","when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n","when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"]}]},{"cell_type":"code","source":["print(xb) # our input to the transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBWDvZa4hbX3","executionInfo":{"status":"ok","timestamp":1675249032421,"user_tz":-60,"elapsed":12,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"48b38ecb-1017-4b5f-e01b-fbec7741a517"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n"]}]},{"cell_type":"markdown","source":["Now that we have our input data ready, let's feed it into a neural network. We're going to use the simplest one for language models: the BiGram."],"metadata":{"id":"geETNFSdlU6J"}},{"cell_type":"code","source":["from ast import Index\n","import torch \n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","  def __init__(self, vocab_size):\n","    super().__init__()\n","    # each token directly reads off the logits for the next token from a lookup table\n","    # // We are creating a token embedding table of size vocab_size * vocab_size, and we use a very light embedding from nn\n","    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","  def forward(self, idx, targets=None):\n","\n","    # ids and targets are both (B, T) tensor of integers\n","    # // and when we pass our indexes here, it is going to refer to that created table and is going to black out a row corresponding to its index\n","    # // ==> So if you look at the printed tensor above, when we have 24 we'll black out the 24th row, when we have 43rd we'll black out the 43rd row, and etc\n","    # // and it is going to arrange that into a Batch, by Time, by Channel (B,T,C) tensor: 4*8*65\n","    logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","    if targets is None:\n","      loss = None\n","    else:\n","      # // Now we need to make the loss function: neg log likelyhood loss\n","      # // In pytorch, the documentation doesn't fit our shape, so instead of rewritting it we'll reshape our logits\n","      B, T, C = logits.shape\n","      # // We basically strech all of the position (from the above tensor) and stretch them in a one-dimensional sequence, and preserve the channel dimension as the second dimension\n","      logits = logits.view(B*T, C)\n","      # // need to do the same to targets\n","      targets = targets.view(B*T)\n","      loss = F.cross_entropy(logits, targets)\n","\n","    return logits, loss\n","\n","\n","  # // Do the generation from the model\n","  # // This was covered in another video ==> Go watch it\n","  def generate(self, idx, max_new_tokens):\n","    #idx is (B, T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","      # get the predictions\n","      logits, loss = self(idx)\n","      #focus only on the last time step\n","      logits = logits[:, -1, :] # becomes (B, C)\n","      # apply softmax to get probabilities\n","      probs = F.softmax(logits, dim=1) # (B, C)\n","      # sample from the distribution  \n","      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","      # append sampled index to the running sequence\n","      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","    return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","# // torch.zeros are the idx\n","# // we create a 1*1 tensor and it holds a zero, and the datatype is integer. We use it as a starting value\n","\n","# // then we're going to ask for 100 tokens, and m.generate will continue that\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlMOjtoalRxm","executionInfo":{"status":"ok","timestamp":1675249032421,"user_tz":-60,"elapsed":8,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"3cee0931-22ee-4cad-b3eb-ee6fcc21252a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 65])\n","tensor(4.8786, grad_fn=<NllLossBackward0>)\n","\n","SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n","wnYWmnxKWWev-tDqXErVKLgJ\n"]}]},{"cell_type":"code","source":["# create a Pytorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"CB4sRwShnSIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We use a bigger batch_size\n","batch_size = 32\n","for steps in range(10000):\n","\n","  # sample a batch of data\n","  xb, yb = get_batch('train')\n","\n","  # evaluate the loss\n","  logits, loss = m(xb, yb)\n","  # zero out greadients of the previous step\n","  optimizer.zero_grad(set_to_none=True)\n","  loss.backward()\n","  # update the params, ==> Look the series online\n","  optimizer.step()\n","\n","print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dET_ucpS68E9","executionInfo":{"status":"ok","timestamp":1675249052705,"user_tz":-60,"elapsed":20287,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"b0f3e4ba-307f-4e59-8cbf-dfabf032017c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.382369041442871\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99rKM5rJ7YLM","executionInfo":{"status":"ok","timestamp":1675249052706,"user_tz":-60,"elapsed":25,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"96cab882-fe4c-454e-e975-cd76bd84b049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n","RIfans picspeserer hee tha,\n","TOFonk? me ain ckntoty ded. bo'llll st ta d:\n","ELIS me hurf lal y, ma dus pe athouo\n","BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n","OLeneerithesinthengove fal amas trr\n","TI ar I t, mes, n IUSt my w, fredeeyove\n","THek' merer, dd\n","We ntem lud engitheso; cer ize helorowaginte the?\n","Thak orblyoruldvicee chot, p,\n","Bealivolde Th li\n"]}]},{"cell_type":"markdown","source":["# The mathematical trick in self-attention\n","\n","\n","The goal below where we have 8 tokens, is to have them talk to each other. But we don't want tokens to talk to the ones in the future. For instance, the 5th token should not talk to the 6th, 7th and 8th token. It cannot get any information about the future, because we are about to try to predict the future.\n","\n","The easiest way to let them communicate is to just do an average of all of the preceeding ones. That way it becomes some sort of summary vector of the environment and is extremely lossy. We'll see how to bring that information abck later.\n","\n","For every t vector in teh sequence we want to find these computed values in the past."],"metadata":{"id":"CtaPDWAdJbf-"}},{"cell_type":"code","source":["# consider the following exampke:\n","\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufD6whlx7uYa","executionInfo":{"status":"ok","timestamp":1675249052706,"user_tz":-60,"elapsed":23,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"2930a9f0-6c59-42d5-b8cb-c4adbb043668"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["# We want x[b,t] = mean_{i<=t} x[b,i]\n","\n","# // bow is the short term for bag of words\n","xbow = torch.zeros((B,T,C))\n","# We iterate through the batches, and then time\n","for b in range(B):\n","  for t in range(T):\n","    # // below are the previous_tokens and are the batch dimension, and up to including the tth token.\n","    # // so xprev becomes of shape (t,C)\n","    xprev = x[b,:t+1] # (t,C)\n","    # // now we average out the time here, and we get a C dimensional vector \n","    xbow[b,t] = torch.mean(xprev,0)"],"metadata":{"id":"vW_jsKzfJsO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GIrTDKMeLvsl","executionInfo":{"status":"ok","timestamp":1675249052706,"user_tz":-60,"elapsed":20,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"804bb700-53c1-4d63-f5c8-dfef46fa44d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1808, -0.0700],\n","        [-0.3596, -0.9152],\n","        [ 0.6258,  0.0255],\n","        [ 0.9545,  0.0643],\n","        [ 0.3612,  1.1679],\n","        [-1.3499, -0.5102],\n","        [ 0.2360, -0.2398],\n","        [-0.9211,  1.5433]])"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["xbow[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJyX46g1Lw2A","executionInfo":{"status":"ok","timestamp":1675249052706,"user_tz":-60,"elapsed":18,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"4fb741e0-bc73-44d0-c559-ada9ac5c4e29"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1808, -0.0700],\n","        [-0.0894, -0.4926],\n","        [ 0.1490, -0.3199],\n","        [ 0.3504, -0.2238],\n","        [ 0.3525,  0.0545],\n","        [ 0.0688, -0.0396],\n","        [ 0.0927, -0.0682],\n","        [-0.0341,  0.1332]])"]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["We observe above that for the first row they are the same, but when moving down, the second row of xbow is an average of the two first rows of x. The third row of xbow is an average of the three first rows of x, and so on. The last one is the average of all the elements of x.\n","\n","This is all good, but is very inefficient. Hence the mathematical trick of using matrix multiplication with torch.mean(xprev, 0)"],"metadata":{"id":"SpPA12geL2TL"}},{"cell_type":"code","source":["# wei is the short for weights. It is our new 'a' from below\n","wei = torch.tril(torch.ones(T,T))\n","wei = wei / wei.sum(1, keepdim=True)\n","wei\n","\n","# Below we reshape the weights to create a new xbow in the wau that was done below for our 'b'\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ---> (B, T, C)\n","torch.allclose(xbow, xbow2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwGsmwqUOC1n","executionInfo":{"status":"ok","timestamp":1675249052706,"user_tz":-60,"elapsed":16,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"0ffc496b-ccb2-46a5-c7da-d425172a969a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["# // What happened is that we are doing a weighted aggregation. But what happens here is that each token only gets information from previous tokens, and this is exactly what we want\n","xbow[0], xbow2[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUHwz-jiO2EQ","executionInfo":{"status":"ok","timestamp":1675249052706,"user_tz":-60,"elapsed":14,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"6d3ed18e-fecd-467a-87a8-a5481165b2b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.1808, -0.0700],\n","         [-0.0894, -0.4926],\n","         [ 0.1490, -0.3199],\n","         [ 0.3504, -0.2238],\n","         [ 0.3525,  0.0545],\n","         [ 0.0688, -0.0396],\n","         [ 0.0927, -0.0682],\n","         [-0.0341,  0.1332]]), tensor([[ 0.1808, -0.0700],\n","         [-0.0894, -0.4926],\n","         [ 0.1490, -0.3199],\n","         [ 0.3504, -0.2238],\n","         [ 0.3525,  0.0545],\n","         [ 0.0688, -0.0396],\n","         [ 0.0927, -0.0682],\n","         [-0.0341,  0.1332]]))"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["# // And we can rewrite that in one more way\n","\n","# version 3 : use Softmax\n","# // trill is again the triangular matrix, starting with zeros in the upper triangle\n","tril = torch.tril(torch.ones(T, T))\n","# // wei is all zeros\n","# // This one tells us how much of each token from the past we want to aggregate and average up\n","wei = torch.zeros((T,T))\n","# // this one makes all elements that are trill 0 to be equaled -inf, and the values that are 1 becomes zeros. It is the same matrix as before except that the 0s become -inf, and the 1s become 0\n","# // This one says that tokens of the past cannot communicate, that's why they are equal -inf\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","# // Then by applying softmax, it works as a normalization layer. So surprise, we get the exact same normalization matrix \n","wei = F.softmax(wei, dim=-1)\n","print(wei)\n","# // Then this does the aggregation\n","xbow3 = wei @ x\n","torch.allclose(xbow, xbow3)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O2qWk_KtPdOm","executionInfo":{"status":"ok","timestamp":1675249052707,"user_tz":-60,"elapsed":14,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"cb352006-1b16-4f01-9b99-0b63e2dfeb31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n","        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n","        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n","        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["# The trick is that torch has a triangular matrix, which compresses information of a squared 1 matrix:\n","torch.tril(torch.ones(3,3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gR9IZ8nYNFll","executionInfo":{"status":"ok","timestamp":1675249052707,"user_tz":-60,"elapsed":12,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"2ec57db7-cd89-4d28-c3f9-c91ea4907459"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0.],\n","        [1., 1., 0.],\n","        [1., 1., 1.]])"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","#a = torch.ones(3, 3)\n","a = torch.tril(torch.ones(3, 3))\n","a = a / torch.sum(a, 1, keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","# // matrix multiplication\n","c = a @ b\n","print('a=')\n","print(a)\n","print('--')\n","print('b=')\n","print(b)\n","print('--')\n","print('c=')\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fp_QuSubLzck","executionInfo":{"status":"ok","timestamp":1675249052707,"user_tz":-60,"elapsed":9,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"19dc668d-c393-4c18-dd4b-56a54f437a55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","--\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","--\n","c=\n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"markdown","source":["# Self Attention Part\n","\n","transformers solve this because every single node or every single token at each position will emit two vectors:\n","- a query vector\n","- a key vector\n","\n","The query vector is roughly speaking 'what am I looking for?'.\n","The key vector is roughly speaking 'what do I contain?'.\n","\n","The way we get affinities between these tokens now in a sequence is we basically do a DOT product between the keys and the queries.\n","\n","So my query dot products with all the keys of all the other tokens, and that dot product now becomes wei. So if the key and the vector and sort of aligned, they will interact to a very high amount, and then we'll get to learn more about that specific token as opposed to any other token in the sequence."],"metadata":{"id":"6yj6VI3zLJHc"}},{"cell_type":"code","source":["# version 4: self-attention!\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# // We'll implement a Head of self-attention\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","value = nn.Linear(C, head_size, bias=False)\n","k = key(x)  # (B, T, 16)\n","q = query(x)  # (B, T, 16)\n","# attention happens now:\n","# // all the queries will dot product with all the keys\n","# // we have to be careful and transpose the last two dimensions of k. Dimension -1, and dimension -2 \n","wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T,T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n"," \n","# v are the elements that we aggregate. These are not the tokens themselves\n","v = value(x)\n","out = wei @ v\n","#out = wei @ x\n","\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTEWQ0X-MqZ_","executionInfo":{"status":"ok","timestamp":1675249052707,"user_tz":-60,"elapsed":8,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"0462d7cd-3e7d-4e36-c7d5-7163094e0d70"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 16])"]},"metadata":{},"execution_count":75}]},{"cell_type":"markdown","source":["Notes:\n","- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n","- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n","- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n","- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a - \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n","- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n","- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"],"metadata":{"id":"JGJQBZNiSjfV"}},{"cell_type":"code","source":[" tril"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"myQ-MLJJLzzH","executionInfo":{"status":"ok","timestamp":1675249052707,"user_tz":-60,"elapsed":6,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"5a821f1f-070d-4002-f704-56c99ce8eda4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["wei[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqYwNkYfL0oH","executionInfo":{"status":"ok","timestamp":1675249053098,"user_tz":-60,"elapsed":396,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"7e541811-6a4c-4acf-9f18-ccd9fcd5f1c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n","        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n","        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n","        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["# Do the \"scaled\" attention\n","k = torch.randn(B,T,head_size)\n","q = torch.randn(B,T,head_size)\n","wei = q @ k.transpose(-2, -1) #* head_size**-0.5"],"metadata":{"id":"0IDejrtYL5Kf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2sI30MDUhQF","executionInfo":{"status":"ok","timestamp":1675249053098,"user_tz":-60,"elapsed":13,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"99589e56-eed0-450e-e811-6f62a83b832f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0449)"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["q.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npXxIMKjUilI","executionInfo":{"status":"ok","timestamp":1675249053098,"user_tz":-60,"elapsed":12,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"cfdf2a53-1630-496a-e802-ac2bc3735c68"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0700)"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["wei.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-da0EZE9UjPz","executionInfo":{"status":"ok","timestamp":1675249053099,"user_tz":-60,"elapsed":10,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"a8c9a19f-c02f-4782-ea83-947b8bf84c6e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(17.4690)"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5blC79zUkB0","executionInfo":{"status":"ok","timestamp":1675249053099,"user_tz":-60,"elapsed":8,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"f4e9bc3b-2e93-4eac-e960-3398b1143a61"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["# // we can see that they become way too sharp, hence why we need scale\n","torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6gq_pF_Uqsj","executionInfo":{"status":"ok","timestamp":1675249053099,"user_tz":-60,"elapsed":6,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"961f91c6-3233-4da1-fb4d-a89787b60d79"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"]},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","source":["# Layernorm (and its relationship to our previous batchnorm)\n","\n","We normalize the rows instead of the columns"],"metadata":{"id":"0EyAPP5Un6Tk"}},{"cell_type":"code","source":["class BatchNorm1d:\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","\n","  def __call__(self, x):\n","    # calculate the forward pass\n","    xmean = x.mean(1, keepdim=True) # batch mean\n","    xvar = x.var(1, keepdim=True) # batch variance\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n","\n","torch.manual_seed(1337)\n","module = BatchNorm1d(100)\n","x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n","x = module(x)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azuO3wA_VMBQ","executionInfo":{"status":"ok","timestamp":1675249712135,"user_tz":-60,"elapsed":205,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"2e009ec8-0d28-469e-ca67-4735100188fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100])"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["x[:,0].mean(), x[:,0].std() # mean, std of one feature across all batch inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vH9m03GfpgGu","executionInfo":{"status":"ok","timestamp":1675249712468,"user_tz":-60,"elapsed":10,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"02a20c0c-4333-405c-b122-021da9db17e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1469), tensor(0.8803))"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["x[0,:].mean(), x[0,:].std() # mean, std of a single input from the batch, of its features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LjtF-3zp1Iq","executionInfo":{"status":"ok","timestamp":1675249712468,"user_tz":-60,"elapsed":7,"user":{"displayName":"Lucas Thil","userId":"16582782570420309877"}},"outputId":"519fd445-072f-4475-be0d-afafb40987cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(-9.5367e-09), tensor(1.0000))"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":[],"metadata":{"id":"4_GWCPOAp8wy"},"execution_count":null,"outputs":[]}]}